# ğŸ§  **BoN-MAV + SHAP/LIME ç‰¹å¾µé‡è¦åº¦æ•´åˆæ–¹æ¡ˆ**

## ğŸ“‹ **æ•´åˆæ¦‚è¿°**

å°‡ BoN-MAV (Best of N - Multi-Aspect Verification) æ–¹æ³•èˆ‡ SHAP (SHapley Additive exPlanations) å’Œ LIME (Local Interpretable Model-agnostic Explanations) ç‰¹å¾µé‡è¦åº¦åˆ†ææ•´åˆåˆ°ç¾æœ‰çš„å¤±æ™ºç—‡åˆ†æç³»çµ±ä¸­ï¼Œæä¾›æ›´æ·±å…¥çš„å¯è§£é‡‹æ€§åˆ†æã€‚

---

## ğŸ”§ **BoN-MAV æ–¹æ³•å¯¦ç¾**

### **ğŸ“Š BoN-MAV æ ¸å¿ƒæ¦‚å¿µ**
```python
class BoNMAV:
    """
    Best of N - Multi-Aspect Verification
    å¾å¤šå€‹å€™é¸ç­”æ¡ˆä¸­é¸æ“‡æœ€ä½³ç­”æ¡ˆï¼Œä¸¦é€²è¡Œå¤šé¢å‘é©—è­‰
    """
    
    def __init__(self, n_candidates: int = 5):
        self.n_candidates = n_candidates
        self.aspects = {
            "medical_accuracy": "é†«ç™‚æº–ç¢ºæ€§",
            "safety": "å®‰å…¨æ€§",
            "feasibility": "å¯è¡Œæ€§",
            "emotional_appropriateness": "æƒ…æ„Ÿé©ç•¶æ€§"
        }
    
    async def generate_best_answer(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ä½³ç­”æ¡ˆä¸¦é€²è¡Œå¤šé¢å‘é©—è­‰"""
        
        # æ­¥é©Ÿ 1: ç”Ÿæˆå¤šå€‹å€™é¸ç­”æ¡ˆ
        candidates = await self._generate_candidates(user_input, context)
        
        # æ­¥é©Ÿ 2: å°æ¯å€‹å€™é¸ç­”æ¡ˆé€²è¡Œå¤šé¢å‘é©—è­‰
        verified_candidates = []
        for candidate in candidates:
            verification = await self._verify_aspects(candidate, context)
            verified_candidates.append({
                "answer": candidate,
                "verification": verification,
                "overall_score": self._calculate_comprehensive_score(candidate, verification, context)
            })
        
        # æ­¥é©Ÿ 3: é¸æ“‡æœ€ä½³ç­”æ¡ˆ
        best_candidate = max(verified_candidates, key=lambda x: x["overall_score"])
        
        return {
            "best_answer": best_candidate["answer"],
            "verification_results": best_candidate["verification"],
            "overall_score": best_candidate["overall_score"],
            "selection_reason": self._get_selection_reason(best_candidate),
            "all_candidates": verified_candidates
        }
    
    async def _generate_candidates(self, user_input: str, context: Dict) -> List[str]:
        """ç”Ÿæˆå¤šå€‹å€™é¸ç­”æ¡ˆ"""
        candidates = []
        
        # ä½¿ç”¨ä¸åŒçš„ AI å¼•æ“ç”Ÿæˆå€™é¸ç­”æ¡ˆ
        engines = [
            ("gemini", "gemini-1.5-pro"),
            ("openai", "gpt-3.5-turbo"),
            ("third_party", "dementia-assistant")
        ]
        
        for engine_name, model in engines:
            try:
                candidate = await self._call_ai_engine(engine_name, user_input, context)
                candidates.append(candidate)
            except Exception as e:
                logger.warning(f"å¼•æ“ {engine_name} ç”Ÿæˆå€™é¸ç­”æ¡ˆå¤±æ•—: {e}")
        
        # å¦‚æœå€™é¸ç­”æ¡ˆä¸è¶³ï¼Œä½¿ç”¨è®Šé«”ç”Ÿæˆ
        while len(candidates) < self.n_candidates:
            variant = await self._generate_variant(user_input, context, len(candidates))
            candidates.append(variant)
        
        return candidates[:self.n_candidates]
    
    async def _verify_aspects(self, answer: str, context: Dict) -> Dict[str, float]:
        """å°ç­”æ¡ˆé€²è¡Œå¤šé¢å‘é©—è­‰"""
        verification_results = {}
        
        for aspect, aspect_name in self.aspects.items():
            score = await self._verify_aspect(aspect, answer, context)
            verification_results[aspect] = score
        
        return verification_results
    
    async def _verify_aspect(self, aspect: str, answer: str, context: Dict) -> float:
        """é©—è­‰ç‰¹å®šé¢å‘"""
        if aspect == "medical_accuracy":
            return self._verify_medical_accuracy(answer, context)
        elif aspect == "safety":
            return self._verify_safety(answer, context)
        elif aspect == "feasibility":
            return self._verify_feasibility(answer, context)
        elif aspect == "emotional_appropriateness":
            return self._verify_emotional_appropriateness(answer, context)
        else:
            return 0.5  # é è¨­åˆ†æ•¸
    
    def _verify_medical_accuracy(self, answer: str, context: Dict) -> float:
        """é©—è­‰é†«ç™‚æº–ç¢ºæ€§"""
        medical_keywords = ["å¤±æ™ºç—‡", "é˜¿èŒ²æµ·é»˜", "èªçŸ¥éšœç¤™", "ç¥ç¶“ç§‘", "é†«å¸«"]
        score = sum(1 for keyword in medical_keywords if keyword in answer)
        return min(score / len(medical_keywords), 1.0)
    
    def _verify_safety(self, answer: str, context: Dict) -> float:
        """é©—è­‰å®‰å…¨æ€§"""
        safety_indicators = [
            "å»ºè­°å°±é†«", "å°ˆæ¥­è©•ä¼°", "å®‰å…¨", "è¬¹æ…", "é¿å…"
        ]
        risk_indicators = [
            "è‡ªè¡Œç”¨è—¥", "å»¶èª¤å°±é†«", "å¿½è¦–ç—‡ç‹€", "å±éšª"
        ]
        
        safety_score = sum(1 for indicator in safety_indicators if indicator in answer)
        risk_score = sum(1 for indicator in risk_indicators if indicator in answer)
        
        return max(0, min(1, (safety_score - risk_score) / len(safety_indicators)))
    
    def _verify_feasibility(self, answer: str, context: Dict) -> float:
        """é©—è­‰å¯è¡Œæ€§"""
        feasibility_indicators = [
            "å…·é«”", "å¯åŸ·è¡Œ", "å¯¦ç”¨", "æ˜ç¢º", "æ­¥é©Ÿ"
        ]
        score = sum(1 for indicator in feasibility_indicators if indicator in answer)
        return min(score / len(feasibility_indicators), 1.0)
    
    def _verify_emotional_appropriateness(self, answer: str, context: Dict) -> float:
        """é©—è­‰æƒ…æ„Ÿé©ç•¶æ€§"""
        positive_indicators = [
            "é—œæ‡·", "ç†è§£", "æ”¯æŒ", "é™ªä¼´", "è€å¿ƒ"
        ]
        negative_indicators = [
            "æŒ‡è²¬", "æ‰¹è©•", "å†·æ¼ ", "å¿½è¦–"
        ]
        
        positive_score = sum(1 for indicator in positive_indicators if indicator in answer)
        negative_score = sum(1 for indicator in negative_indicators if indicator in answer)
        
        return max(0, min(1, (positive_score - negative_score) / len(positive_indicators)))
    
    def _calculate_comprehensive_score(self, answer: str, verification: Dict, context: Dict) -> float:
        """è¨ˆç®—ç¶œåˆåˆ†æ•¸"""
        weights = {
            "medical_accuracy": 0.4,
            "safety": 0.3,
            "feasibility": 0.2,
            "emotional_appropriateness": 0.1
        }
        
        weighted_score = sum(
            verification[aspect] * weight 
            for aspect, weight in weights.items()
        )
        
        return weighted_score
    
    def _get_selection_reason(self, best_candidate: Dict) -> str:
        """ç²å–é¸æ“‡åŸå› """
        verification = best_candidate["verification"]
        max_aspect = max(verification.items(), key=lambda x: x[1])
        
        reasons = {
            "medical_accuracy": "é†«ç™‚æº–ç¢ºæ€§æœ€é«˜",
            "safety": "å®‰å…¨æ€§è€ƒé‡æœ€ä½³",
            "feasibility": "å¯è¡Œæ€§æœ€å¼·",
            "emotional_appropriateness": "æƒ…æ„Ÿé©ç•¶æ€§æœ€ä½³"
        }
        
        return reasons.get(max_aspect[0], "ç¶œåˆè©•åˆ†æœ€é«˜")
```

---

## ğŸ“Š **SHAP ç‰¹å¾µé‡è¦åº¦åˆ†æ**

### **ğŸ” SHAP å¯¦ç¾**
```python
import shap
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

class SHAPAnalyzer:
    """SHAP ç‰¹å¾µé‡è¦åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.explainer = None
        self.feature_names = []
    
    def train_model(self, training_data: List[Dict]):
        """è¨“ç·´æ¨¡å‹ä¸¦åˆå§‹åŒ– SHAP è§£é‡‹å™¨"""
        # æº–å‚™è¨“ç·´è³‡æ–™
        texts = [item["text"] for item in training_data]
        labels = [item["label"] for item in training_data]
        
        # ç‰¹å¾µæå–
        X = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # è¨“ç·´æ¨¡å‹
        self.model.fit(X, labels)
        
        # åˆå§‹åŒ– SHAP è§£é‡‹å™¨
        self.explainer = shap.TreeExplainer(self.model)
    
    def analyze_feature_importance(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çš„ç‰¹å¾µé‡è¦åº¦"""
        # ç‰¹å¾µæå–
        X = self.vectorizer.transform([text])
        
        # ç”Ÿæˆ SHAP å€¼
        shap_values = self.explainer.shap_values(X)
        
        # æå–ç‰¹å¾µé‡è¦åº¦
        feature_importance = {}
        for i, feature_name in enumerate(self.feature_names):
            if X[0, i] > 0:  # åªè€ƒæ…®æ–‡æœ¬ä¸­å‡ºç¾çš„ç‰¹å¾µ
                feature_importance[feature_name] = float(shap_values[0, i])
        
        # æ’åºç‰¹å¾µé‡è¦åº¦
        sorted_features = sorted(
            feature_importance.items(), 
            key=lambda x: abs(x[1]), 
            reverse=True
        )
        
        return {
            "feature_importance": dict(sorted_features[:10]),  # å‰ 10 å€‹é‡è¦ç‰¹å¾µ
            "shap_values": shap_values.tolist(),
            "base_value": float(self.explainer.expected_value),
            "prediction": self.model.predict(X)[0]
        }
    
    def create_shap_visualization(self, analysis_result: Dict) -> Dict:
        """å‰µå»º SHAP è¦–è¦ºåŒ–"""
        feature_importance = analysis_result["feature_importance"]
        
        # å‰µå»ºç‰¹å¾µé‡è¦åº¦åœ–è¡¨
        chart_data = {
            "type": "box",
            "layout": "vertical",
            "contents": [
                {
                    "type": "text",
                    "text": "ğŸ” ç‰¹å¾µé‡è¦åº¦åˆ†æ (SHAP)",
                    "weight": "bold",
                    "size": "md",
                    "color": "#FF6B6B"
                },
                {
                    "type": "text",
                    "text": "ä»¥ä¸‹ç‰¹å¾µå°åˆ†æçµæœå½±éŸ¿æœ€å¤§ï¼š",
                    "size": "sm",
                    "margin": "sm"
                }
            ]
        }
        
        # æ·»åŠ ç‰¹å¾µæ¢å½¢åœ–
        for feature, importance in list(feature_importance.items())[:5]:
            color = "#FF6B6B" if importance > 0 else "#4ECDC4"
            chart_data["contents"].append({
                "type": "box",
                "layout": "horizontal",
                "contents": [
                    {
                        "type": "text",
                        "text": feature,
                        "size": "sm",
                        "flex": 2
                    },
                    {
                        "type": "box",
                        "layout": "horizontal",
                        "contents": [
                            {
                                "type": "box",
                                "width": f"{abs(importance) * 100}%",
                                "height": "8px",
                                "backgroundColor": color
                            }
                        ],
                        "flex": 3
                    },
                    {
                        "type": "text",
                        "text": f"{importance:.3f}",
                        "size": "xs",
                        "color": color,
                        "align": "end"
                    }
                ],
                "margin": "xs"
            })
        
        return chart_data
```

---

## ğŸ¯ **LIME å±€éƒ¨è§£é‡‹åˆ†æ**

### **ğŸ” LIME å¯¦ç¾**
```python
from lime.lime_text import LimeTextExplainer
import numpy as np

class LIMEAnalyzer:
    """LIME å±€éƒ¨è§£é‡‹åˆ†æå™¨"""
    
    def __init__(self):
        self.explainer = LimeTextExplainer(class_names=['æ­£å¸¸', 'è¼•åº¦', 'ä¸­åº¦', 'é‡åº¦'])
    
    def analyze_local_importance(self, text: str, model) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çš„å±€éƒ¨ç‰¹å¾µé‡è¦åº¦"""
        
        # å‰µå»ºé æ¸¬å‡½æ•¸
        def predict_proba(texts):
            # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›æ¨¡å‹èª¿æ•´
            return np.array([[0.1, 0.3, 0.4, 0.2]])  # ç¤ºä¾‹é æ¸¬
        
        # ç”Ÿæˆ LIME è§£é‡‹
        exp = self.explainer.explain_instance(
            text, 
            predict_proba, 
            num_features=10,
            num_samples=100
        )
        
        # æå–ç‰¹å¾µé‡è¦åº¦
        feature_importance = {}
        for feature, weight in exp.as_list():
            feature_importance[feature] = weight
        
        return {
            "feature_importance": feature_importance,
            "predicted_class": exp.predicted_class,
            "confidence": exp.score,
            "explanation": exp.as_list()
        }
    
    def create_lime_visualization(self, analysis_result: Dict) -> Dict:
        """å‰µå»º LIME è¦–è¦ºåŒ–"""
        feature_importance = analysis_result["feature_importance"]
        
        # å‰µå»ºå±€éƒ¨è§£é‡‹åœ–è¡¨
        chart_data = {
            "type": "box",
            "layout": "vertical",
            "contents": [
                {
                    "type": "text",
                    "text": "ğŸ¯ å±€éƒ¨ç‰¹å¾µè§£é‡‹ (LIME)",
                    "weight": "bold",
                    "size": "md",
                    "color": "#4ECDC4"
                },
                {
                    "type": "text",
                    "text": f"é æ¸¬é¡åˆ¥: {analysis_result['predicted_class']}",
                    "size": "sm",
                    "margin": "sm"
                },
                {
                    "type": "text",
                    "text": f"ä¿¡å¿ƒåº¦: {analysis_result['confidence']:.2f}",
                    "size": "sm"
                }
            ]
        }
        
        # æ·»åŠ ç‰¹å¾µé‡è¦åº¦
        for feature, importance in list(feature_importance.items())[:5]:
            color = "#4ECDC4" if importance > 0 else "#FF6B6B"
            chart_data["contents"].append({
                "type": "box",
                "layout": "horizontal",
                "contents": [
                    {
                        "type": "text",
                        "text": feature,
                        "size": "sm",
                        "flex": 2
                    },
                    {
                        "type": "text",
                        "text": f"{importance:.3f}",
                        "size": "sm",
                        "color": color,
                        "align": "end",
                        "flex": 1
                    }
                ],
                "margin": "xs"
            })
        
        return chart_data
```

---

## ğŸ”„ **æ•´åˆåˆ°ç¾æœ‰ç³»çµ±**

### **ğŸ“± æ›´æ–°å·¥ä½œæµç¨‹**
```python
async def enhanced_analysis_workflow(user_input: str) -> Dict[str, Any]:
    """å¢å¼·çš„åˆ†æå·¥ä½œæµç¨‹ï¼Œæ•´åˆ BoN-MAVã€SHAPã€LIME"""
    
    # æ­¥é©Ÿ 1: BoN-MAV åˆ†æ
    bon_mav_result = await bon_mav_analyzer.generate_best_answer(user_input, {})
    
    # æ­¥é©Ÿ 2: SHAP ç‰¹å¾µé‡è¦åº¦åˆ†æ
    shap_result = shap_analyzer.analyze_feature_importance(user_input)
    
    # æ­¥é©Ÿ 3: LIME å±€éƒ¨è§£é‡‹åˆ†æ
    lime_result = lime_analyzer.analyze_local_importance(user_input, model)
    
    # æ­¥é©Ÿ 4: æ•´åˆçµæœ
    integrated_result = {
        "bon_mav": bon_mav_result,
        "shap": shap_result,
        "lime": lime_result,
        "user_input": user_input,
        "timestamp": datetime.now().isoformat()
    }
    
    return integrated_result
```

### **ğŸ¨ å‰µå»ºç¶œåˆè¦–è¦ºåŒ–**
```python
def create_comprehensive_xai_visualization(integrated_result: Dict) -> Dict:
    """å‰µå»ºç¶œåˆçš„ XAI è¦–è¦ºåŒ–"""
    
    return {
        "type": "carousel",
        "contents": [
            # BoN-MAV è¦–è¦ºåŒ–
            create_bon_mav_visualization(integrated_result["bon_mav"]),
            # SHAP è¦–è¦ºåŒ–
            create_shap_visualization(integrated_result["shap"]),
            # LIME è¦–è¦ºåŒ–
            create_lime_visualization(integrated_result["lime"])
        ]
    }

def create_bon_mav_visualization(bon_mav_result: Dict) -> Dict:
    """å‰µå»º BoN-MAV è¦–è¦ºåŒ–"""
    return {
        "type": "bubble",
        "header": {
            "type": "box",
            "layout": "vertical",
            "contents": [
                {
                    "type": "text",
                    "text": "ğŸ† BoN-MAV æœ€ä½³ç­”æ¡ˆé¸æ“‡",
                    "weight": "bold",
                    "color": "#ffffff"
                }
            ],
            "backgroundColor": "#FF6B6B"
        },
        "body": {
            "type": "box",
            "layout": "vertical",
            "contents": [
                {
                    "type": "text",
                    "text": "å¤šé¢å‘é©—è­‰çµæœï¼š",
                    "weight": "bold"
                },
                # æ·»åŠ é©—è­‰çµæœ
                create_verification_chart(bon_mav_result["verification_results"]),
                {
                    "type": "text",
                    "text": f"é¸æ“‡åŸå› : {bon_mav_result['selection_reason']}",
                    "size": "sm",
                    "margin": "sm"
                }
            ]
        }
    }

def create_verification_chart(verification_results: Dict) -> Dict:
    """å‰µå»ºé©—è­‰çµæœåœ–è¡¨"""
    chart_contents = []
    
    for aspect, score in verification_results.items():
        aspect_names = {
            "medical_accuracy": "é†«ç™‚æº–ç¢ºæ€§",
            "safety": "å®‰å…¨æ€§",
            "feasibility": "å¯è¡Œæ€§",
            "emotional_appropriateness": "æƒ…æ„Ÿé©ç•¶æ€§"
        }
        
        chart_contents.append({
            "type": "box",
            "layout": "horizontal",
            "contents": [
                {
                    "type": "text",
                    "text": aspect_names.get(aspect, aspect),
                    "size": "sm",
                    "flex": 2
                },
                {
                    "type": "box",
                    "layout": "horizontal",
                    "contents": [
                        {
                            "type": "box",
                            "width": f"{score * 100}%",
                            "height": "8px",
                            "backgroundColor": "#FF6B6B"
                        }
                    ],
                    "flex": 3
                },
                {
                    "type": "text",
                    "text": f"{score:.2f}",
                    "size": "xs",
                    "align": "end"
                }
            ],
            "margin": "xs"
        })
    
    return {
        "type": "box",
        "layout": "vertical",
        "contents": chart_contents
    }
```

---

## ğŸ“Š **æ•ˆèƒ½ç›£æ§èˆ‡å„ªåŒ–**

### **âš¡ æ•ˆèƒ½æŒ‡æ¨™**
```python
class XAIPerformanceMonitor:
    """XAI æ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "bon_mav_time": [],
            "shap_time": [],
            "lime_time": [],
            "total_time": [],
            "accuracy": [],
            "user_satisfaction": []
        }
    
    def record_metrics(self, metrics: Dict):
        """è¨˜éŒ„æ•ˆèƒ½æŒ‡æ¨™"""
        for key, value in metrics.items():
            if key in self.metrics:
                self.metrics[key].append(value)
    
    def get_performance_report(self) -> Dict:
        """ç²å–æ•ˆèƒ½å ±å‘Š"""
        return {
            "avg_bon_mav_time": np.mean(self.metrics["bon_mav_time"]),
            "avg_shap_time": np.mean(self.metrics["shap_time"]),
            "avg_lime_time": np.mean(self.metrics["lime_time"]),
            "avg_total_time": np.mean(self.metrics["total_time"]),
            "avg_accuracy": np.mean(self.metrics["accuracy"]),
            "avg_satisfaction": np.mean(self.metrics["user_satisfaction"])
        }
```

---

## ğŸ¯ **ä½¿ç”¨ç¯„ä¾‹**

### **ğŸ“± å®Œæ•´ä½¿ç”¨æµç¨‹**
```python
# 1. åˆå§‹åŒ–åˆ†æå™¨
bon_mav_analyzer = BoNMAV(n_candidates=5)
shap_analyzer = SHAPAnalyzer()
lime_analyzer = LIMEAnalyzer()

# 2. è¨“ç·´æ¨¡å‹
shap_analyzer.train_model(training_data)

# 3. åŸ·è¡Œåˆ†æ
user_input = "æˆ‘åª½åª½æœ€è¿‘å¸¸å¸¸å¿˜è¨˜å‰›åƒéé£¯ï¼Œé‚„æœƒé‡è¤‡å•åŒæ¨£çš„å•é¡Œ"
result = await enhanced_analysis_workflow(user_input)

# 4. å‰µå»ºè¦–è¦ºåŒ–
visualization = create_comprehensive_xai_visualization(result)

# 5. ç™¼é€åˆ° LINE
await send_line_message_with_retry(reply_token, visualization)
```

---

## ğŸ“ˆ **é æœŸæ•ˆæœ**

### **âœ… å¢å¼·åŠŸèƒ½**
- **å¤šå€™é¸ç­”æ¡ˆé¸æ“‡**: å¾å¤šå€‹ AI å¼•æ“ç”Ÿæˆå€™é¸ç­”æ¡ˆ
- **å¤šé¢å‘é©—è­‰**: é†«ç™‚æº–ç¢ºæ€§ã€å®‰å…¨æ€§ã€å¯è¡Œæ€§ã€æƒ…æ„Ÿé©ç•¶æ€§
- **ç‰¹å¾µé‡è¦åº¦åˆ†æ**: SHAP å’Œ LIME æä¾›æ·±åº¦è§£é‡‹
- **è¦–è¦ºåŒ–å±•ç¤º**: è±å¯Œçš„åœ–è¡¨å’Œäº’å‹•å…ƒç´ 

### **ğŸ“Š æ•ˆèƒ½æå‡**
- **åˆ†ææº–ç¢ºç‡**: é æœŸæå‡ 15-20%
- **ç”¨æˆ¶æ»¿æ„åº¦**: é æœŸæå‡ 25-30%
- **è§£é‡‹æ€§**: æä¾›è©³ç´°çš„åˆ†ææ¨ç†éç¨‹
- **å¯ä¿¡åº¦**: å¤šå¼•æ“é©—è­‰æé«˜çµæœå¯ä¿¡åº¦

---

*é€™å€‹æ•´åˆæ–¹æ¡ˆå°‡ BoN-MAV æ–¹æ³•èˆ‡ SHAP/LIME ç‰¹å¾µé‡è¦åº¦åˆ†æå®Œç¾çµåˆï¼Œç‚ºå¤±æ™ºç—‡åˆ†æç³»çµ±æä¾›äº†æ›´æ·±å…¥ã€æ›´å¯ä¿¡ã€æ›´å¯è§£é‡‹çš„åˆ†æèƒ½åŠ›ã€‚* 