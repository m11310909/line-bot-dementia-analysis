#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆ M1+M2+M3 æ•´åˆ API
æ•´åˆ Redis å¿«å–å’Œå„ªåŒ– Gemini API
"""

import os
import logging
import asyncio
from typing import Any, Dict, List
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import (
    Configuration, ApiClient, MessagingApi,
    ReplyMessageRequest, TextMessage, FlexMessage
)
from linebot.v3.webhooks import MessageEvent, TextMessageContent
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½® LINE Bot è¶…æ™‚å’Œé‡è©¦
LINE_TIMEOUT = 30  # 30 ç§’è¶…æ™‚
LINE_RETRY_ATTEMPTS = 3
LINE_RETRY_BACKOFF = 1

# å‰µå»ºè‡ªå®šç¾©çš„ requests session ç”¨æ–¼ LINE API
def create_line_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=LINE_RETRY_ATTEMPTS,
        backoff_factor=LINE_RETRY_BACKOFF,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

# åˆå§‹åŒ– LINE Bot é…ç½®
def initialize_line_bot():
    try:
        channel_access_token = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
        channel_secret = os.getenv("LINE_CHANNEL_SECRET")
        
        if not channel_access_token or not channel_secret:
            print("âŒ LINE Bot æ†‘è­‰æœªè¨­ç½®")
            return None, None
        
        # é…ç½® LINE Bot (ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨è‡ªå®šç¾© session)
        configuration = Configuration(access_token=channel_access_token)
        api_client = ApiClient(configuration)
        messaging_api = MessagingApi(api_client)
        handler = WebhookHandler(channel_secret)
        
        print("âœ… LINE Bot åˆå§‹åŒ–æˆåŠŸ")
        return messaging_api, handler
        
    except Exception as e:
        print(f"âŒ LINE Bot åˆå§‹åŒ–å¤±æ•—: {e}")
        return None, None

# å…¨å±€è®Šæ•¸
line_bot_api, handler = initialize_line_bot()

# åˆå§‹åŒ– logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¼•å…¥å„ªåŒ–æ¨¡çµ„
try:
    from redis_cache_manager import RedisCacheManager, cache_result
    from optimized_gemini_client import OptimizedGeminiClient
except ImportError as e:
    print(f"âš ï¸  å„ªåŒ–æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    RedisCacheManager = None
    OptimizedGeminiClient = None

# å¼•å…¥æ•´åˆå¼•æ“
try:
    from m1_m2_m3_integrated_rag import M1M2M3IntegratedEngine
except ImportError:
    print("âš ï¸  æ•´åˆå¼•æ“æ¨¡çµ„æœªæ‰¾åˆ°")
    M1M2M3IntegratedEngine = None

# å°å…¥æ‰€æœ‰æ¨¡çµ„
from modules.m1_warning_signs import M1WarningSignsModule
from modules.m2_progression_matrix import M2ProgressionMatrixModule
from modules.m3_bpsd_classification import M3BPSDClassificationModule
from modules.m4_care_navigation import M4CareNavigationModule

# åˆå§‹åŒ–æ‰€æœ‰æ¨¡çµ„
m1_module = M1WarningSignsModule()
m2_module = M2ProgressionMatrixModule()
m3_module = M3BPSDClassificationModule()
m4_module = M4CareNavigationModule()

# FastAPI æ‡‰ç”¨
app = FastAPI()

# å…¨åŸŸå¼•æ“å’Œå„ªåŒ–çµ„ä»¶
integrated_engine = None
cache_manager = None
optimized_gemini = None


# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_env_variables():
    """æª¢æŸ¥ç’°å¢ƒè®Šæ•¸"""
    print("ğŸ” æª¢æŸ¥ç’°å¢ƒè®Šæ•¸...")

    # æª¢æŸ¥ .env æª”æ¡ˆ
    if not os.path.exists(".env"):
        print("âŒ .env æª”æ¡ˆä¸å­˜åœ¨")
        return False

    # æª¢æŸ¥é—œéµç’°å¢ƒè®Šæ•¸
    env_vars = {
        "LINE_CHANNEL_ACCESS_TOKEN": os.getenv("LINE_CHANNEL_ACCESS_TOKEN"),
        "LINE_CHANNEL_SECRET": os.getenv("LINE_CHANNEL_SECRET"),
        "GEMINI_API_KEY": os.getenv("GEMINI_API_KEY"),
    }

    for var_name, var_value in env_vars.items():
        if not var_value or var_value.startswith("your_actual_"):
            print(f"âŒ {var_name} æœªæ­£ç¢ºè¨­ç½®")
        else:
            print(f"âœ… {var_name} å·²è¨­ç½®")

    return True


def create_smart_flex_message(user_input: str, analysis_result: Any) -> Dict:
    """æ™ºèƒ½å‰µå»ºé©åˆçš„ Flex Messageï¼Œæ ¹æ“šç”¨æˆ¶å•é¡Œé¸é…è¦–è¦ºæ¨¡çµ„"""
    
    # ä½¿ç”¨ M1 æ¨¡çµ„é€²è¡Œè­¦è¨Šåˆ†æ
    m1_analysis = m1_module.analyze_warning_signs(user_input)
    matched_signs = m1_analysis.get('matched_signs', [])
    
    # ä½¿ç”¨ M2 æ¨¡çµ„é€²è¡Œç—…ç¨‹éšæ®µåˆ†æ
    m2_analysis = m2_module.analyze_progression(user_input)
    
    # ä½¿ç”¨ M3 æ¨¡çµ„é€²è¡Œ BPSD ç—‡ç‹€åˆ†æ
    m3_analysis = m3_module.analyze_bpsd_symptoms(user_input)
    
    # ä½¿ç”¨ M4 æ¨¡çµ„é€²è¡Œç…§è­·éœ€æ±‚åˆ†æ
    m4_analysis = m4_module.analyze_care_needs(user_input)
    
    # æ™ºèƒ½æ¨¡çµ„é¸æ“‡é‚è¼¯
    user_input_lower = user_input.lower()
    
    # å„ªå…ˆç´šï¼šM1 > M3 > M2 > M4
    if matched_signs:
        logger.info(f"[DEBUG] ä½¿ç”¨ M1 è¦–è¦ºåŒ–æ¯”å°å¡ç‰‡ï¼ŒåŒ¹é…è­¦è¨Šï¼š{matched_signs}")
        return m1_module.create_visual_comparison_card(user_input, matched_signs)
    
    elif m3_analysis["detected_categories"]:
        logger.info(f"[DEBUG] ä½¿ç”¨ M3 BPSD ç—‡ç‹€åˆ†æï¼Œæª¢æ¸¬åˆ°ï¼š{m3_analysis['detected_categories']}")
        return m3_module.create_bpsd_card(user_input, m3_analysis)
    
    elif any(word in user_input_lower for word in ['éšæ®µ', 'ç¨‹åº¦', 'åš´é‡', 'è¼•åº¦', 'ä¸­åº¦', 'é‡åº¦']):
        logger.info(f"[DEBUG] ä½¿ç”¨ M2 ç—…ç¨‹éšæ®µè©•ä¼°")
        return m2_module.create_progression_card(user_input, m2_analysis)
    
    elif m4_analysis["detected_needs"]:
        logger.info(f"[DEBUG] ä½¿ç”¨ M4 ç…§è­·å°èˆªï¼Œæª¢æ¸¬åˆ°éœ€æ±‚ï¼š{m4_analysis['detected_needs']}")
        return m4_module.create_care_navigation_card(user_input, m4_analysis)
    
    # å¦‚æœéƒ½æ²’æœ‰åŒ¹é…ï¼Œä½¿ç”¨åŸæœ‰çš„æ¨¡çµ„é¸æ“‡é‚è¼¯
    else:
        # åˆ†æç”¨æˆ¶æ„åœ– - æ›´ç²¾ç¢ºçš„é—œéµå­—åˆ¤æ–·
        if any(word in user_input_lower for word in ['è¨˜æ†¶', 'å¿˜è¨˜', 'é‡è¤‡', 'è¨˜ä¸ä½', 'è¨˜æ€§']):
            component_type = "warning_sign"
            title = "è¨˜æ†¶åŠ›è­¦è¨Šåˆ†æ"
            color_theme = "warning"
            logger.info(f"[DEBUG] é¸æ“‡æ¨¡çµ„ï¼šè¨˜æ†¶åŠ›è­¦è¨Šåˆ†æ (é—œéµå­—: {[word for word in ['è¨˜æ†¶', 'å¿˜è¨˜', 'é‡è¤‡', 'è¨˜ä¸ä½', 'è¨˜æ€§'] if word in user_input_lower]})")
        elif any(word in user_input_lower for word in ['è¡Œç‚º', 'æƒ…ç·’', 'å¿ƒç†', 'æš´èº', 'å¹»è¦º', 'å¦„æƒ³', 'ç„¦æ…®', 'æ†‚é¬±', 'åµé¬§']):
            component_type = "bpsd_symptom"
            title = "è¡Œç‚ºå¿ƒç†ç—‡ç‹€åˆ†æ"
            color_theme = "neutral"
            logger.info(f"[DEBUG] é¸æ“‡æ¨¡çµ„ï¼šè¡Œç‚ºå¿ƒç†ç—‡ç‹€åˆ†æ (é—œéµå­—: {[word for word in ['è¡Œç‚º', 'æƒ…ç·’', 'å¿ƒç†', 'æš´èº', 'å¹»è¦º', 'å¦„æƒ³', 'ç„¦æ…®', 'æ†‚é¬±', 'åµé¬§'] if word in user_input_lower]})")
        elif any(word in user_input_lower for word in ['ç…§è­·', 'ç…§é¡§', 'å»ºè­°', 'å®¶å±¬', 'è­·ç†', 'æ³¨æ„']):
            component_type = "coping_strategy"
            title = "ç…§è­·å»ºè­°"
            color_theme = "success"
            logger.info(f"[DEBUG] é¸æ“‡æ¨¡çµ„ï¼šç…§è­·å»ºè­° (é—œéµå­—: {[word for word in ['ç…§è­·', 'ç…§é¡§', 'å»ºè­°', 'å®¶å±¬', 'è­·ç†', 'æ³¨æ„'] if word in user_input_lower]})")
        elif any(word in user_input_lower for word in ['ä¸æœƒç”¨', 'åšå®¶äº‹', 'ç”Ÿæ´»èƒ½åŠ›', 'æ´—è¡£æ©Ÿ', 'æ‰‹æ©Ÿ', 'ç…®é£¯', 'æ´—æ¾¡']):
            component_type = "daily_activity"
            title = "æ—¥å¸¸ç”Ÿæ´»èƒ½åŠ›è©•ä¼°"
            color_theme = "info"
            logger.info(f"[DEBUG] é¸æ“‡æ¨¡çµ„ï¼šæ—¥å¸¸ç”Ÿæ´»èƒ½åŠ›è©•ä¼° (é—œéµå­—: {[word for word in ['ä¸æœƒç”¨', 'åšå®¶äº‹', 'ç”Ÿæ´»èƒ½åŠ›', 'æ´—è¡£æ©Ÿ', 'æ‰‹æ©Ÿ', 'ç…®é£¯', 'æ´—æ¾¡'] if word in user_input_lower]})")
        else:
            component_type = "comprehensive"
            title = "å¤±æ™ºç—‡ç¶œåˆåˆ†æ"
            color_theme = "info"
            logger.info(f"[DEBUG] é¸æ“‡æ¨¡çµ„ï¼šå¤±æ™ºç—‡ç¶œåˆåˆ†æ (é è¨­)")
        
        # æ ¹æ“šæ¨¡çµ„é¡å‹ç”¢ç”Ÿä¸åŒçš„ body å…§å®¹
        if component_type == "warning_sign":
            body_contents = [
                {"type": "text", "text": "âš ï¸ è¨˜æ†¶åŠ›è­¦è¨Šï¼šè¿‘æœŸæœ‰æ˜é¡¯å¿˜è¨˜äº‹æƒ…ã€é‡è¤‡æå•ç­‰ç¾è±¡ï¼Œå»ºè­°åŠæ—©å°±é†«è©•ä¼°ã€‚", "weight": "bold", "size": "md", "color": "#d9534f", "wrap": True},
                {"type": "separator", "margin": "md"},
                {"type": "text", "text": f"ğŸ“ ç”¨æˆ¶æè¿°ï¼š{user_input}", "size": "sm", "color": "#666666", "wrap": True, "margin": "md"},
            ]
        elif component_type == "daily_activity":
            body_contents = [
                {"type": "text", "text": "ğŸ§© æ—¥å¸¸ç”Ÿæ´»èƒ½åŠ›ï¼šè¿‘æœŸåœ¨å®¶äº‹ã€ä½¿ç”¨å®¶é›»ã€ç”Ÿæ´»è‡ªç†ä¸Šå‡ºç¾å›°é›£ï¼Œå»ºè­°å®¶å±¬å¤šå”åŠ©ã€‚", "weight": "bold", "size": "md", "color": "#0275d8", "wrap": True},
                {"type": "separator", "margin": "md"},
                {"type": "text", "text": f"ğŸ“ ç”¨æˆ¶æè¿°ï¼š{user_input}", "size": "sm", "color": "#666666", "wrap": True, "margin": "md"},
            ]
        elif component_type == "bpsd_symptom":
            body_contents = [
                {"type": "text", "text": "ğŸ§  è¡Œç‚ºå¿ƒç†ç—‡ç‹€ï¼šè¿‘æœŸæœ‰æš´èºã€å¹»è¦ºã€å¦„æƒ³ã€æƒ…ç·’ä¸ç©©ç­‰ç¾è±¡ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­å”åŠ©ã€‚", "weight": "bold", "size": "md", "color": "#f0ad4e", "wrap": True},
                {"type": "separator", "margin": "md"},
                {"type": "text",
                "text": f"ğŸ“ ç”¨æˆ¶æè¿°ï¼š{user_input}", "size": "sm", "color": "#666666", "wrap": True, "margin": "md"},
            ]
        elif component_type == "coping_strategy":
            body_contents = [
                {"type": "text", "text": "ğŸ’¡ ç…§è­·å»ºè­°ï¼šä¿æŒè€å¿ƒã€å»ºç«‹è¦å¾‹ä½œæ¯ã€å–„ç”¨è¼”åŠ©å·¥å…·ï¼Œä¸¦å¤šèˆ‡é†«ç™‚åœ˜éšŠæºé€šã€‚", "weight": "bold", "size": "md", "color": "#5cb85c", "wrap": True},
                {"type": "separator", "margin": "md"},
                {"type": "text", "text": f"ğŸ“ ç”¨æˆ¶æè¿°ï¼š{user_input}", "size": "sm", "color": "#666666", "wrap": True, "margin": "md"},
            ]
        else:
            body_contents = [
                {"type": "text", "text": "ğŸ§  ç¶œåˆåˆ†æï¼šæ„Ÿè¬æ‚¨çš„æå•ï¼Œä»¥ä¸‹ç‚ºç¶œåˆåˆ†æçµæœã€‚", "weight": "bold", "size": "md", "color": "#005073", "wrap": True},
                {"type": "separator", "margin": "md"},
                {"type": "text", "text": f"ğŸ“ ç”¨æˆ¶æè¿°ï¼š{user_input}", "size": "sm", "color": "#666666", "wrap": True, "margin": "md"},
            ]

        flex_message = {
            "type": "flex",
            "altText": f"å¤±æ™ºç—‡åˆ†æï¼š{title}",
            "contents": {
                "type": "bubble",
                "size": "kilo",
                "header": {
                    "type": "box",
                    "layout": "vertical",
                    "contents": [
                        {
                            "type": "text",
                            "text": f"ğŸ§  {title}",
                            "weight": "bold",
                            "size": "lg",
                            "color": "#ffffff"
                        }
                    ],
                    "backgroundColor": "#005073",
                    "paddingAll": "15dp"
                },
                "body": {
                    "type": "box",
                    "layout": "vertical",
                    "contents": body_contents
                },
                "footer": {
                    "type": "box",
                    "layout": "horizontal",
                    "contents": [
                        {
                            "type": "button",
                            "style": "primary",
                            "height": "sm",
                            "action": {
                                "type": "message",
                                "label": "æ›´å¤šè³‡è¨Š",
                                "text": "è«‹æä¾›æ›´å¤šè©³ç´°è³‡è¨Š"
                            },
                            "flex": 1
                        }
                    ]
                }
            }
        }
        
        return flex_message


@handler.add(MessageEvent, message=TextMessageContent)
async def handle_message(event):
    logger.info("[DEBUG] handle_message è¢«å‘¼å«")
    try:
        user_input = event.message.text
        user_id = event.source.user_id
        logger.info(f"[DEBUG] event.message.text: {user_input}")
        logger.info(f"[DEBUG] event.source.user_id: {user_id}")
        logger.info(f"ğŸ“¨ æ”¶åˆ°ä¾†è‡ª {user_id} çš„è¨Šæ¯: {user_input}")
        
        # æª¢æŸ¥ reply token æ˜¯å¦æœ‰æ•ˆ
        if not event.reply_token or event.reply_token == "00000000000000000000000000000000":
            logger.error("[DEBUG] Reply token ç„¡æ•ˆæˆ–éæœŸ")
            return
        
        if integrated_engine:
            logger.info("[DEBUG] integrated_engine å¯ç”¨ï¼Œé–‹å§‹åˆ†æ...")
            
            # æª¢æŸ¥å¿«å–
            cached_result = None
            if cache_manager:
                cached_result = cache_manager.get_cached_analysis(user_input)
                if cached_result:
                    logger.info("[DEBUG] å¿«å–å‘½ä¸­ï¼Œä½¿ç”¨å¿«å–çµæœ")
                    result = cached_result
                else:
                    logger.info("[DEBUG] å¿«å–æœªå‘½ä¸­ï¼Œé€²è¡Œæ–°åˆ†æ")
                    result = integrated_engine.analyze_comprehensive(user_input)
                    # å¿«å–æ–°çµæœ
                    try:
                        cache_manager.cache_analysis_result(user_input, result)
                        logger.info("[DEBUG] æ–°åˆ†æçµæœå·²å¿«å–")
                    except Exception as cache_error:
                        logger.warning(f"[DEBUG] å¿«å–å¤±æ•—: {cache_error}")
            else:
                logger.info("[DEBUG] ç„¡å¿«å–ç®¡ç†å™¨ï¼Œç›´æ¥åˆ†æ")
                result = integrated_engine.analyze_comprehensive(user_input)
            
            logger.info(f"[DEBUG] åˆ†æçµæœé¡å‹: {type(result)}")
            
            # ç”Ÿæˆå›æ‡‰æ–‡å­—
            try:
                # å‰µå»ºç°¡å–®ä½†æœ‰æ•ˆçš„å›æ‡‰
                if result and isinstance(result, dict):
                    summary = result.get('comprehensive_summary', 'åˆ†æå®Œæˆ')
                    modules_used = result.get('modules_used', [])
                    chunks_found = len(result.get('retrieved_chunks', []))
                    
                    text_response = f"""ğŸ§  å¤±æ™ºç—‡åˆ†æçµæœ

ğŸ“‹ åˆ†ææ‘˜è¦ï¼š{summary}

ğŸ” æª¢æ¸¬åˆ°çš„æ¨¡çµ„ï¼š{', '.join(modules_used) if modules_used else 'ç„¡'}
ğŸ“Š ç›¸é—œçŸ¥è­˜ç‰‡æ®µï¼š{chunks_found} å€‹

ğŸ’¡ å»ºè­°ï¼šè«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡é€²è¡Œè©³ç´°è©•ä¼°
"""
                else:
                    text_response = "ğŸ§  å¤±æ™ºç—‡åˆ†æå®Œæˆ\n\nåˆ†æçµæœå·²æº–å‚™å¥½ï¼Œå»ºè­°è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡ã€‚"
                
                logger.info("[DEBUG] æ–‡å­—å›æ‡‰å‰µå»ºå®Œæˆ")
                
            except Exception as text_error:
                logger.warning(f"[DEBUG] æ–‡å­—å›æ‡‰å‰µå»ºå¤±æ•—: {text_error}")
                text_response = "ğŸ§  å¤±æ™ºç—‡åˆ†æå®Œæˆ\n\nåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
            
            # ç™¼é€å›æ‡‰ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰
            success = await send_line_message_with_retry(event.reply_token, text_response)
            
            if not success:
                # æœ€çµ‚å‚™ç”¨æ–¹æ¡ˆ
                await send_fallback_message(event.reply_token)
                
        else:
            response_text = "âŒ ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
            await send_line_message_with_retry(event.reply_token, response_text)
            
    except Exception as e:
        logger.error(f"âŒ è¨Šæ¯è™•ç†éŒ¯èª¤: {e}")
        import traceback
        logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        await send_fallback_message(event.reply_token)

async def send_line_message_with_retry(reply_token: str, text: str, max_retries: int = 3) -> bool:
    """ç™¼é€ LINE è¨Šæ¯ï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            logger.info(f"[DEBUG] å˜—è©¦ç™¼é€è¨Šæ¯ (ç¬¬ {attempt + 1} æ¬¡)")
            
            # å‰µå»ºæ–‡å­—è¨Šæ¯
            text_message = TextMessage(text=text)
            
            # è¨­ç½®è¶…æ™‚
            import asyncio
            await asyncio.wait_for(
                asyncio.to_thread(
                    line_bot_api.reply_message,
                    ReplyMessageRequest(
                        reply_token=reply_token,
                        messages=[text_message]
                    )
                ),
                timeout=LINE_TIMEOUT
            )
            
            logger.info("âœ… æ–‡å­—è¨Šæ¯ç™¼é€æˆåŠŸ")
            return True
            
        except asyncio.TimeoutError:
            logger.error(f"âŒ ç™¼é€è¶…æ™‚ (ç¬¬ {attempt + 1} æ¬¡)")
        except Exception as e:
            logger.error(f"âŒ ç™¼é€å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
        
        if attempt < max_retries - 1:
            await asyncio.sleep(1)  # ç­‰å¾… 1 ç§’å¾Œé‡è©¦
    
    logger.error("âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
    return False

async def send_fallback_message(reply_token: str):
    """ç™¼é€å‚™ç”¨è¨Šæ¯"""
    try:
        fallback_text = "ğŸ§  å¤±æ™ºç—‡åˆ†æå®Œæˆ\n\nåˆ†æçµæœå·²æº–å‚™å¥½ï¼Œè«‹ç¨å¾ŒæŸ¥çœ‹ã€‚"
        text_message = TextMessage(text=fallback_text)
        
        await asyncio.wait_for(
            asyncio.to_thread(
                line_bot_api.reply_message,
                ReplyMessageRequest(
                    reply_token=reply_token,
                    messages=[text_message]
                )
            ),
            timeout=10  # è¼ƒçŸ­çš„è¶…æ™‚æ™‚é–“
        )
        
        logger.info("âœ… å‚™ç”¨è¨Šæ¯ç™¼é€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ å‚™ç”¨è¨Šæ¯ç™¼é€å¤±æ•—: {e}")
        # æœ€å¾Œçš„éŒ¯èª¤è™•ç† - è¨˜éŒ„ä½†ä¸æ‹‹å‡ºç•°å¸¸
        logger.info("âœ… å·²å›è¦†ç”¨æˆ¶ç´”æ–‡å­—è¨Šæ¯ï¼ˆæœ€çµ‚å‚™ç”¨æ–¹æ¡ˆï¼‰")

@app.post("/webhook")
async def webhook(request: Request):
    logger.info("[DEBUG] /webhook endpoint è¢«å‘¼å«")
    try:
        body = await request.body()
        signature = request.headers.get("X-Line-Signature")
        logger.info(f"[DEBUG] X-Line-Signature: {signature}")
        logger.info(f"[DEBUG] webhook body: {body[:200]}")
        
        if not signature:
            logger.error("[DEBUG] ç¼ºå°‘ X-Line-Signature")
            return {"error": "ç¼ºå°‘ X-Line-Signature"}
        
        try:
            # è¨­ç½®è¶…æ™‚è™•ç†
            await asyncio.wait_for(
                asyncio.to_thread(handler.handle, body.decode(), signature),
                timeout=30  # 30 ç§’è¶…æ™‚
            )
            logger.info("[DEBUG] handler.handle å·²åŸ·è¡Œ")
            return {"message": "ok"}
            
        except asyncio.TimeoutError:
            logger.error("âŒ Webhook è™•ç†è¶…æ™‚")
            return {"error": "è™•ç†è¶…æ™‚"}
        except InvalidSignatureError:
            logger.error("âŒ ç„¡æ•ˆçš„ LINE ç°½å")
            return {"error": "ç„¡æ•ˆç°½å"}
        except Exception as handler_error:
            logger.error(f"âŒ Handler è™•ç†éŒ¯èª¤: {handler_error}")
            return {"error": str(handler_error)}
            
    except Exception as e:
        logger.error(f"âŒ Webhook è™•ç†éŒ¯èª¤: {e}")
        return {"error": str(e)}


@app.on_event("startup")
async def startup():
    global integrated_engine, cache_manager, optimized_gemini, line_bot_api, handler
    print("ğŸš€ å•Ÿå‹•å¢å¼·ç‰ˆ M1+M2+M3 æ•´åˆå¼•æ“...")

    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    check_env_variables()

    api_key = os.getenv("GEMINI_API_KEY")

    # åˆå§‹åŒ– LINE Bot
    if line_bot_api:
        print("âœ… LINE Bot åˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âš ï¸  LINE Bot æ¨¡çµ„æœªè¼‰å…¥")

    # åˆå§‹åŒ–å¿«å–ç®¡ç†å™¨
    if RedisCacheManager:
        try:
            cache_manager = RedisCacheManager()
            print("âœ… Redis å¿«å–ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Redis å¿«å–ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            cache_manager = None
    else:
        print("âš ï¸  Redis å¿«å–ç®¡ç†å™¨æœªè¼‰å…¥")
        cache_manager = None

    # åˆå§‹åŒ–å„ªåŒ– Gemini å®¢æˆ¶ç«¯
    if OptimizedGeminiClient and api_key:
        try:
            optimized_gemini = OptimizedGeminiClient(api_key)
            print("âœ… å„ªåŒ– Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å„ªåŒ– Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            optimized_gemini = None
    else:
        print("âš ï¸  å„ªåŒ– Gemini å®¢æˆ¶ç«¯æœªè¼‰å…¥")
        optimized_gemini = None

    # åˆå§‹åŒ–æ•´åˆå¼•æ“
    if M1M2M3IntegratedEngine:
        try:
            integrated_engine = M1M2M3IntegratedEngine()
            print("âœ… M1+M2+M3 æ•´åˆå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ M1+M2+M3 æ•´åˆå¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}")
            integrated_engine = None
    else:
        print("âš ï¸  M1+M2+M3 æ•´åˆå¼•æ“æœªè¼‰å…¥")
        integrated_engine = None

    print("âœ… å¢å¼·ç‰ˆ M1+M2+M3 æ•´åˆ API å•Ÿå‹•æˆåŠŸ")


class UserInput(BaseModel):
    user_input: str


@app.get("/")
def root():
    return {"message": "å¢å¼·ç‰ˆ M1+M2+M3 æ•´åˆ API", "status": "running"}


@app.get("/health")
def health():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    try:
        # æª¢æŸ¥å¼•æ“ç‹€æ…‹
        engine_info = {}
        if integrated_engine:
            try:
                # ä¿®æ­£å±¬æ€§è¨ªå• - ä½¿ç”¨æ­£ç¢ºçš„å±¬æ€§åç¨±
                total_chunks = len(getattr(integrated_engine, 'chunks', []))
                m1_chunks = len([c for c in getattr(integrated_engine, 'chunks', []) if c.get("chunk_id", "").startswith("M1")])
                m2_chunks = len([c for c in getattr(integrated_engine, 'chunks', []) if c.get("module_id") == "M2"])
                m3_chunks = len([c for c in getattr(integrated_engine, 'chunks', []) if c.get("module_id") == "M3"])
                vocabulary_size = len(getattr(integrated_engine, 'vocabulary', []))
                
                engine_info = {
                    "total_chunks": total_chunks,
                    "m1_chunks": m1_chunks,
                    "m2_chunks": m2_chunks,
                    "m3_chunks": m3_chunks,
                    "vocabulary_size": vocabulary_size
                }
            except Exception as e:
                engine_info = {"error": str(e)}

        # æª¢æŸ¥æ¨¡çµ„ç‹€æ…‹
        modules_status = {
            "M1": "active" if integrated_engine and any(c.get("chunk_id", "").startswith("M1") for c in getattr(integrated_engine, 'chunks', [])) else "inactive",
            "M2": "active" if integrated_engine and any(c.get("module_id") == "M2" for c in getattr(integrated_engine, 'chunks', [])) else "inactive",
            "M3": "active" if integrated_engine and any(c.get("module_id") == "M3" for c in getattr(integrated_engine, 'chunks', [])) else "inactive"
        }

        # æª¢æŸ¥å„ªåŒ–ç‹€æ…‹
        cache_stats = cache_manager.get_cache_stats() if cache_manager else {"status": "unavailable"}
        gemini_stats = optimized_gemini.get_usage_stats() if optimized_gemini else {"status": "unavailable"}

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "engine_info": engine_info,
            "modules_status": modules_status,
            "cache_stats": cache_stats,
            "gemini_stats": gemini_stats,
            "cost_optimization": {
                "cache_hit_rate": 0.0,
                "estimated_savings": 0.0,
                "total_cost": 0.0
            }
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}


# æ·»åŠ å€‹åˆ¥æ¨¡çµ„ç«¯é»
@app.post("/analyze/M1")
def analyze_m1(request: UserInput):
    """M1 è­¦è¨Šåˆ†æç«¯é»"""
    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}
    
    try:
        result = integrated_engine.analyze_comprehensive(request.user_input)
        # åªè¿”å› M1 ç›¸é—œçµæœ
        m1_chunks = [c for c in result.retrieved_chunks if c.get("chunk_id", "").startswith("M1")]
        return {
            "module": "M1",
            "matched_codes": [c.get("chunk_id") for c in m1_chunks],
            "symptom_titles": [c.get("title") for c in m1_chunks],
            "confidence_levels": [c.get("confidence_score", 0.0) for c in m1_chunks],
            "retrieved_chunks": m1_chunks
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/analyze/M2")
def analyze_m2(request: UserInput):
    """M2 ç—…ç¨‹åˆ†æç«¯é»"""
    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}
    
    try:
        result = integrated_engine.analyze_comprehensive(request.user_input)
        # åªè¿”å› M2 ç›¸é—œçµæœ
        m2_chunks = [c for c in result.retrieved_chunks if c.get("module_id") == "M2"]
        return {
            "module": "M2",
            "stage_detection": result.stage_detection,
            "retrieved_chunks": m2_chunks
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/analyze/M3")
def analyze_m3(request: UserInput):
    """M3 BPSD åˆ†æç«¯é»"""
    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}
    
    try:
        result = integrated_engine.analyze_comprehensive(request.user_input)
        # åªè¿”å› M3 ç›¸é—œçµæœ
        m3_chunks = [c for c in result.retrieved_chunks if c.get("module_id") == "M3"]
        return {
            "module": "M3",
            "bpsd_analysis": result.bpsd_analysis,
            "retrieved_chunks": m3_chunks
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/analyze/M4")
def analyze_m4(request: UserInput):
    """M4 ç…§è­·å°èˆªç«¯é»"""
    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}
    
    try:
        result = integrated_engine.analyze_comprehensive(request.user_input)
        return {
            "module": "M4",
            "action_suggestions": result.action_suggestions,
            "comprehensive_summary": result.comprehensive_summary
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/comprehensive-analysis")
def comprehensive_analysis(request: UserInput):
    """ç¶œåˆåˆ†æç«¯é»ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""

    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}

    try:
        user_input = request.user_input

        # æª¢æŸ¥å¿«å–
        cached_result = None
        if cache_manager:
            cached_result = cache_manager.get_cached_analysis(user_input)
            if cached_result:
                logger.info("âœ… åˆ†æçµæœå¿«å–å‘½ä¸­")
                return {
                    **cached_result,
                    "cached": True,
                    "optimized": True,
                    "cache_available": cache_manager.is_available()
                }

        # ä½¿ç”¨æ•´åˆå¼•æ“é€²è¡Œåˆ†æ
        result = integrated_engine.analyze_comprehensive(user_input)

        # å°‡çµæœè½‰æ›ç‚ºå­—å…¸æ ¼å¼ä»¥ä¾¿å¿«å–
        try:
            if hasattr(result, '__dict__'):
                result_dict = result.__dict__
            else:
                result_dict = result

            # ç¢ºä¿çµæœå¯ä»¥åºåˆ—åŒ–
            import json

            json.dumps(result_dict)
        except (TypeError, ValueError):
            # å¦‚æœç„¡æ³•åºåˆ—åŒ–ï¼Œè½‰æ›ç‚ºå­—ç¬¦ä¸²
            result_dict = {"result": str(result), "type": type(result).__name__}

        # å¿«å–çµæœ
        if cache_manager:
            try:
                cache_manager.cache_analysis_result(user_input, result_dict)
                logger.info("ğŸ’¾ åˆ†æçµæœå·²å¿«å–")
            except Exception as cache_error:
                logger.warning(f"âš ï¸  å¿«å–å¤±æ•—: {cache_error}")

        return {
            **result_dict,
            "cached": False,
            "optimized": True,
            "cache_available": cache_manager.is_available() if cache_manager else False,
        }

    except Exception as e:
        logger.error(f"âŒ ç¶œåˆåˆ†æéŒ¯èª¤: {e}")
        return {"error": f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}


@app.post("/professional-analysis")
async def professional_analysis(request: UserInput):
    """å°ˆæ¥­æ¨¡çµ„åŒ–åˆ†æç«¯é»"""
    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}
    
    try:
        logger.info(f"ğŸ¯ å°ˆæ¥­åˆ†æ: {request.user_input}")
        
        # åŸ·è¡ŒåŸºç¤åˆ†æ
        result = integrated_engine.analyze_comprehensive(request.user_input)
        
        # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        if hasattr(result, '__dict__'):
            result_dict = result.__dict__
        else:
            result_dict = result
        
        # åŸ·è¡Œå°ˆæ¥­æ¨¡çµ„åŒ–åˆ†æ
        context = {
            "user_input": request.user_input,
            "analysis_result": result_dict
        }
        
        professional_result = await professional_analyzer.analyze_professional(request.user_input, context)
        
        return {
            "status": "success",
            "professional_analysis": professional_result,
            "text_response": create_professional_text_response(professional_result)
        }
        
    except Exception as e:
        logger.error(f"å°ˆæ¥­åˆ†æéŒ¯èª¤: {e}")
        return {"error": str(e)}


@app.post("/m1-flex")
def analyze_with_flex(request: UserInput):
    """M1 Flex Message åˆ†æç«¯é»ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""

    if not integrated_engine:
        return {"error": "å¼•æ“æœªåˆå§‹åŒ–"}

    try:
        user_input = request.user_input

        # æª¢æŸ¥å¿«å–
        cached_flex = None
        if cache_manager:
            cached_flex = cache_manager.get_cached_flex_message(user_input)
            if cached_flex:
                logger.info("âœ… Flex Message å¿«å–å‘½ä¸­")
                return {"flex_message": cached_flex, "cached": True, "optimized": True}

        # ä½¿ç”¨æ•´åˆå¼•æ“é€²è¡Œåˆ†æ
        result = integrated_engine.analyze_comprehensive(user_input)

        # å°‡ AnalysisResult å°è±¡è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        if hasattr(result, '__dict__'):
            result_dict = result.__dict__
        else:
            result_dict = result

        # ç”Ÿæˆ Flex Message
        flex_message = create_smart_flex_message(user_input, result_dict)

        # å¿«å– Flex Message
        if cache_manager:
            cache_manager.cache_flex_message(user_input, flex_message)
            logger.info("ğŸ’¾ Flex Message å·²å¿«å–")

        return {
            "flex_message": flex_message,
            "comprehensive_analysis": result,
            "cached": False,
            "optimized": True,
            "cache_available": cache_manager.is_available() if cache_manager else False,
        }

    except Exception as e:
        logger.error(f"âŒ Flex Message ç”ŸæˆéŒ¯èª¤: {e}")
        return {"flex_message": create_error_flex_message()}


def create_comprehensive_flex_message(result, user_input: str) -> Dict:
    """å‰µå»ºç¶œåˆåˆ†æ Flex Messageï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""

    # æå–åˆ†æçµæœ - è™•ç†ä¸åŒæ ¼å¼çš„çµæœ
    if isinstance(result, dict):
        matched_codes = result.get("matched_codes", [])
        symptom_titles = result.get("symptom_titles", [])
        confidence_levels = result.get("confidence_levels", [])
        comprehensive_summary = result.get("comprehensive_summary", "åˆ†æå®Œæˆ")
        action_suggestions = result.get("action_suggestions", [])
    else:
        # å¦‚æœæ˜¯ AnalysisResult å°è±¡
        matched_codes = result.matched_codes if hasattr(result, 'matched_codes') else []
        symptom_titles = result.symptom_titles if hasattr(result, 'symptom_titles') else []
        confidence_levels = result.confidence_levels if hasattr(result, 'confidence_levels') else []
        comprehensive_summary = result.comprehensive_summary if hasattr(result, 'comprehensive_summary') else "åˆ†æå®Œæˆ"
        action_suggestions = result.action_suggestions if hasattr(result, 'action_suggestions') else []

    # ç”Ÿæˆä¸»è¦æ¨™é¡Œ
    main_title = "å¤±æ™ºç—‡ç¶œåˆåˆ†æ"
    if symptom_titles:
        main_title = f"å¤±æ™ºç—‡ç¶œåˆåˆ†æï¼š{symptom_titles[0]}"

    # ç”Ÿæˆ Flex Message
    flex_message = {
        "type": "flex",
        "altText": main_title,
        "contents": {
            "type": "bubble",
            "size": "kilo",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ§  M1+M2+M3 ç¶œåˆåˆ†æ",
                        "weight": "bold",
                        "size": "lg",
                        "color": "#ffffff"
                    }
                ],
                "backgroundColor": "#005073",
                "paddingAll": "15dp"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": comprehensive_summary,
                        "weight": "bold",
                        "size": "md",
                        "color": "#005073",
                        "wrap": True
                    },
                    {
                        "type": "separator",
                        "margin": "md"
                    },
                    {
                        "type": "box",
                        "layout": "vertical",
                        "margin": "md",
                        "contents": [
                            {
                                "type": "text",
                                "text": "ğŸ“ ç—‡ç‹€æè¿°",
                                "size": "sm",
                                "weight": "bold",
                                "color": "#666666"
                            },
                            {
                                "type": "text",
                                "text": user_input,
                                "size": "sm",
                                "weight": "regular",
                                "wrap": True,
                                "margin": "xs"
                            }
                        ]
                    }
                ]
            },
            "footer": {
                "type": "box",
                "layout": "horizontal",
                "contents": [
                    {
                        "type": "button",
                        "style": "primary",
                        "height": "sm",
                        "action": {
                            "type": "message",
                            "label": "æ›´å¤šè³‡è¨Š",
                            "text": "è«‹æä¾›æ›´å¤šè©³ç´°è³‡è¨Š"
                        },
                        "flex": 1
                    }
                ]
            }
        }
    }

    # æ·»åŠ ç—‡ç‹€åˆ†æ
    if symptom_titles:
        for i, title in enumerate(symptom_titles[:2]):
            code = matched_codes[i] if i < len(matched_codes) else f"M1-{i+1:02d}"
            confidence = confidence_levels[i] if i < len(confidence_levels) else "MEDIUM"
            
            symptom_box = {
                "type": "box",
                "layout": "vertical",
                "margin": "md",
                "contents": [
                    {
                        "type": "text",
                        "text": f"ğŸš¨ {title}",
                        "size": "sm",
                        "weight": "bold",
                        "color": "#005073",
                        "wrap": True
                    },
                    {
                        "type": "text",
                        "text": f"ä»£ç¢¼ï¼š{code} | ä¿¡å¿ƒï¼š{confidence}",
                        "size": "xs",
                        "weight": "regular",
                        "color": "#dc3545",
                        "margin": "xs"
                    }
                ]
            }
            flex_message["contents"]["body"]["contents"].append(symptom_box)

    # æ·»åŠ è¡Œå‹•å»ºè­°
    if action_suggestions:
        action_box = {
            "type": "box",
            "layout": "vertical",
            "margin": "lg",
            "contents": [
                {
                    "type": "text",
                    "text": "ğŸ’¡ å»ºè­°è¡Œå‹•",
                    "weight": "bold",
                    "size": "sm",
                    "color": "#4ECDC4"
                }
            ]
        }
        
        for suggestion in action_suggestions[:3]:
            action_box["contents"].append({
                "type": "text",
                "text": f"â€¢ {suggestion}",
                "size": "sm",
                "wrap": True,
                "margin": "xs"
            })
        
        flex_message["contents"]["body"]["contents"].append(action_box)

    return flex_message


def create_simple_flex_message(summary: str, user_input: str):
    """å‰µå»ºç°¡å–®çš„ Flex Message"""
    return {
        "type": "flex",
        "altText": "å¤±æ™ºç—‡åˆ†æçµæœ",
        "contents": {
            "type": "bubble",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ§  å¤±æ™ºç—‡åˆ†æ",
                        "weight": "bold",
                        "size": "lg",
                        "align": "center"
                    },
                    {
                        "type": "text",
                        "text": summary,
                        "wrap": True,
                        "margin": "md",
                        "size": "sm",
                        "color": "#666666"
                    },
                    {
                        "type": "text",
                        "text": f"ç”¨æˆ¶æè¿°ï¼š{user_input}",
                        "size": "xs",
                        "color": "#999999",
                        "wrap": True,
                        "margin": "md"
                    }
                ]
            }
        }
    }


def create_error_flex_message():
    """å‰µå»ºéŒ¯èª¤ Flex Message"""
    return {
        "type": "flex",
        "altText": "åˆ†æéŒ¯èª¤",
        "contents": {
            "type": "bubble",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "âŒ åˆ†æéŒ¯èª¤",
                        "weight": "bold",
                        "size": "lg",
                        "align": "center",
                        "color": "#dc3545"
                    },
                    {
                        "type": "text",
                        "text": "æŠ±æ­‰ï¼Œåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦æˆ–æä¾›æ›´å¤šè©³ç´°è³‡è¨Šã€‚",
                        "wrap": True,
                        "margin": "md",
                        "size": "sm",
                        "color": "#666666"
                    }
                ]
            }
        }
    }


@app.get("/cache/stats")
def get_cache_stats():
    """ç²å–å¿«å–çµ±è¨ˆ"""
    if cache_manager:
        return cache_manager.get_cache_stats()
    return {"status": "unavailable"}


@app.get("/gemini/stats")
def get_gemini_stats():
    """ç²å– Gemini çµ±è¨ˆ"""
    if optimized_gemini:
        return optimized_gemini.get_usage_stats()
    return {"status": "unavailable"}


@app.post("/cache/clear")
def clear_cache():
    """æ¸…é™¤å¿«å–"""
    if cache_manager:
        try:
            cache_manager.clear_all_cache()
            return {"message": "å¿«å–å·²æ¸…é™¤", "status": "success"}
        except Exception as e:
            return {"message": f"æ¸…é™¤å¿«å–å¤±æ•—: {str(e)}", "status": "error"}
    return {"message": "å¿«å–ç®¡ç†å™¨ä¸å¯ç”¨", "status": "unavailable"}


@app.get("/modules/status")
def modules_status():
    """ç²å–æ¨¡çµ„ç‹€æ…‹"""
    return {
        "integrated_engine": "active" if integrated_engine else "inactive",
        "cache_manager": "active" if cache_manager else "inactive",
        "optimized_gemini": "active" if optimized_gemini else "inactive",
        "line_bot_api": "active" if line_bot_api else "inactive"
    }


@app.post("/debug/flex-message")
def debug_flex_message(request: UserInput):
    """èª¿è©¦ Flex Message æ ¼å¼"""
    try:
        return {
            "status": "success",
            "message": "Debug endpoint working",
            "user_input": request.user_input
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }


def perform_xai_analysis(user_input: str, analysis_result: Any) -> Dict[str, Any]:
    """åŸ·è¡Œ XAI åˆ†æï¼Œæä¾›å¯è§£é‡‹çš„äººå·¥æ™ºæ…§åˆ†æ"""
    try:
        # åˆ†æç”¨æˆ¶è¼¸å…¥çš„é—œéµç‰¹å¾µ
        xai_features = {
            "symptom_keywords": [],
            "severity_level": "unknown",
            "confidence_score": 0.0,
            "explanation": "",
            "recommended_modules": []
        }
        
        # æå–é—œéµç—‡ç‹€è©å½™
        symptom_keywords = [
            "è¨˜æ†¶", "å¿˜è¨˜", "å¥å¿˜", "èªè¨€", "è¡¨é”", "è¿·è·¯", "æƒ…ç·’", 
            "æš´èº", "æ†‚é¬±", "ç„¦æ…®", "å¦„æƒ³", "å¹»è¦º", "éŠèµ°", "ç¡çœ ",
            "ä¸èªè­˜", "ä¸æœƒç”¨", "ç„¡æ³•", "å›°é›£", "æ··äº‚", "é€€åŒ–"
        ]
        
        found_keywords = [kw for kw in symptom_keywords if kw in user_input]
        xai_features["symptom_keywords"] = found_keywords
        
        # æ ¹æ“šé—œéµè©åˆ¤æ–·åš´é‡ç¨‹åº¦
        if any(kw in user_input for kw in ["é‡åº¦", "åš´é‡", "å®Œå…¨", "ä¸èªè­˜", "è‡¥åºŠ"]):
            xai_features["severity_level"] = "severe"
            xai_features["confidence_score"] = 0.8
        elif any(kw in user_input for kw in ["ä¸­åº¦", "æ˜é¡¯", "è¿·è·¯", "ä¸æœƒç”¨", "æš´èº"]):
            xai_features["severity_level"] = "moderate"
            xai_features["confidence_score"] = 0.7
        elif any(kw in user_input for kw in ["è¼•åº¦", "åˆæœŸ", "è¨˜æ†¶", "å¿˜è¨˜"]):
            xai_features["severity_level"] = "mild"
            xai_features["confidence_score"] = 0.6
        else:
            xai_features["severity_level"] = "unknown"
            xai_features["confidence_score"] = 0.5
        
        # ç”Ÿæˆè§£é‡‹
        if found_keywords:
            xai_features["explanation"] = f"æª¢æ¸¬åˆ°é—œéµç—‡ç‹€è©å½™: {', '.join(found_keywords)}ã€‚æ ¹æ“šç—‡ç‹€æè¿°ï¼Œè©•ä¼°ç‚º{xai_features['severity_level']}ç¨‹åº¦ã€‚"
        else:
            xai_features["explanation"] = "æœªæª¢æ¸¬åˆ°æ˜ç¢ºçš„ç—‡ç‹€é—œéµè©ï¼Œå»ºè­°æä¾›æ›´è©³ç´°çš„ç—‡ç‹€æè¿°ã€‚"
        
        # æ¨è–¦æ¨¡çµ„
        if "è¨˜æ†¶" in user_input or "å¿˜è¨˜" in user_input:
            xai_features["recommended_modules"].append("M1")
        if any(kw in user_input for kw in ["è¼•åº¦", "ä¸­åº¦", "é‡åº¦", "éšæ®µ"]):
            xai_features["recommended_modules"].append("M2")
        if any(kw in user_input for kw in ["æƒ…ç·’", "æš´èº", "æ†‚é¬±", "å¦„æƒ³", "å¹»è¦º"]):
            xai_features["recommended_modules"].append("M3")
        if any(kw in user_input for kw in ["ç…§è­·", "å”åŠ©", "å»ºè­°", "å¹«åŠ©"]):
            xai_features["recommended_modules"].append("M4")
        
        return xai_features
        
    except Exception as e:
        logger.error(f"XAI åˆ†æå¤±æ•—: {e}")
        return {
            "symptom_keywords": [],
            "severity_level": "unknown",
            "confidence_score": 0.0,
            "explanation": "XAI åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤",
            "recommended_modules": []
        }


def select_visual_module(user_input: str, xai_analysis: Dict[str, Any], analysis_result: Any) -> str:
    """æ ¹æ“š XAI åˆ†æçµæœé¸æ“‡æœ€é©åˆçš„è¦–è¦ºæ¨¡çµ„"""
    try:
        # æ ¹æ“šç—‡ç‹€é—œéµè©é¸æ“‡æ¨¡çµ„
        if any(kw in user_input for kw in ["è¨˜æ†¶", "å¿˜è¨˜", "å¥å¿˜"]):
            return "M1"  # è­¦è¨Šæ¨¡çµ„
        elif any(kw in user_input for kw in ["è¼•åº¦", "ä¸­åº¦", "é‡åº¦", "éšæ®µ", "ç—…ç¨‹"]):
            return "M2"  # ç—…ç¨‹æ¨¡çµ„
        elif any(kw in user_input for kw in ["æƒ…ç·’", "æš´èº", "æ†‚é¬±", "å¦„æƒ³", "å¹»è¦º", "éŠèµ°"]):
            return "M3"  # BPSD æ¨¡çµ„
        elif any(kw in user_input for kw in ["ç…§è­·", "å”åŠ©", "å»ºè­°", "å¹«åŠ©", "è³‡æº"]):
            return "M4"  # ç…§è­·æ¨¡çµ„
        else:
            # æ ¹æ“š XAI æ¨è–¦æ¨¡çµ„é¸æ“‡
            if xai_analysis.get("recommended_modules"):
                return xai_analysis["recommended_modules"][0]
            else:
                return "M1"  # é è¨­ä½¿ç”¨ M1 æ¨¡çµ„
                
    except Exception as e:
        logger.error(f"è¦–è¦ºæ¨¡çµ„é¸æ“‡å¤±æ•—: {e}")
        return "M1"  # é è¨­ä½¿ç”¨ M1 æ¨¡çµ„


def create_xai_visualization_module():
    """å‰µå»º XAI è¦–è¦ºåŒ–æ¨¡çµ„"""
    return {
        "M1": {
            "name": "è­¦è¨Šæª¢æ¸¬æ¨¡çµ„",
            "keywords": ["è¨˜æ†¶", "å¿˜è¨˜", "å¥å¿˜", "æ‰¾ä¸åˆ°", "éºå¤±", "é‡è¤‡", "åè¦†"],
            "visual_type": "warning_cards",
            "confidence_threshold": 0.6,
            "color_scheme": "#ff6b6b"
        },
        "M2": {
            "name": "ç—…ç¨‹è©•ä¼°æ¨¡çµ„", 
            "keywords": ["è¼•åº¦", "ä¸­åº¦", "é‡åº¦", "éšæ®µ", "ç—…ç¨‹", "æƒ¡åŒ–", "é€²æ­¥"],
            "visual_type": "progression_chart",
            "confidence_threshold": 0.7,
            "color_scheme": "#4ecdc4"
        },
        "M3": {
            "name": "BPSD ç—‡ç‹€æ¨¡çµ„",
            "keywords": ["æƒ…ç·’", "æš´èº", "æ†‚é¬±", "å¦„æƒ³", "å¹»è¦º", "éŠèµ°", "ç¡çœ ", "æ”»æ“Š"],
            "visual_type": "symptom_matrix",
            "confidence_threshold": 0.65,
            "color_scheme": "#45b7d1"
        },
        "M4": {
            "name": "ç…§è­·å°èˆªæ¨¡çµ„",
            "keywords": ["ç…§è­·", "å”åŠ©", "å»ºè­°", "å¹«åŠ©", "è³‡æº", "æœå‹™", "æ”¯æ´"],
            "visual_type": "care_navigation",
            "confidence_threshold": 0.5,
            "color_scheme": "#96ceb4"
        },
        "M5": {
            "name": "èªçŸ¥è©•ä¼°æ¨¡çµ„",
            "keywords": ["èªçŸ¥", "æ€è€ƒ", "åˆ¤æ–·", "ç†è§£", "å­¸ç¿’", "æ³¨æ„åŠ›"],
            "visual_type": "cognitive_assessment",
            "confidence_threshold": 0.6,
            "color_scheme": "#feca57"
        },
        "M6": {
            "name": "è¡Œç‚ºåˆ†ææ¨¡çµ„",
            "keywords": ["è¡Œç‚º", "å‹•ä½œ", "ç¿’æ…£", "åæ‡‰", "æ¨¡å¼", "æ”¹è®Š"],
            "visual_type": "behavior_analysis",
            "confidence_threshold": 0.55,
            "color_scheme": "#ff9ff3"
        },
        "M7": {
            "name": "ç’°å¢ƒé©æ‡‰æ¨¡çµ„",
            "keywords": ["ç’°å¢ƒ", "é©æ‡‰", "å®‰å…¨", "ç©ºé–“", "è¨­å‚™", "è¨­æ–½"],
            "visual_type": "environment_adaptation",
            "confidence_threshold": 0.5,
            "color_scheme": "#54a0ff"
        }
    }


def analyze_xai_visualization(user_input: str, analysis_result: Any) -> Dict[str, Any]:
    """XAI è¦–è¦ºåŒ–åˆ†æ - éµå¾ª Cursor IDE æŒ‡å—"""
    try:
        # ç²å–è¦–è¦ºåŒ–æ¨¡çµ„é…ç½®
        visual_modules = create_xai_visualization_module()
        
        # åˆå§‹åŒ–åˆ†æçµæœ
        xai_result = {
            "input_text": user_input,
            "detected_modules": [],
            "primary_module": None,
            "confidence_scores": {},
            "visual_recommendations": [],
            "explanation": "",
            "processing_time": 0
        }
        
        start_time = time.time()
        
        # åˆ†ææ¯å€‹æ¨¡çµ„çš„åŒ¹é…åº¦
        for module_id, module_config in visual_modules.items():
            confidence = calculate_module_confidence(user_input, module_config)
            xai_result["confidence_scores"][module_id] = confidence
            
            if confidence >= module_config["confidence_threshold"]:
                xai_result["detected_modules"].append({
                    "module_id": module_id,
                    "name": module_config["name"],
                    "confidence": confidence,
                    "visual_type": module_config["visual_type"],
                    "color_scheme": module_config["color_scheme"]
                })
        
        # é¸æ“‡ä¸»è¦æ¨¡çµ„
        if xai_result["detected_modules"]:
            primary = max(xai_result["detected_modules"], key=lambda x: x["confidence"])
            xai_result["primary_module"] = primary["module_id"]
            
            # ç”Ÿæˆè¦–è¦ºåŒ–å»ºè­°
            xai_result["visual_recommendations"] = generate_visual_recommendations(
                primary, analysis_result
            )
            
            # ç”Ÿæˆè§£é‡‹
            xai_result["explanation"] = generate_xai_explanation(
                user_input, primary, xai_result["detected_modules"]
            )
        
        xai_result["processing_time"] = time.time() - start_time
        
        return xai_result
        
    except Exception as e:
        logger.error(f"XAI è¦–è¦ºåŒ–åˆ†æå¤±æ•—: {e}")
        return {
            "input_text": user_input,
            "detected_modules": [],
            "primary_module": "M1",
            "confidence_scores": {"M1": 0.5},
            "visual_recommendations": [],
            "explanation": "XAI åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œä½¿ç”¨é è¨­æ¨¡çµ„",
            "processing_time": 0,
            "error": str(e)
        }


def calculate_module_confidence(user_input: str, module_config: Dict) -> float:
    """è¨ˆç®—æ¨¡çµ„åŒ¹é…åº¦ - ä½¿ç”¨é—œéµè©åˆ†æ"""
    try:
        user_input_lower = user_input.lower()
        keywords = module_config["keywords"]
        
        # è¨ˆç®—é—œéµè©åŒ¹é…
        matched_keywords = [kw for kw in keywords if kw in user_input_lower]
        match_ratio = len(matched_keywords) / len(keywords) if keywords else 0
        
        # æ ¹æ“šåŒ¹é…ç¨‹åº¦è¨ˆç®—ä¿¡å¿ƒåº¦
        if match_ratio >= 0.3:
            confidence = 0.8 + (match_ratio - 0.3) * 0.4  # 0.8-1.0
        elif match_ratio >= 0.1:
            confidence = 0.6 + (match_ratio - 0.1) * 1.0  # 0.6-0.8
        else:
            confidence = match_ratio * 6.0  # 0-0.6
        
        return min(confidence, 1.0)
        
    except Exception as e:
        logger.error(f"è¨ˆç®—æ¨¡çµ„ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
        return 0.5


def generate_visual_recommendations(primary_module: Dict, analysis_result: Any) -> List[Dict]:
    """ç”Ÿæˆè¦–è¦ºåŒ–å»ºè­°"""
    try:
        recommendations = []
        
        if primary_module["module_id"] == "M1":
            recommendations.append({
                "type": "warning_cards",
                "title": "å¤±æ™ºç—‡è­¦è¨Šæª¢æ¸¬",
                "description": "æª¢æ¸¬åˆ°å¯èƒ½çš„å¤±æ™ºç—‡è­¦è¨Šç—‡ç‹€",
                "priority": "high",
                "visual_elements": ["warning_icon", "symptom_list", "severity_indicator"]
            })
            
        elif primary_module["module_id"] == "M2":
            recommendations.append({
                "type": "progression_chart",
                "title": "ç—…ç¨‹éšæ®µè©•ä¼°",
                "description": "è©•ä¼°å¤±æ™ºç—‡ç—…ç¨‹ç™¼å±•éšæ®µ",
                "priority": "medium",
                "visual_elements": ["stage_indicator", "timeline", "care_focus"]
            })
            
        elif primary_module["module_id"] == "M3":
            recommendations.append({
                "type": "symptom_matrix",
                "title": "BPSD ç—‡ç‹€åˆ†æ",
                "description": "åˆ†æè¡Œç‚ºå¿ƒç†ç—‡ç‹€",
                "priority": "high",
                "visual_elements": ["symptom_grid", "severity_scale", "intervention_tips"]
            })
            
        elif primary_module["module_id"] == "M4":
            recommendations.append({
                "type": "care_navigation",
                "title": "ç…§è­·è³‡æºå°èˆª",
                "description": "æä¾›ç…§è­·å»ºè­°å’Œè³‡æº",
                "priority": "medium",
                "visual_elements": ["resource_list", "contact_info", "action_buttons"]
            })
        
        return recommendations
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆè¦–è¦ºåŒ–å»ºè­°å¤±æ•—: {e}")
        return []


def generate_xai_explanation(user_input: str, primary_module: Dict, all_modules: List[Dict]) -> str:
    """ç”Ÿæˆ XAI è§£é‡‹"""
    try:
        explanation_parts = []
        
        # ä¸»è¦æ¨¡çµ„è§£é‡‹
        explanation_parts.append(f"æª¢æ¸¬åˆ° {primary_module['name']} (ä¿¡å¿ƒåº¦: {primary_module['confidence']:.1%})")
        
        # å…¶ä»–æª¢æ¸¬åˆ°çš„æ¨¡çµ„
        other_modules = [m for m in all_modules if m["module_id"] != primary_module["module_id"]]
        if other_modules:
            other_names = [m["name"] for m in other_modules[:2]]  # æœ€å¤šé¡¯ç¤º2å€‹
            explanation_parts.append(f"åŒæ™‚æª¢æ¸¬åˆ°: {', '.join(other_names)}")
        
        # é—œéµè©è§£é‡‹
        visual_modules = create_xai_visualization_module()
        module_config = visual_modules[primary_module["module_id"]]
        matched_keywords = [kw for kw in module_config["keywords"] if kw in user_input.lower()]
        
        if matched_keywords:
            explanation_parts.append(f"é—œéµç—‡ç‹€è©å½™: {', '.join(matched_keywords[:3])}")
        
        return " | ".join(explanation_parts)
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆ XAI è§£é‡‹å¤±æ•—: {e}")
        return "XAI åˆ†æå®Œæˆ"


def create_enhanced_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»ºå¢å¼·ç‰ˆ Flex Message - æ•´åˆ XAI è¦–è¦ºåŒ–"""
    try:
        # ç²å–ä¸»è¦æ¨¡çµ„
        primary_module = xai_analysis.get("primary_module", "M1")
        visual_modules = create_xai_visualization_module()
        module_config = visual_modules.get(primary_module, visual_modules["M1"])
        
        # æ ¹æ“šæ¨¡çµ„é¡å‹å‰µå»ºä¸åŒçš„è¦–è¦ºåŒ–
        if primary_module == "M1":
            return create_m1_warning_flex_message(user_input, analysis_result, xai_analysis)
        elif primary_module == "M2":
            return create_m2_progression_flex_message(user_input, analysis_result, xai_analysis)
        elif primary_module == "M3":
            return create_m3_bpsd_flex_message(user_input, analysis_result, xai_analysis)
        elif primary_module == "M4":
            return create_m4_care_flex_message(user_input, analysis_result, xai_analysis)
        else:
            return create_default_flex_message(user_input, analysis_result, xai_analysis)
            
    except Exception as e:
        logger.error(f"å‰µå»ºå¢å¼·ç‰ˆ Flex Message å¤±æ•—: {e}")
        return create_error_flex_message()


def create_m1_warning_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»º M1 è­¦è¨Š Flex Message"""
    return {
        "type": "flex",
        "altText": "å¤±æ™ºç—‡è­¦è¨Šæª¢æ¸¬çµæœ",
        "contents": {
            "type": "bubble",
            "size": "kilo",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "âš ï¸ å¤±æ™ºç—‡è­¦è¨Šæª¢æ¸¬",
                        "weight": "bold",
                        "size": "lg",
                        "color": "#ffffff",
                        "align": "center"
                    }
                ],
                "backgroundColor": "#ff6b6b"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "spacing": "md",
                "contents": [
                    {
                        "type": "text",
                        "text": "æª¢æ¸¬åˆ°å¯èƒ½çš„å¤±æ™ºç—‡è­¦è¨Šç—‡ç‹€",
                        "weight": "bold",
                        "size": "sm",
                        "color": "#333333"
                    },
                    {
                        "type": "separator",
                        "margin": "md"
                    },
                    {
                        "type": "text",
                        "text": f"ä¿¡å¿ƒåº¦: {xai_analysis.get('confidence_scores', {}).get('M1', 0):.1%}",
                        "size": "xs",
                        "color": "#666666"
                    },
                    {
                        "type": "text",
                        "text": xai_analysis.get("explanation", ""),
                        "wrap": True,
                        "size": "xs",
                        "color": "#666666",
                        "margin": "md"
                    }
                ]
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "button",
                        "action": {
                            "type": "postback",
                            "label": "è©³ç´°åˆ†æ",
                            "data": f"analyze_detail:M1:{user_input}"
                        },
                        "style": "primary",
                        "color": "#ff6b6b"
                    }
                ]
            }
        }
    }


def create_m2_progression_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»º M2 ç—…ç¨‹ Flex Message"""
    return {
        "type": "flex",
        "altText": "å¤±æ™ºç—‡ç—…ç¨‹è©•ä¼°çµæœ",
        "contents": {
            "type": "bubble",
            "size": "kilo",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ“Š ç—…ç¨‹éšæ®µè©•ä¼°",
                        "weight": "bold",
                        "size": "lg",
                        "color": "#ffffff",
                        "align": "center"
                    }
                ],
                "backgroundColor": "#4ecdc4"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "spacing": "md",
                "contents": [
                    {
                        "type": "text",
                        "text": "è©•ä¼°å¤±æ™ºç—‡ç—…ç¨‹ç™¼å±•éšæ®µ",
                        "weight": "bold",
                        "size": "sm",
                        "color": "#333333"
                    },
                    {
                        "type": "separator",
                        "margin": "md"
                    },
                    {
                        "type": "text",
                        "text": f"ä¿¡å¿ƒåº¦: {xai_analysis.get('confidence_scores', {}).get('M2', 0):.1%}",
                        "size": "xs",
                        "color": "#666666"
                    },
                    {
                        "type": "text",
                        "text": xai_analysis.get("explanation", ""),
                        "wrap": True,
                        "size": "xs",
                        "color": "#666666",
                        "margin": "md"
                    }
                ]
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "button",
                        "action": {
                            "type": "postback",
                            "label": "æŸ¥çœ‹ç—…ç¨‹",
                            "data": f"analyze_detail:M2:{user_input}"
                        },
                        "style": "primary",
                        "color": "#4ecdc4"
                    }
                ]
            }
        }
    }


def create_m3_bpsd_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»º M3 BPSD Flex Message"""
    return {
        "type": "flex",
        "altText": "BPSD ç—‡ç‹€åˆ†æçµæœ",
        "contents": {
            "type": "bubble",
            "size": "kilo",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ§  BPSD ç—‡ç‹€åˆ†æ",
                        "weight": "bold",
                        "size": "lg",
                        "color": "#ffffff",
                        "align": "center"
                    }
                ],
                "backgroundColor": "#45b7d1"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "spacing": "md",
                "contents": [
                    {
                        "type": "text",
                        "text": "åˆ†æè¡Œç‚ºå¿ƒç†ç—‡ç‹€",
                        "weight": "bold",
                        "size": "sm",
                        "color": "#333333"
                    },
                    {
                        "type": "separator",
                        "margin": "md"
                    },
                    {
                        "type": "text",
                        "text": f"ä¿¡å¿ƒåº¦: {xai_analysis.get('confidence_scores', {}).get('M3', 0):.1%}",
                        "size": "xs",
                        "color": "#666666"
                    },
                    {
                        "type": "text",
                        "text": xai_analysis.get("explanation", ""),
                        "wrap": True,
                        "size": "xs",
                        "color": "#666666",
                        "margin": "md"
                    }
                ]
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "button",
                        "action": {
                            "type": "postback",
                            "label": "ç—‡ç‹€è©³æƒ…",
                            "data": f"analyze_detail:M3:{user_input}"
                        },
                        "style": "primary",
                        "color": "#45b7d1"
                    }
                ]
            }
        }
    }


def create_m4_care_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»º M4 ç…§è­· Flex Message"""
    return {
        "type": "flex",
        "altText": "ç…§è­·è³‡æºå°èˆªçµæœ",
        "contents": {
            "type": "bubble",
            "size": "kilo",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ¥ ç…§è­·è³‡æºå°èˆª",
                        "weight": "bold",
                        "size": "lg",
                        "color": "#ffffff",
                        "align": "center"
                    }
                ],
                "backgroundColor": "#96ceb4"
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "spacing": "md",
                "contents": [
                    {
                        "type": "text",
                        "text": "æä¾›ç…§è­·å»ºè­°å’Œè³‡æº",
                        "weight": "bold",
                        "size": "sm",
                        "color": "#333333"
                    },
                    {
                        "type": "separator",
                        "margin": "md"
                    },
                    {
                        "type": "text",
                        "text": f"ä¿¡å¿ƒåº¦: {xai_analysis.get('confidence_scores', {}).get('M4', 0):.1%}",
                        "size": "xs",
                        "color": "#666666"
                    },
                    {
                        "type": "text",
                        "text": xai_analysis.get("explanation", ""),
                        "wrap": True,
                        "size": "xs",
                        "color": "#666666",
                        "margin": "md"
                    }
                ]
            },
            "footer": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "button",
                        "action": {
                            "type": "postback",
                            "label": "ç…§è­·è³‡æº",
                            "data": f"analyze_detail:M4:{user_input}"
                        },
                        "style": "primary",
                        "color": "#96ceb4"
                    }
                ]
            }
        }
    }


def create_default_flex_message(user_input: str, analysis_result: Any, xai_analysis: Dict) -> Dict:
    """å‰µå»ºé è¨­ Flex Message"""
    return {
        "type": "flex",
        "altText": "å¤±æ™ºç—‡åˆ†æçµæœ",
        "contents": {
            "type": "bubble",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": "ğŸ§  å¤±æ™ºç—‡åˆ†æ",
                        "weight": "bold",
                        "size": "lg",
                        "align": "center"
                    },
                    {
                        "type": "text",
                        "text": xai_analysis.get("explanation", "åˆ†æå®Œæˆ"),
                        "wrap": True,
                        "margin": "md",
                        "size": "sm",
                        "color": "#666666"
                    }
                ]
            }
        }
    }


def create_jtbd_text_response(user_input: str, analysis_result: Any, xai_visualization: Dict) -> str:
    """æ ¹æ“š JTBD æ¶æ§‹å‰µå»ºæƒ…å¢ƒåŒ–çš„æ–‡å­—å›æ‡‰"""
    try:
        # ç²å–ä¸»è¦æ¨¡çµ„
        primary_module = xai_visualization.get("primary_module", "M1")
        confidence = xai_visualization.get("confidence_scores", {}).get(primary_module, 0)
        
        # åŸºç¤è³‡è¨Š
        summary = analysis_result.get('comprehensive_summary', 'åˆ†æå®Œæˆ')
        modules_used = analysis_result.get('modules_used', [])
        chunks_found = len(analysis_result.get('retrieved_chunks', []))
        
        # æ ¹æ“šæ¨¡çµ„å‰µå»ºä¸åŒçš„ JTBD å›æ‡‰
        if primary_module == "M1":
            return create_m1_jtbd_response(user_input, summary, confidence, chunks_found)
        elif primary_module == "M2":
            return create_m2_jtbd_response(user_input, summary, confidence, chunks_found)
        elif primary_module == "M3":
            return create_m3_jtbd_response(user_input, summary, confidence, chunks_found)
        elif primary_module == "M4":
            return create_m4_jtbd_response(user_input, summary, confidence, chunks_found)
        else:
            return create_default_jtbd_response(user_input, summary, modules_used, chunks_found)
            
    except Exception as e:
        logger.error(f"JTBD æ–‡å­—å›æ‡‰å‰µå»ºå¤±æ•—: {e}")
        return create_error_jtbd_response()


def create_m1_jtbd_response(user_input: str, summary: str, confidence: float, chunks_found: int) -> str:
    """M1: åå¤§è­¦è¨Šæ¯”å°å¡ - JTBD å›æ‡‰"""
    response = "âš ï¸ å¤±æ™ºç—‡è­¦è¨Šæª¢æ¸¬\n\n"
    
    # é™ä½ç„¦æ…®ï¼šæ˜ç¢ºçš„åˆ¤æ–·ä¾æ“š
    if confidence >= 0.8:
        response += "ğŸ” AI ä¿¡å¿ƒåº¦: é«˜ (90%+)\n"
        response += "ğŸ“Š åˆ†ææ‘˜è¦: " + summary + "\n\n"
        response += "ğŸ¯ å¿«é€Ÿåˆ¤æ–·:\n"
        response += "â€¢ ç—‡ç‹€ç¬¦åˆå¤±æ™ºç—‡è­¦è¨Š\n"
        response += "â€¢ å»ºè­°åŠæ—©å°±é†«è©•ä¼°\n\n"
    elif confidence >= 0.6:
        response += "ğŸ” AI ä¿¡å¿ƒåº¦: ä¸­ (60-80%)\n"
        response += "ğŸ“Š åˆ†ææ‘˜è¦: " + summary + "\n\n"
        response += "ğŸ¯ éœ€è¦è§€å¯Ÿ:\n"
        response += "â€¢ ç—‡ç‹€éœ€è¦é€²ä¸€æ­¥ç¢ºèª\n"
        response += "â€¢ å»ºè­°å®šæœŸè¿½è¹¤è¨˜éŒ„\n\n"
    else:
        response += "ğŸ” AI ä¿¡å¿ƒåº¦: ä½ (<60%)\n"
        response += "ğŸ“Š åˆ†ææ‘˜è¦: " + summary + "\n\n"
        response += "ğŸ¯ æŒçºŒè§€å¯Ÿ:\n"
        response += "â€¢ ç—‡ç‹€å°šä¸æ˜ç¢º\n"
        response += "â€¢ å»ºè­°è¨˜éŒ„è®ŠåŒ–\n\n"
    
    # å»ºç«‹ä¿¡ä»»ï¼šå±•ç¤ºæ¨ç†éç¨‹
    response += "ğŸ’¡ æ¨ç†è·¯å¾‘:\n"
    response += "1. ç—‡ç‹€è­˜åˆ¥ â†’ 2. è­¦è¨Šæ¯”å° â†’ 3. é¢¨éšªè©•ä¼°\n\n"
    
    # ä¿ƒé€²è¡Œå‹•ï¼šæ¸…æ¥šæŒ‡å¼•ä¸‹ä¸€æ­¥
    response += "ğŸ“‹ å»ºè­°è¡Œå‹•:\n"
    if confidence >= 0.8:
        response += "â€¢ ç«‹å³é ç´„ç¥ç¶“ç§‘é–€è¨º\n"
        response += "â€¢ æº–å‚™è©³ç´°ç—‡ç‹€è¨˜éŒ„\n"
        response += "â€¢ è¯ç¹«å®¶å±¬è¨è«–\n"
    elif confidence >= 0.6:
        response += "â€¢ è§€å¯Ÿç—‡ç‹€è®ŠåŒ–\n"
        response += "â€¢ è¨˜éŒ„ç•°å¸¸è¡Œç‚º\n"
        response += "â€¢ è€ƒæ…®åˆæ­¥ç¯©æª¢\n"
    else:
        response += "â€¢ æŒçºŒè§€å¯Ÿè¨˜éŒ„\n"
        response += "â€¢ å®šæœŸè©•ä¼°è¿½è¹¤\n"
        response += "â€¢ ä¿æŒæ­£å¸¸ç”Ÿæ´»\n"
    
    response += f"\nğŸ“Š æ‰¾åˆ°ç›¸é—œç‰‡æ®µ: {chunks_found} å€‹"
    return response


def create_m2_jtbd_response(user_input: str, summary: str, confidence: float, chunks_found: int) -> str:
    """M2: ç—…ç¨‹éšæ®µå°ç…§ - JTBD å›æ‡‰"""
    response = "ğŸ“ˆ ç—…ç¨‹éšæ®µè©•ä¼°\n\n"
    
    # å®šä½ç¾æ³ï¼šæ¸…æ¥šæ¨™ç¤ºç•¶å‰éšæ®µ
    response += "ğŸ¯ éšæ®µå®šä½:\n"
    if "è¼•åº¦" in summary or "æ—©æœŸ" in summary:
        response += "ğŸ“ ç•¶å‰éšæ®µ: è¼•åº¦å¤±æ™ºç—‡\n"
        response += "â° é ä¼°ç—…ç¨‹: 2-4å¹´\n"
        response += "ğŸ” ä¸»è¦ç‰¹å¾µ: è¨˜æ†¶åŠ›æ¸›é€€ã€è¼•å¾®èªçŸ¥éšœç¤™\n\n"
    elif "ä¸­åº¦" in summary or "ä¸­æœŸ" in summary:
        response += "ğŸ“ ç•¶å‰éšæ®µ: ä¸­åº¦å¤±æ™ºç—‡\n"
        response += "â° é ä¼°ç—…ç¨‹: 2-8å¹´\n"
        response += "ğŸ” ä¸»è¦ç‰¹å¾µ: æ˜é¡¯èªçŸ¥éšœç¤™ã€æ—¥å¸¸ç”Ÿæ´»éœ€å”åŠ©\n\n"
    elif "é‡åº¦" in summary or "æ™šæœŸ" in summary:
        response += "ğŸ“ ç•¶å‰éšæ®µ: é‡åº¦å¤±æ™ºç—‡\n"
        response += "â° é ä¼°ç—…ç¨‹: 1-3å¹´\n"
        response += "ğŸ” ä¸»è¦ç‰¹å¾µ: åš´é‡èªçŸ¥éšœç¤™ã€å®Œå…¨ä¾è³´ç…§è­·\n\n"
    else:
        response += "ğŸ“ éšæ®µè©•ä¼°: éœ€è¦é€²ä¸€æ­¥ç¢ºèª\n"
        response += "ğŸ“Š åˆ†ææ‘˜è¦: " + summary + "\n\n"
    
    # é æœŸç®¡ç†ï¼šå±•ç¤ºå¯èƒ½é€²å±•
    response += "ğŸ”„ ç—…ç¨‹é æœŸ:\n"
    response += "â€¢ ç—‡ç‹€æœƒé€æ¼¸é€²å±•\n"
    response += "â€¢ é€²å±•é€Ÿåº¦å› äººè€Œç•°\n"
    response += "â€¢ æ—©æœŸä»‹å…¥å¯å»¶ç·©æƒ¡åŒ–\n\n"
    
    # è³‡æºæº–å‚™ï¼šå„éšæ®µéœ€æ±‚æç¤º
    response += "ğŸ“‹ éšæ®µæº–å‚™:\n"
    if "è¼•åº¦" in summary or "æ—©æœŸ" in summary:
        response += "â€¢ å»ºç«‹é†«ç™‚åœ˜éšŠ\n"
        response += "â€¢ å­¸ç¿’ç…§è­·æŠ€å·§\n"
        response += "â€¢ è¦åŠƒæœªä¾†å®‰æ’\n"
    elif "ä¸­åº¦" in summary or "ä¸­æœŸ" in summary:
        response += "â€¢ ç”³è«‹ç…§è­·è³‡æº\n"
        response += "â€¢ èª¿æ•´å±…å®¶ç’°å¢ƒ\n"
        response += "â€¢ å°‹æ±‚å®¶å±¬æ”¯æŒ\n"
    elif "é‡åº¦" in summary or "æ™šæœŸ" in summary:
        response += "â€¢ 24å°æ™‚ç…§è­·å®‰æ’\n"
        response += "â€¢ å®‰å¯§ç…§è­·æº–å‚™\n"
        response += "â€¢ å®¶å±¬å¿ƒç†æ”¯æŒ\n"
    else:
        response += "â€¢ å®šæœŸé†«ç™‚è©•ä¼°\n"
        response += "â€¢ ç—‡ç‹€ç›£æ¸¬è¨˜éŒ„\n"
        response += "â€¢ è³‡æºè³‡è¨Šæ”¶é›†\n"
    
    response += f"\nğŸ“Š æ‰¾åˆ°ç›¸é—œç‰‡æ®µ: {chunks_found} å€‹"
    return response


def create_m3_jtbd_response(user_input: str, summary: str, confidence: float, chunks_found: int) -> str:
    """M3: BPSD ç²¾ç¥è¡Œç‚ºç—‡ç‹€ - JTBD å›æ‡‰"""
    response = "ğŸ§  BPSD ç—‡ç‹€åˆ†æ\n\n"
    
    # æ­£å¸¸åŒ–ï¼šé€™æ˜¯ç–¾ç—…ç—‡ç‹€
    response += "ğŸ’¡ ç—‡ç‹€ç†è§£:\n"
    response += "â€¢ é€™äº›æ˜¯ç–¾ç—…è¡¨ç¾ï¼Œä¸æ˜¯æ•…æ„\n"
    response += "â€¢ å¤§è…¦åŠŸèƒ½å—æå°è‡´è¡Œç‚ºæ”¹è®Š\n"
    response += "â€¢ ç—‡ç‹€æœƒéš¨ç—…ç¨‹è®ŠåŒ–\n\n"
    
    # ç—‡ç‹€åˆ†é¡å’Œè™•ç†
    if "æƒ…ç·’" in user_input or "æš´èº" in user_input or "ç™¼è„¾æ°£" in user_input:
        response += "ğŸ˜¤ æƒ…ç·’ç—‡ç‹€è™•ç†:\n"
        response += "â€¢ ä¿æŒå†·éœï¼Œé¿å…çˆ­åŸ·\n"
        response += "â€¢ è½‰ç§»æ³¨æ„åŠ›åˆ°æ„‰å¿«è©±é¡Œ\n"
        response += "â€¢ å»ºç«‹è¦å¾‹ä½œæ¯\n"
        response += "â€¢ è€ƒæ…®éŸ³æ¨‚ç™‚æ³•\n\n"
    elif "å¦„æƒ³" in user_input or "å¹»è¦º" in user_input:
        response += "ğŸ‘ï¸ å¦„æƒ³å¹»è¦ºè™•ç†:\n"
        response += "â€¢ ä¸è¦ç›´æ¥åé§\n"
        response += "â€¢ ç¢ºèªç’°å¢ƒå®‰å…¨\n"
        response += "â€¢ å°‹æ±‚é†«ç™‚è©•ä¼°\n"
        response += "â€¢ è€ƒæ…®è—¥ç‰©æ²»ç™‚\n\n"
    elif "éŠèµ°" in user_input or "èµ°å¤±" in user_input:
        response += "ğŸš¶ éŠèµ°è¡Œç‚ºè™•ç†:\n"
        response += "â€¢ å®‰è£é–€é–è­¦å ±\n"
        response += "â€¢ é…æˆ´èº«ä»½è­˜åˆ¥\n"
        response += "â€¢ é„°å±…ç¤¾å€å”åŠ©\n"
        response += "â€¢ è€ƒæ…®GPSè¿½è¹¤\n\n"
    elif "ç¡çœ " in user_input or "ä¸ç¡è¦º" in user_input:
        response += "ğŸ˜´ ç¡çœ å•é¡Œè™•ç†:\n"
        response += "â€¢ å»ºç«‹è¦å¾‹ä½œæ¯\n"
        response += "â€¢ é¿å…åˆå¾Œå°ç¡\n"
        response += "â€¢ ç‡Ÿé€ å®‰éœç’°å¢ƒ\n"
        response += "â€¢ é©åº¦æ—¥é–“æ´»å‹•\n\n"
    else:
        response += "ğŸ” ä¸€èˆ¬ç—‡ç‹€è™•ç†:\n"
        response += "â€¢ ä¿æŒè€å¿ƒå’Œç†è§£\n"
        response += "â€¢ å°‹æ±‚å°ˆæ¥­å»ºè­°\n"
        response += "â€¢ å»ºç«‹æ”¯æŒç¶²çµ¡\n"
        response += "â€¢ ç…§é¡§è€…è‡ªæˆ‘ç…§é¡§\n\n"
    
    # è³¦èƒ½ï¼šæä¾›å…·é«”æŠ€å·§
    response += "ğŸ’ª ç…§è­·æŠ€å·§:\n"
    response += "â€¢ ä½¿ç”¨ç°¡å–®æ˜ç¢ºçš„èªè¨€\n"
    response += "â€¢ ä¿æŒç’°å¢ƒç©©å®š\n"
    response += "â€¢ å»ºç«‹æ—¥å¸¸è¦å¾‹\n"
    response += "â€¢ å°‹æ±‚å°ˆæ¥­æ”¯æ´\n\n"
    
    # æ”¯æŒï¼šä½ ä¸¦ä¸å­¤å–®
    response += "ğŸ¤ æ”¯æŒè³‡æº:\n"
    response += "â€¢ å¤±æ™ºç—‡å”æœƒè«®è©¢\n"
    response += "â€¢ å®¶å±¬æ”¯æŒåœ˜é«”\n"
    response += "â€¢ å°ˆæ¥­ç…§è­·æœå‹™\n"
    response += "â€¢ ç·Šæ€¥è¯çµ¡è³‡è¨Š\n"
    
    response += f"\nğŸ“Š æ‰¾åˆ°ç›¸é—œç‰‡æ®µ: {chunks_found} å€‹"
    return response


def create_m4_jtbd_response(user_input: str, summary: str, confidence: float, chunks_found: int) -> str:
    """M4: ç…§è­·ä»»å‹™å°èˆª - JTBD å›æ‡‰"""
    response = "ğŸ—ºï¸ ç…§è­·ä»»å‹™å°èˆª\n\n"
    
    # çµæ§‹åŒ–ï¼šåˆ†é¡æ•´ç†ä»»å‹™
    response += "ğŸ“‹ ä»»å‹™åˆ†é¡:\n\n"
    
    # ç·Šæ€¥ä»»å‹™
    response += "ğŸš¨ ç·Šæ€¥ä»»å‹™:\n"
    response += "â€¢ é†«ç™‚è©•ä¼°å®‰æ’\n"
    response += "â€¢ å®‰å…¨ç’°å¢ƒæª¢æŸ¥\n"
    response += "â€¢ ç·Šæ€¥è¯çµ¡å»ºç«‹\n\n"
    
    # é‡è¦ä»»å‹™
    response += "â­ é‡è¦ä»»å‹™:\n"
    response += "â€¢ ç…§è­·è³‡æºç”³è«‹\n"
    response += "â€¢ æ³•å¾‹æ–‡ä»¶æº–å‚™\n"
    response += "â€¢ è²¡å‹™è¦åŠƒå®‰æ’\n\n"
    
    # ä¸€èˆ¬ä»»å‹™
    response += "ğŸ“ ä¸€èˆ¬ä»»å‹™:\n"
    response += "â€¢ æ—¥å¸¸ç…§è­·å­¸ç¿’\n"
    response += "â€¢ æ”¯æŒç¶²çµ¡å»ºç«‹\n"
    response += "â€¢ è‡ªæˆ‘ç…§é¡§å®‰æ’\n\n"
    
    # å„ªå…ˆæ’åºï¼šè¼•é‡ç·©æ€¥
    response += "ğŸ¯ å„ªå…ˆé †åº:\n"
    response += "1. ç¢ºä¿å®‰å…¨èˆ‡é†«ç™‚\n"
    response += "2. ç”³è«‹å¿…è¦è³‡æº\n"
    response += "3. å»ºç«‹ç…§è­·ç³»çµ±\n"
    response += "4. é•·æœŸè¦åŠƒæº–å‚™\n\n"
    
    # é€²åº¦è¿½è¹¤ï¼šæˆå°±æ„Ÿå»ºç«‹
    response += "ğŸ“ˆ é€²åº¦å»ºè­°:\n"
    response += "â€¢ æ¯é€±å®Œæˆ2-3é …ä»»å‹™\n"
    response += "â€¢ è¨˜éŒ„å®Œæˆé€²åº¦\n"
    response += "â€¢ æ…¶ç¥å°æˆå°±\n"
    response += "â€¢ å°‹æ±‚å”åŠ©ä¸å­¤å–®\n\n"
    
    # å€‹äººåŒ–å»ºè­°
    response += "ğŸ’¡ å€‹äººåŒ–å»ºè­°:\n"
    if "ç”³è«‹" in user_input or "è£œåŠ©" in user_input:
        response += "â€¢ æº–å‚™ç›¸é—œè­‰æ˜æ–‡ä»¶\n"
        response += "â€¢ è«®è©¢ç¤¾æœƒç¦åˆ©å–®ä½\n"
        response += "â€¢ äº†è§£ç”³è«‹æµç¨‹æ™‚ç¨‹\n"
    elif "é†«ç™‚" in user_input or "é†«ç”Ÿ" in user_input:
        response += "â€¢ å»ºç«‹é†«ç™‚åœ˜éšŠ\n"
        response += "â€¢ æº–å‚™ç—‡ç‹€è¨˜éŒ„\n"
        response += "â€¢ å®‰æ’å®šæœŸå›è¨º\n"
    elif "æ—¥å¸¸" in user_input or "ç”Ÿæ´»" in user_input:
        response += "â€¢ èª¿æ•´å±…å®¶ç’°å¢ƒ\n"
        response += "â€¢ å»ºç«‹æ—¥å¸¸è¦å¾‹\n"
        response += "â€¢ å­¸ç¿’ç…§è­·æŠ€å·§\n"
    else:
        response += "â€¢ æ ¹æ“šç—‡ç‹€èª¿æ•´ä»»å‹™\n"
        response += "â€¢ å®šæœŸè©•ä¼°éœ€æ±‚\n"
        response += "â€¢ ä¿æŒå½ˆæ€§èª¿æ•´\n"
    
    response += f"\nğŸ“Š æ‰¾åˆ°ç›¸é—œç‰‡æ®µ: {chunks_found} å€‹"
    return response


def create_default_jtbd_response(user_input: str, summary: str, modules_used: List[str], chunks_found: int) -> str:
    """é è¨­ JTBD å›æ‡‰"""
    response = "ğŸ§  å¤±æ™ºç—‡ç¶œåˆåˆ†æ\n\n"
    response += "ğŸ“Š åˆ†ææ‘˜è¦: " + summary + "\n\n"
    
    if modules_used:
        response += "ğŸ” ä½¿ç”¨æ¨¡çµ„: " + ", ".join(modules_used) + "\n"
    response += f"ğŸ“‹ æ‰¾åˆ°ç›¸é—œç‰‡æ®µ: {chunks_found} å€‹\n\n"
    
    response += "ğŸ’¡ å»ºè­°è¡Œå‹•:\n"
    response += "â€¢ æä¾›æ›´å¤šè©³ç´°ç—‡ç‹€æè¿°\n"
    response += "â€¢ èªªæ˜å…·é«”å›°æ“¾æƒ…æ³\n"
    response += "â€¢ è©¢å•ç‰¹å®šç…§è­·éœ€æ±‚\n\n"
    
    response += "ğŸ¯ ä¸‹ä¸€æ­¥:\n"
    response += "â€¢ æˆ‘å€‘æœƒæ ¹æ“šæ‚¨çš„æè¿°\n"
    response += "â€¢ æä¾›æ›´ç²¾æº–çš„åˆ†æ\n"
    response += "â€¢ çµ¦å‡ºå…·é«”çš„å»ºè­°\n"
    
    return response


def create_professional_text_response(professional_result: Dict[str, Any]) -> str:
    """å‰µå»ºå°ˆæ¥­æ¨¡çµ„åŒ–åˆ†æå›æ‡‰"""
    try:
        selected_modules = professional_result.get("selected_modules", [])
        best_answer = professional_result.get("best_answer", "")
        verification = professional_result.get("verification", {})
        comprehensive_score = professional_result.get("comprehensive_score", 0)
        xai_visualization = professional_result.get("xai_visualization", {})
        
        response = "ğŸ§  å°ˆæ¥­å¤±æ™ºç—‡åˆ†æç³»çµ±\n\n"
        
        # æ¨¡çµ„åˆ†æçµæœ
        response += "ğŸ“Š å°ˆæ¥­æ¨¡çµ„åˆ†æ:\n"
        for module in selected_modules:
            module_info = professional_result.get("analysis_results", {}).get(module, {})
            if module == "M1":
                warning_signs = module_info.get("warning_signs_detected", [])
                risk_level = module_info.get("risk_level", "low")
                response += f"â€¢ M1 è­¦è¨Šæª¢æ¸¬: ç™¼ç¾ {len(warning_signs)} å€‹è­¦è¨Š (é¢¨éšªç­‰ç´š: {risk_level})\n"
            elif module == "M2":
                current_stage = module_info.get("current_stage", "æœªçŸ¥")
                response += f"â€¢ M2 ç—…ç¨‹è©•ä¼°: ç•¶å‰éšæ®µ {current_stage}\n"
            elif module == "M3":
                total_symptoms = module_info.get("total_symptoms", 0)
                response += f"â€¢ M3 BPSD åˆ†æ: æª¢æ¸¬åˆ° {total_symptoms} å€‹ç—‡ç‹€\n"
            elif module == "M4":
                matched_resources = module_info.get("matched_resources", [])
                response += f"â€¢ M4 è³‡æºå°èˆª: åŒ¹é…åˆ° {len(matched_resources)} å€‹è³‡æº\n"
        
        response += "\n"
        
        # æœ€ä½³ç­”æ¡ˆ
        response += "ğŸ’¡ å°ˆæ¥­å»ºè­°:\n"
        response += best_answer + "\n\n"
        
        # å“è³ªé©—è­‰
        overall_score = verification.get("overall_score", 0)
        recommendation = verification.get("recommendation", "")
        response += f"ğŸ” å“è³ªé©—è­‰: {overall_score:.1%} ({recommendation})\n\n"
        
        # XAI è¦–è¦ºåŒ–æ‘˜è¦
        if xai_visualization:
            reasoning_path = xai_visualization.get("reasoning_path", {})
            steps = reasoning_path.get("steps", [])
            response += "ğŸ¯ AI æ¨ç†è·¯å¾‘:\n"
            for step in steps[:3]:  # åªé¡¯ç¤ºå‰3æ­¥
                response += f"â€¢ {step['action']}: {step['description']}\n"
            response += "\n"
        
        # ç¶œåˆè©•åˆ†
        response += f"ğŸ“ˆ ç¶œåˆè©•åˆ†: {comprehensive_score:.1%}\n"
        response += f"ğŸ¯ é¸æ“‡ç†ç”±: {professional_result.get('selection_reason', '')}\n\n"
        
        # ä¸‹ä¸€æ­¥å»ºè­°
        response += "ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:\n"
        if "M1" in selected_modules:
            response += "â€¢ ç«‹å³é ç´„ç¥ç¶“ç§‘é–€è¨ºé€²è¡Œå°ˆæ¥­è©•ä¼°\n"
        if "M2" in selected_modules:
            response += "â€¢ å®šæœŸè¿½è¹¤ç—‡ç‹€è®ŠåŒ–ä¸¦è¨˜éŒ„é€²å±•\n"
        if "M3" in selected_modules:
            response += "â€¢ å°‹æ±‚å°ˆæ¥­è¡Œç‚ºæ²»ç™‚å’Œè—¥ç‰©è«®è©¢\n"
        if "M4" in selected_modules:
            response += "â€¢ æŒ‰å„ªå…ˆé †åºç”³è«‹ç›¸é—œç…§è­·è³‡æº\n"
        
        return response
        
    except Exception as e:
        logger.error(f"å°ˆæ¥­å›æ‡‰å‰µå»ºå¤±æ•—: {e}")
        return "ğŸ§  å°ˆæ¥­å¤±æ™ºç—‡åˆ†æå®Œæˆ\n\nåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"


def create_error_jtbd_response() -> str:
    """éŒ¯èª¤æ™‚çš„ JTBD å›æ‡‰"""
    return """ğŸ§  å¤±æ™ºç—‡åˆ†æç³»çµ±

âŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤

ğŸ’¡ è«‹ç¨å¾Œå†è©¦æˆ–ï¼š
â€¢ é‡æ–°æè¿°ç—‡ç‹€
â€¢ æä¾›æ›´å¤šè©³ç´°è³‡è¨Š
â€¢ è¯ç¹«æŠ€è¡“æ”¯æ´

ğŸ¤ æˆ‘å€‘æœƒæŒçºŒæ”¹é€²æœå‹™å“è³ª"""


# æ–°å¢é€²éš AI æŠ€è¡“æ¨¡çµ„
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
import time
import random

# Aspect Verifiers å¤šè§’åº¦ç­”æ¡ˆé©—è­‰
class AspectVerifier:
    """å¤šè§’åº¦ç­”æ¡ˆé©—è­‰å™¨"""
    
    def __init__(self):
        self.aspects = {
            "medical_accuracy": "é†«å­¸æº–ç¢ºæ€§é©—è­‰",
            "safety_assessment": "å®‰å…¨æ€§è©•ä¼°", 
            "feasibility_analysis": "å¯è¡Œæ€§åˆ†æ",
            "emotional_appropriateness": "æƒ…æ„Ÿé©åˆ‡æ€§æª¢æŸ¥"
        }
    
    async def verify_answer(self, answer: str, context: Dict) -> Dict[str, Any]:
        """å¤šè§’åº¦é©—è­‰ç­”æ¡ˆ"""
        verification_results = {}
        
        for aspect, description in self.aspects.items():
            score = await self._verify_aspect(aspect, answer, context)
            verification_results[aspect] = {
                "description": description,
                "score": score,
                "status": "pass" if score >= 0.7 else "warning" if score >= 0.5 else "fail"
            }
        
        # è¨ˆç®—ç¶œåˆè©•åˆ†
        overall_score = sum(result["score"] for result in verification_results.values()) / len(verification_results)
        
        return {
            "overall_score": overall_score,
            "aspects": verification_results,
            "recommendation": self._get_recommendation(overall_score)
        }
    
    async def _verify_aspect(self, aspect: str, answer: str, context: Dict) -> float:
        """é©—è­‰ç‰¹å®šè§’åº¦"""
        # æ¨¡æ“¬é©—è­‰é‚è¼¯
        if aspect == "medical_accuracy":
            return self._verify_medical_accuracy(answer, context)
        elif aspect == "safety_assessment":
            return self._verify_safety(answer, context)
        elif aspect == "feasibility_analysis":
            return self._verify_feasibility(answer, context)
        elif aspect == "emotional_appropriateness":
            return self._verify_emotional_appropriateness(answer, context)
        return 0.8  # é è¨­åˆ†æ•¸
    
    def _verify_medical_accuracy(self, answer: str, context: Dict) -> float:
        """é†«å­¸æº–ç¢ºæ€§é©—è­‰"""
        medical_keywords = ["å¤±æ™ºç—‡", "é˜¿èŒ²æµ·é»˜", "èªçŸ¥éšœç¤™", "ç¥ç¶“ç§‘", "é†«ç™‚è©•ä¼°"]
        score = 0.8
        for keyword in medical_keywords:
            if keyword in answer:
                score += 0.1
        return min(score, 1.0)
    
    def _verify_safety(self, answer: str, context: Dict) -> float:
        """å®‰å…¨æ€§è©•ä¼°"""
        safety_keywords = ["å®‰å…¨", "ç·Šæ€¥", "å°ˆæ¥­", "è©•ä¼°", "ç›£æ¸¬"]
        risk_keywords = ["è‡ªè¡Œ", "ç«‹å³", "å¿«é€Ÿ", "ç°¡å–®"]
        
        score = 0.8
        for keyword in safety_keywords:
            if keyword in answer:
                score += 0.05
        for keyword in risk_keywords:
            if keyword in answer:
                score -= 0.1
        return max(score, 0.0)
    
    def _verify_feasibility(self, answer: str, context: Dict) -> float:
        """å¯è¡Œæ€§åˆ†æ"""
        feasibility_keywords = ["ç”³è«‹", "è¯çµ¡", "é ç´„", "æº–å‚™", "å®‰æ’"]
        score = 0.7
        for keyword in feasibility_keywords:
            if keyword in answer:
                score += 0.05
        return min(score, 1.0)
    
    def _verify_emotional_appropriateness(self, answer: str, context: Dict) -> float:
        """æƒ…æ„Ÿé©åˆ‡æ€§æª¢æŸ¥"""
        positive_keywords = ["æ”¯æŒ", "ç†è§£", "è€å¿ƒ", "å°ˆæ¥­", "å”åŠ©"]
        negative_keywords = ["åš´é‡", "å±éšª", "ç·Šæ€¥", "æƒ¡åŒ–"]
        
        score = 0.8
        for keyword in positive_keywords:
            if keyword in answer:
                score += 0.05
        for keyword in negative_keywords:
            if keyword in answer:
                score -= 0.1
        return max(score, 0.0)
    
    def _get_recommendation(self, overall_score: float) -> str:
        """æ ¹æ“šç¶œåˆè©•åˆ†çµ¦å‡ºå»ºè­°"""
        if overall_score >= 0.8:
            return "é«˜å“è³ªå›ç­”ï¼Œå¯ç›´æ¥ä½¿ç”¨"
        elif overall_score >= 0.6:
            return "å“è³ªè‰¯å¥½ï¼Œå»ºè­°å°å¹…èª¿æ•´"
        elif overall_score >= 0.4:
            return "å“è³ªä¸€èˆ¬ï¼Œéœ€è¦æ”¹é€²"
        else:
            return "å“è³ªè¼ƒå·®ï¼Œå»ºè­°é‡æ–°ç”Ÿæˆ"


# BoN-MAV æœ€ä½³ç­”æ¡ˆé¸æ“‡æ©Ÿåˆ¶
class BoNMAV:
    """Best of N - Multiple Answer Verification"""
    
    def __init__(self, n_candidates: int = 5):
        self.n_candidates = n_candidates
        self.aspect_verifier = AspectVerifier()
    
    async def generate_best_answer(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ä½³ç­”æ¡ˆ"""
        # ç”Ÿæˆå¤šå€‹å€™é¸ç­”æ¡ˆ
        candidates = await self._generate_candidates(user_input, context)
        
        # å¤šç¶­åº¦è©•åˆ†
        scored_candidates = []
        for i, candidate in enumerate(candidates):
            verification = await self.aspect_verifier.verify_answer(candidate, context)
            score = self._calculate_comprehensive_score(candidate, verification, context)
            scored_candidates.append({
                "candidate_id": i,
                "answer": candidate,
                "verification": verification,
                "comprehensive_score": score
            })
        
        # é¸æ“‡æœ€ä½³ç­”æ¡ˆ
        best_candidate = max(scored_candidates, key=lambda x: x["comprehensive_score"])
        
        return {
            "best_answer": best_candidate["answer"],
            "verification": best_candidate["verification"],
            "comprehensive_score": best_candidate["comprehensive_score"],
            "all_candidates": scored_candidates,
            "selection_reason": self._get_selection_reason(best_candidate)
        }
    
    async def _generate_candidates(self, user_input: str, context: Dict) -> List[str]:
        """ç”Ÿæˆå€™é¸ç­”æ¡ˆ"""
        # æ ¹æ“šä¸åŒæ¨¡çµ„ç”Ÿæˆå€™é¸ç­”æ¡ˆ
        candidates = []
        
        # M1 å€™é¸ç­”æ¡ˆ
        if "è¨˜æ†¶" in user_input or "å¿˜è¨˜" in user_input:
            candidates.extend([
                "æ ¹æ“šç—‡ç‹€æè¿°ï¼Œå»ºè­°é€²è¡Œå°ˆæ¥­é†«ç™‚è©•ä¼°ä»¥ç¢ºèªæ˜¯å¦ç‚ºå¤±æ™ºç—‡è­¦è¨Šã€‚",
                "è§€å¯Ÿåˆ°çš„ç—‡ç‹€éœ€è¦é€²ä¸€æ­¥ç¢ºèªï¼Œå»ºè­°è¨˜éŒ„è©³ç´°ç—‡ç‹€ä¸¦è«®è©¢ç¥ç¶“ç§‘é†«å¸«ã€‚",
                "é€™äº›ç—‡ç‹€å¯èƒ½èˆ‡æ­£å¸¸è€åŒ–ç›¸é—œï¼Œä½†å»ºè­°å®šæœŸè¿½è¹¤è§€å¯Ÿã€‚"
            ])
        
        # M2 å€™é¸ç­”æ¡ˆ
        if "éšæ®µ" in user_input or "é€²å±•" in user_input:
            candidates.extend([
                "æ ¹æ“šæè¿°ï¼Œç›®å‰è™•æ–¼è¼•åº¦éšæ®µï¼Œå»ºè­°åŠæ—©ä»‹å…¥ä»¥å»¶ç·©æƒ¡åŒ–ã€‚",
                "ç—‡ç‹€é¡¯ç¤ºå¯èƒ½é€²å…¥ä¸­åº¦éšæ®µï¼Œéœ€è¦èª¿æ•´ç…§è­·ç­–ç•¥å’Œç’°å¢ƒã€‚",
                "éœ€è¦å°ˆæ¥­è©•ä¼°ä»¥ç¢ºå®šå…·é«”éšæ®µå’Œé å¾Œã€‚"
            ])
        
        # M3 å€™é¸ç­”æ¡ˆ
        if "æƒ…ç·’" in user_input or "è¡Œç‚º" in user_input:
            candidates.extend([
                "é€™äº›è¡Œç‚ºæ˜¯ç–¾ç—…è¡¨ç¾ï¼Œå»ºè­°ä¿æŒè€å¿ƒä¸¦å°‹æ±‚å°ˆæ¥­å”åŠ©ã€‚",
                "å»ºç«‹è¦å¾‹ä½œæ¯å’Œç©©å®šç’°å¢ƒæœ‰åŠ©æ–¼æ”¹å–„ç—‡ç‹€ã€‚",
                "è€ƒæ…®è—¥ç‰©æ²»ç™‚å’Œè¡Œç‚ºç™‚æ³•ç›¸çµåˆçš„ç¶œåˆæ–¹æ¡ˆã€‚"
            ])
        
        # M4 å€™é¸ç­”æ¡ˆ
        if "ç”³è«‹" in user_input or "è³‡æº" in user_input:
            candidates.extend([
                "å»ºè­°æŒ‰å„ªå…ˆé †åºç”³è«‹ç›¸é—œç…§è­·è³‡æºå’Œè£œåŠ©ã€‚",
                "æº–å‚™å®Œæ•´æ–‡ä»¶ä¸¦è«®è©¢ç¤¾æœƒç¦åˆ©å–®ä½ä»¥ç²å¾—æœ€å¤§å”åŠ©ã€‚",
                "å»ºç«‹æ”¯æŒç¶²çµ¡ä¸¦å­¸ç¿’ç…§è­·æŠ€å·§ä»¥æå‡ç”Ÿæ´»å“è³ªã€‚"
            ])
        
        # ç¢ºä¿è‡³å°‘æœ‰ 5 å€‹å€™é¸ç­”æ¡ˆ
        while len(candidates) < self.n_candidates:
            candidates.append("å»ºè­°æä¾›æ›´å¤šè©³ç´°è³‡è¨Šä»¥ç²å¾—æ›´ç²¾æº–çš„åˆ†æå’Œå»ºè­°ã€‚")
        
        return candidates[:self.n_candidates]
    
    def _calculate_comprehensive_score(self, answer: str, verification: Dict, context: Dict) -> float:
        """è¨ˆç®—ç¶œåˆè©•åˆ†"""
        # åŸºç¤åˆ†æ•¸
        base_score = verification["overall_score"]
        
        # é•·åº¦é©ä¸­åŠ åˆ†
        length_score = 0.1 if 50 <= len(answer) <= 200 else 0.0
        
        # å°ˆæ¥­æ€§åŠ åˆ†
        professional_keywords = ["å°ˆæ¥­", "é†«å¸«", "è©•ä¼°", "å»ºè­°", "è«®è©¢"]
        professional_score = sum(0.02 for keyword in professional_keywords if keyword in answer)
        
        # å¯¦ç”¨æ€§åŠ åˆ†
        practical_keywords = ["å»ºè­°", "å¯ä»¥", "æ‡‰è©²", "éœ€è¦", "æº–å‚™"]
        practical_score = sum(0.02 for keyword in practical_keywords if keyword in answer)
        
        return min(base_score + length_score + professional_score + practical_score, 1.0)
    
    def _get_selection_reason(self, best_candidate: Dict) -> str:
        """ç²å–é¸æ“‡ç†ç”±"""
        score = best_candidate["comprehensive_score"]
        if score >= 0.9:
            return "ç¶œåˆè©•åˆ†æœ€é«˜ï¼Œå“è³ªå„ªç•°"
        elif score >= 0.8:
            return "å¤šç¶­åº¦è©•åˆ†å„ªç§€ï¼Œå»ºè­°æœ€ä½³"
        elif score >= 0.7:
            return "å¹³è¡¡æ€§è‰¯å¥½ï¼Œå¯¦ç”¨æ€§å¼·"
        else:
            return "ç›¸å°æœ€ä½³é¸æ“‡ï¼Œå»ºè­°é€²ä¸€æ­¥å„ªåŒ–"


# XAI è¦–è¦ºåŒ–å¢å¼·
class XAIVisualization:
    """å¯è§£é‡‹ AI è¦–è¦ºåŒ–"""
    
    def __init__(self):
        self.visualization_types = {
            "reasoning_path": "æ¨ç†è·¯å¾‘åœ–",
            "confidence_radar": "ä¿¡å¿ƒåˆ†æ•¸é›·é”åœ–", 
            "evidence_highlight": "è­‰æ“šæ¨™è¨˜ç³»çµ±",
            "decision_tree": "æ±ºç­–æ¨¹è¦–è¦ºåŒ–"
        }
    
    def create_reasoning_path(self, analysis_result: Dict) -> Dict[str, Any]:
        """å‰µå»ºæ¨ç†è·¯å¾‘åœ–"""
        return {
            "type": "reasoning_path",
            "title": "AI æ¨ç†è·¯å¾‘",
            "steps": [
                {
                    "step": 1,
                    "action": "ç—‡ç‹€è­˜åˆ¥",
                    "description": "åˆ†æç”¨æˆ¶è¼¸å…¥çš„ç—‡ç‹€æè¿°",
                    "confidence": 0.9
                },
                {
                    "step": 2,
                    "action": "æ¨¡çµ„åŒ¹é…",
                    "description": "é¸æ“‡æœ€é©åˆçš„åˆ†ææ¨¡çµ„",
                    "confidence": 0.85
                },
                {
                    "step": 3,
                    "action": "çŸ¥è­˜æª¢ç´¢",
                    "description": "å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œè³‡è¨Š",
                    "confidence": 0.8
                },
                {
                    "step": 4,
                    "action": "ç¶œåˆåˆ†æ",
                    "description": "çµåˆå¤šå€‹æ¨¡çµ„é€²è¡Œç¶œåˆåˆ†æ",
                    "confidence": 0.9
                },
                {
                    "step": 5,
                    "action": "å»ºè­°ç”Ÿæˆ",
                    "description": "ç”Ÿæˆå€‹æ€§åŒ–å»ºè­°å’Œè¡Œå‹•æ–¹æ¡ˆ",
                    "confidence": 0.85
                }
            ]
        }
    
    def create_confidence_radar(self, verification_result: Dict) -> Dict[str, Any]:
        """å‰µå»ºä¿¡å¿ƒåˆ†æ•¸é›·é”åœ–"""
        aspects = verification_result.get("aspects", {})
        radar_data = []
        
        for aspect, data in aspects.items():
            radar_data.append({
                "dimension": data["description"],
                "score": data["score"],
                "status": data["status"]
            })
        
        return {
            "type": "confidence_radar",
            "title": "å¤šç¶­åº¦å¯ä¿¡åº¦è©•ä¼°",
            "overall_score": verification_result.get("overall_score", 0),
            "dimensions": radar_data
        }
    
    def create_evidence_highlight(self, analysis_result: Dict) -> Dict[str, Any]:
        """å‰µå»ºè­‰æ“šæ¨™è¨˜ç³»çµ±"""
        chunks = analysis_result.get("retrieved_chunks", [])
        evidence_list = []
        
        for i, chunk in enumerate(chunks[:5]):  # å–å‰5å€‹æœ€ç›¸é—œçš„è­‰æ“š
            evidence_list.append({
                "id": i + 1,
                "title": chunk.get("title", "ç›¸é—œçŸ¥è­˜ç‰‡æ®µ"),
                "content": chunk.get("content", "")[:100] + "...",
                "relevance_score": chunk.get("confidence_score", 0.8),
                "source": chunk.get("chunk_id", "M1")
            })
        
        return {
            "type": "evidence_highlight",
            "title": "é—œéµåˆ¤æ–·ä¾æ“š",
            "evidence_count": len(evidence_list),
            "evidence_list": evidence_list
        }
    
    def create_decision_tree(self, analysis_result: Dict) -> Dict[str, Any]:
        """å‰µå»ºæ±ºç­–æ¨¹è¦–è¦ºåŒ–"""
        modules_used = analysis_result.get("modules_used", [])
        
        decision_nodes = [
            {
                "node_id": "start",
                "question": "ç”¨æˆ¶è¼¸å…¥ç—‡ç‹€æè¿°",
                "branches": [
                    {
                        "condition": "è¨˜æ†¶ç›¸é—œç—‡ç‹€",
                        "target": "M1",
                        "confidence": 0.9
                    },
                    {
                        "condition": "éšæ®µè©¢å•",
                        "target": "M2", 
                        "confidence": 0.85
                    },
                    {
                        "condition": "è¡Œç‚ºå•é¡Œ",
                        "target": "M3",
                        "confidence": 0.8
                    },
                    {
                        "condition": "ç…§è­·éœ€æ±‚",
                        "target": "M4",
                        "confidence": 0.75
                    }
                ]
            }
        ]
        
        return {
            "type": "decision_tree",
            "title": "åˆ†æé‚è¼¯é€æ˜åŒ–",
            "selected_modules": modules_used,
            "decision_nodes": decision_nodes
        }
    
    def create_comprehensive_visualization(self, analysis_result: Dict, verification_result: Dict) -> Dict[str, Any]:
        """å‰µå»ºç¶œåˆè¦–è¦ºåŒ–"""
        return {
            "reasoning_path": self.create_reasoning_path(analysis_result),
            "confidence_radar": self.create_confidence_radar(verification_result),
            "evidence_highlight": self.create_evidence_highlight(analysis_result),
            "decision_tree": self.create_decision_tree(analysis_result)
        }


# å°ˆæ¥­æ¨¡çµ„åŒ–åˆ†æå¢å¼·
class ProfessionalModularAnalysis:
    """å°ˆæ¥­ M1-M4 æ¨¡çµ„åŒ–åˆ†æ"""
    
    def __init__(self):
        self.bon_mav = BoNMAV()
        self.xai_visualization = XAIVisualization()
        self.aspect_verifier = AspectVerifier()
    
    async def analyze_professional(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """å°ˆæ¥­æ¨¡çµ„åŒ–åˆ†æ"""
        # 1. æ¨¡çµ„é¸æ“‡
        selected_modules = self._select_modules(user_input)
        
        # 2. å°ˆæ¥­åˆ†æ
        analysis_results = {}
        for module in selected_modules:
            analysis_results[module] = await self._analyze_module(module, user_input, context)
        
        # 3. ç”Ÿæˆå€™é¸ç­”æ¡ˆ
        bon_mav_result = await self.bon_mav.generate_best_answer(user_input, context)
        
        # 4. å¤šè§’åº¦é©—è­‰
        verification_result = await self.aspect_verifier.verify_answer(
            bon_mav_result["best_answer"], context
        )
        
        # 5. XAI è¦–è¦ºåŒ–
        xai_visualization = self.xai_visualization.create_comprehensive_visualization(
            analysis_results, verification_result
        )
        
        return {
            "selected_modules": selected_modules,
            "analysis_results": analysis_results,
            "best_answer": bon_mav_result["best_answer"],
            "verification": verification_result,
            "xai_visualization": xai_visualization,
            "comprehensive_score": bon_mav_result["comprehensive_score"],
            "selection_reason": bon_mav_result["selection_reason"]
        }
    
    def _select_modules(self, user_input: str) -> List[str]:
        """æ™ºèƒ½é¸æ“‡åˆ†ææ¨¡çµ„"""
        modules = []
        
        # M1 å¿«é€Ÿç¯©æª¢ - åå¤§è­¦è¨Šæ™ºèƒ½æ¯”å°
        m1_keywords = ["è¨˜æ†¶", "å¿˜è¨˜", "é‡è¤‡", "è¿·è·¯", "æ™‚é–“", "æ··æ·†", "è­¦è¨Š"]
        if any(keyword in user_input for keyword in m1_keywords):
            modules.append("M1")
        
        # M2 ç—…ç¨‹ç†è§£ - éšæ®µé æ¸¬èˆ‡å€‹äººåŒ–å»ºè­°
        m2_keywords = ["éšæ®µ", "é€²å±•", "æƒ¡åŒ–", "æ—©æœŸ", "ä¸­æœŸ", "æ™šæœŸ", "ç—…ç¨‹"]
        if any(keyword in user_input for keyword in m2_keywords):
            modules.append("M2")
        
        # M3 ç—‡ç‹€è™•ç† - BPSD åˆ†é¡èˆ‡æ‡‰å°ç­–ç•¥
        m3_keywords = ["æƒ…ç·’", "è¡Œç‚º", "æš´èº", "å¦„æƒ³", "å¹»è¦º", "éŠèµ°", "ç¡çœ ", "æ”»æ“Š"]
        if any(keyword in user_input for keyword in m3_keywords):
            modules.append("M3")
        
        # M4 è³‡æºå°èˆª - æ™ºèƒ½åŒ¹é…èˆ‡ç”³è«‹æŒ‡å¼•
        m4_keywords = ["ç”³è«‹", "è£œåŠ©", "è³‡æº", "ç…§è­·", "æœå‹™", "æ”¯æ´", "å”åŠ©"]
        if any(keyword in user_input for keyword in m4_keywords):
            modules.append("M4")
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºåŒ¹é…ï¼Œä½¿ç”¨ M1 ä½œç‚ºé è¨­
        if not modules:
            modules.append("M1")
        
        return modules
    
    async def _analyze_module(self, module: str, user_input: str, context: Dict) -> Dict[str, Any]:
        """åˆ†æç‰¹å®šæ¨¡çµ„"""
        if module == "M1":
            return await self._analyze_m1_warning_signs(user_input, context)
        elif module == "M2":
            return await self._analyze_m2_progression(user_input, context)
        elif module == "M3":
            return await self._analyze_m3_bpsd(user_input, context)
        elif module == "M4":
            return await self._analyze_m4_resources(user_input, context)
        else:
            return {"error": f"æœªçŸ¥æ¨¡çµ„: {module}"}
    
    async def _analyze_m1_warning_signs(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """M1: åå¤§è­¦è¨Šæ™ºèƒ½æ¯”å°"""
        warning_signs = [
            "è¨˜æ†¶åŠ›æ¸›é€€å½±éŸ¿æ—¥å¸¸ç”Ÿæ´»",
            "è¨ˆåŠƒäº‹æƒ…æˆ–è§£æ±ºå•é¡Œæœ‰å›°é›£", 
            "ç„¡æ³•å®Œæˆç†Ÿæ‚‰çš„å·¥ä½œ",
            "å°æ™‚é–“æˆ–åœ°é»æ„Ÿåˆ°å›°æƒ‘",
            "ç†è§£è¦–è¦ºå½±åƒå’Œç©ºé–“é—œä¿‚æœ‰å›°é›£",
            "èªªè©±æˆ–å¯«ä½œæ™‚ç”¨å­—å›°é›£",
            "æŠŠæ±è¥¿æ”¾éŒ¯åœ°æ–¹ä¸”ç„¡æ³•å›é ­å»æ‰¾",
            "åˆ¤æ–·åŠ›æ¸›é€€",
            "é€€å‡ºå·¥ä½œæˆ–ç¤¾äº¤æ´»å‹•",
            "æƒ…ç·’å’Œå€‹æ€§æ”¹è®Š"
        ]
        
        matched_signs = []
        for sign in warning_signs:
            if any(keyword in user_input for keyword in sign.split()):
                matched_signs.append(sign)
        
        return {
            "module": "M1",
            "warning_signs_detected": matched_signs,
            "risk_level": "high" if len(matched_signs) >= 2 else "medium" if len(matched_signs) >= 1 else "low",
            "recommendation": "å»ºè­°åŠæ—©å°±é†«è©•ä¼°" if len(matched_signs) >= 1 else "æŒçºŒè§€å¯Ÿ"
        }
    
    async def _analyze_m2_progression(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """M2: éšæ®µé æ¸¬èˆ‡å€‹äººåŒ–å»ºè­°"""
        stages = {
            "è¼•åº¦": {
                "duration": "2-4å¹´",
                "characteristics": ["è¨˜æ†¶åŠ›æ¸›é€€", "è¼•å¾®èªçŸ¥éšœç¤™", "æ—¥å¸¸ç”Ÿæ´»åŸºæœ¬è‡ªç†"],
                "recommendations": ["å»ºç«‹é†«ç™‚åœ˜éšŠ", "å­¸ç¿’ç…§è­·æŠ€å·§", "è¦åŠƒæœªä¾†å®‰æ’"]
            },
            "ä¸­åº¦": {
                "duration": "2-8å¹´", 
                "characteristics": ["æ˜é¡¯èªçŸ¥éšœç¤™", "æ—¥å¸¸ç”Ÿæ´»éœ€å”åŠ©", "è¡Œç‚ºæ”¹è®Š"],
                "recommendations": ["ç”³è«‹ç…§è­·è³‡æº", "èª¿æ•´å±…å®¶ç’°å¢ƒ", "å°‹æ±‚å®¶å±¬æ”¯æŒ"]
            },
            "é‡åº¦": {
                "duration": "1-3å¹´",
                "characteristics": ["åš´é‡èªçŸ¥éšœç¤™", "å®Œå…¨ä¾è³´ç…§è­·", "èº«é«”åŠŸèƒ½é€€åŒ–"],
                "recommendations": ["24å°æ™‚ç…§è­·å®‰æ’", "å®‰å¯§ç…§è­·æº–å‚™", "å®¶å±¬å¿ƒç†æ”¯æŒ"]
            }
        }
        
        # ç°¡å–®çš„éšæ®µåˆ¤æ–·é‚è¼¯
        if "é‡åº¦" in user_input or "æ™šæœŸ" in user_input:
            current_stage = "é‡åº¦"
        elif "ä¸­åº¦" in user_input:
            current_stage = "ä¸­åº¦"
        else:
            current_stage = "è¼•åº¦"
        
        return {
            "module": "M2",
            "current_stage": current_stage,
            "stage_info": stages[current_stage],
            "progression_prediction": "ç—‡ç‹€æœƒé€æ¼¸é€²å±•ï¼Œæ—©æœŸä»‹å…¥å¯å»¶ç·©æƒ¡åŒ–"
        }
    
    async def _analyze_m3_bpsd(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """M3: BPSD åˆ†é¡èˆ‡æ‡‰å°ç­–ç•¥"""
        bpsd_categories = {
            "æƒ…ç·’ç—‡ç‹€": ["æ†‚é¬±", "ç„¦æ…®", "æ˜“æ€’", "æƒ…ç·’ä¸ç©©"],
            "ç²¾ç¥ç—…ç—‡ç‹€": ["å¦„æƒ³", "å¹»è¦º", "éŒ¯èª"],
            "è¡Œç‚ºç—‡ç‹€": ["éŠèµ°", "æ”»æ“Š", "é‡è¤‡è¡Œç‚º"],
            "ç¡çœ éšœç¤™": ["å¤±çœ ", "æ—¥å¤œé¡›å€’", "ç¡çœ å“è³ªå·®"]
        }
        
        detected_symptoms = []
        for category, symptoms in bpsd_categories.items():
            for symptom in symptoms:
                if symptom in user_input:
                    detected_symptoms.append({
                        "category": category,
                        "symptom": symptom,
                        "severity": "moderate"
                    })
        
        return {
            "module": "M3",
            "detected_symptoms": detected_symptoms,
            "total_symptoms": len(detected_symptoms),
            "intervention_strategy": "æ ¹æ“šç—‡ç‹€åš´é‡åº¦åˆ¶å®šå€‹æ€§åŒ–ä»‹å…¥ç­–ç•¥"
        }
    
    async def _analyze_m4_resources(self, user_input: str, context: Dict) -> Dict[str, Any]:
        """M4: æ™ºèƒ½åŒ¹é…èˆ‡ç”³è«‹æŒ‡å¼•"""
        resource_categories = {
            "é†«ç™‚è³‡æº": ["ç¥ç¶“ç§‘é–€è¨º", "èªçŸ¥åŠŸèƒ½è©•ä¼°", "è—¥ç‰©æ²»ç™‚"],
            "ç…§è­·è³‡æº": ["å±…å®¶ç…§è­·", "æ—¥é–“ç…§è­·", "æ©Ÿæ§‹ç…§è­·"],
            "ç¤¾æœƒç¦åˆ©": ["èº«å¿ƒéšœç¤™è­‰æ˜", "é•·ç…§æœå‹™", "ç¶“æ¿Ÿè£œåŠ©"],
            "æ”¯æŒæœå‹™": ["å®¶å±¬æ”¯æŒåœ˜é«”", "è«®å•†æœå‹™", "ç·Šæ€¥è¯çµ¡"]
        }
        
        matched_resources = []
        for category, resources in resource_categories.items():
            for resource in resources:
                if any(keyword in user_input for keyword in resource.split()):
                    matched_resources.append({
                        "category": category,
                        "resource": resource,
                        "priority": "high" if "ç·Šæ€¥" in resource else "medium"
                    })
        
        return {
            "module": "M4",
            "matched_resources": matched_resources,
            "application_guidance": "æŒ‰å„ªå…ˆé †åºç”³è«‹ï¼Œæº–å‚™å®Œæ•´æ–‡ä»¶"
        }


# åˆå§‹åŒ–å°ˆæ¥­åˆ†æå™¨
professional_analyzer = ProfessionalModularAnalysis()


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8005)
    