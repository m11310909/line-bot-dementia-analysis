# day1_m1_rag_integration.py
"""
M1 MVP + RAG æ•´åˆç¬¬ä¸€å¤©ï¼šæ ¸å¿ƒåŠŸèƒ½æ•´åˆ
ä¿ç•™ç¾æœ‰æˆç†ŸåŠŸèƒ½ï¼ŒåŠ å…¥ RAG æª¢ç´¢å¢å¼·
"""

import json
import os
from typing import Dict, List, Optional
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from datetime import datetime

# ===== ç¬¬ä¸€æ­¥ï¼šè³‡æ–™è½‰æ›èˆ‡æ•´åˆ =====

def convert_m1_mvp_to_chunks():
    """
    å°‡ç¾æœ‰ M1 MVP çš„è­¦è¨Šè³‡æ–™è½‰æ›ç‚º RAG chunk æ ¼å¼
    ä¿ç•™æ‰€æœ‰ç¾æœ‰çš„çŸ¥è­˜å…§å®¹
    """

    # ç¾æœ‰ M1 MVP çš„åå¤§è­¦è¨Šè³‡æ–™ï¼ˆåŸºæ–¼ä½ çš„æ–‡ä»¶ï¼‰
    m1_warnings = [
        {
            "matched_warning_code": "M1-01",
            "symptom_title": "è¨˜æ†¶åŠ›æ¸›é€€å½±éŸ¿æ—¥å¸¸ç”Ÿæ´»",
            "normal_behavior": "å¶çˆ¾å¿˜è¨˜ç´„æœƒã€åŒäº‹å§“åæˆ–é›»è©±ï¼Œä½†äº‹å¾Œæœƒæƒ³èµ·ä¾†",
            "dementia_indicator": "å¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹ã€é‡è¦çš„æ—¥æœŸæˆ–äº‹ä»¶ï¼›åŒæ¨£çš„äº‹æƒ…åè¦†è©¢å•ï¼›éœ€è¦ä¾è³´è¼”åŠ©å·¥å…·æˆ–å®¶äººå”åŠ©è™•ç†ä»¥å‰å¯ä»¥è‡ªå·±æ‡‰ä»˜çš„äº‹æƒ…",
            "action_suggestion": "è‹¥è¨˜æ†¶å•é¡ŒæŒçºŒåŠ åŠ‡ï¼Œå»ºè­°è«®è©¢é†«å¸«é€²è¡Œé€²ä¸€æ­¥è©•ä¼°",
            "source": "TADA åå¤§è­¦è¨Š"
        },
        {
            "matched_warning_code": "M1-02", 
            "symptom_title": "è¨ˆåŠƒäº‹æƒ…æˆ–è§£æ±ºå•é¡Œæœ‰å›°é›£",
            "normal_behavior": "å¶çˆ¾éœ€è¦ä»–äººå”åŠ©ä½¿ç”¨å¾®æ³¢çˆè¨­å®šæˆ–éŒ„è£½é›»è¦–ç¯€ç›®",
            "dementia_indicator": "ç„¡æ³•å°ˆå¿ƒï¼Œåšäº‹éœ€è¦æ¯”ä»¥å‰æ›´é•·çš„æ™‚é–“ï¼›è™•ç†é‡‘éŒ¢æœ‰å›°é›£ï¼Œä¾‹å¦‚å¸³å–®ç¹³è²»æˆ–ç®¡ç†é–‹éŠ·",
            "action_suggestion": "å¦‚æœè¨ˆåŠƒèƒ½åŠ›æ˜é¡¯ä¸‹é™ï¼Œå»ºè­°å°±é†«è©•ä¼°èªçŸ¥åŠŸèƒ½",
            "source": "TADA åå¤§è­¦è¨Š"
        },
        {
            "matched_warning_code": "M1-03",
            "symptom_title": "ç„¡æ³•å‹ä»»åŸæœ¬ç†Ÿæ‚‰çš„äº‹å‹™",
            "normal_behavior": "å¶çˆ¾éœ€è¦å”åŠ©éŒ„è£½é›»è¦–ç¯€ç›®æˆ–ä½¿ç”¨æ–°çš„é›»å™¨ç”¨å“",
            "dementia_indicator": "ç„¡æ³•å®ŒæˆåŸæœ¬ç†Ÿæ‚‰çš„å·¥ä½œï¼Œä¾‹å¦‚ï¼šç†Ÿæ‚‰çš„åœ°æ–¹è¿·è·¯ã€ç„¡æ³•ç®¡ç†é ç®—ã€å¿˜è¨˜å–œæ„›éŠæˆ²çš„è¦å‰‡",
            "action_suggestion": "è‹¥ç„¡æ³•å®Œæˆç†Ÿæ‚‰ä»»å‹™ï¼Œæ‡‰åŠæ—©å°±é†«æª¢æŸ¥",
            "source": "TADA åå¤§è­¦è¨Š"
        },
        {
            "matched_warning_code": "M1-04",
            "symptom_title": "å°æ™‚é–“åœ°é»æ„Ÿåˆ°æ··æ·†",
            "normal_behavior": "å¶çˆ¾å¿˜è¨˜ä»Šå¤©æ˜¯æ˜ŸæœŸå¹¾ï¼Œä½†ç¨å¾Œæœƒæƒ³èµ·ä¾†",
            "dementia_indicator": "æä¸æ¸…æ¥šå¹´æœˆæ—¥ã€å­£ç¯€ï¼›å¿˜è¨˜è‡ªå·±èº«åœ¨ä½•è™•æˆ–å¦‚ä½•åˆ°é”è©²åœ°",
            "action_suggestion": "æ™‚ç©ºèªçŸ¥æ··äº‚æ™‚ï¼Œéœ€è¦å°ˆæ¥­é†«ç™‚è©•ä¼°",
            "source": "TADA åå¤§è­¦è¨Š"
        },
        {
            "matched_warning_code": "M1-05",
            "symptom_title": "ç†è§£è¦–è¦ºå½±åƒå’Œç©ºé–“é—œä¿‚æœ‰å›°é›£",
            "normal_behavior": "å› ç™½å…§éšœç­‰è¦–è¦ºè®ŠåŒ–é€ æˆçš„è¦–è¦ºå•é¡Œ",
            "dementia_indicator": "ç„¡æ³•åˆ¤æ–·è·é›¢ã€æ±ºå®šé¡è‰²æˆ–å°æ¯”ï¼Œå½±éŸ¿é§•é§›èƒ½åŠ›",
            "action_suggestion": "å‡ºç¾è¦–è¦ºç©ºé–“å•é¡Œæ™‚ï¼Œæ‡‰é¿å…é–‹è»Šä¸¦å°±é†«æª¢æŸ¥",
            "source": "TADA åå¤§è­¦è¨Š"
        }
    ]

    # è½‰æ›ç‚ºæ¨™æº– RAG chunk æ ¼å¼
    rag_chunks = []
    for warning in m1_warnings:
        chunk = {
            "chunk_id": warning["matched_warning_code"],
            "module_id": "M1",
            "chunk_type": "warning_sign",
            "title": warning["symptom_title"],
            "content": f"""
ã€æ­£å¸¸è€åŒ–ã€‘{warning['normal_behavior']}

ã€å¤±æ™ºè­¦è¨Šã€‘{warning['dementia_indicator']}

ã€å»ºè­°è¡Œå‹•ã€‘{warning['action_suggestion']}
            """.strip(),
            "keywords": extract_keywords(warning["symptom_title"]),
            "confidence_score": 0.95,  # M1 è³‡æ–™æ˜¯å®˜æ–¹æ¬Šå¨ï¼Œè¨­é«˜ä¿¡å¿ƒåº¦
            "source": warning["source"],
            "explanation_data": {
                "normal_behavior": warning["normal_behavior"],
                "dementia_indicator": warning["dementia_indicator"],
                "reasoning": "åŸºæ–¼å°ç£å¤±æ™ºç—‡å”æœƒå®˜æ–¹åå¤§è­¦è¨Š"
            },
            "language": "zh-TW",
            "created_at": datetime.now().isoformat()
        }
        rag_chunks.append(chunk)

    return rag_chunks

def extract_keywords(title: str) -> List[str]:
    """å¾æ¨™é¡Œæå–é—œéµå­—"""
    keyword_mapping = {
        "è¨˜æ†¶åŠ›": ["è¨˜æ†¶", "å¥å¿˜", "å¿˜è¨˜"],
        "è¨ˆåŠƒ": ["è¨ˆåŠƒ", "è§£æ±ºå•é¡Œ", "å°ˆæ³¨"],
        "ç†Ÿæ‚‰": ["å·¥ä½œ", "ä»»å‹™", "æŠ€èƒ½"],
        "æ™‚é–“åœ°é»": ["æ™‚é–“", "åœ°é»", "æ–¹å‘æ„Ÿ"],
        "è¦–è¦º": ["è¦–è¦º", "ç©ºé–“", "è·é›¢"]
    }

    keywords = []
    for key, values in keyword_mapping.items():
        if key in title:
            keywords.extend(values)

    return keywords[:3]  # é™åˆ¶é—œéµå­—æ•¸é‡

# ===== ç¬¬äºŒæ­¥ï¼šRAG æª¢ç´¢å¼•æ“æ•´åˆ =====

class EnhancedM1RAGEngine:
    """
    å¢å¼·çš„ M1 RAG å¼•æ“
    æ•´åˆç¾æœ‰ MVP åŠŸèƒ½ + æ–°çš„å‘é‡æª¢ç´¢èƒ½åŠ›
    """

    def __init__(self, gemini_api_key: str):
        # ä¿ç•™ç¾æœ‰çš„ Gemini é…ç½®
        genai.configure(api_key=gemini_api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')

        # æ–°å¢ RAG æª¢ç´¢èƒ½åŠ›
        self.sentence_model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.chunks = []
        self.index = None

        # è¼‰å…¥ä¸¦å»ºç«‹ç´¢å¼•
        self.load_m1_chunks()
        self.build_vector_index()

    def load_m1_chunks(self):
        """è¼‰å…¥ M1 chunks è³‡æ–™"""
        self.chunks = convert_m1_mvp_to_chunks()
        print(f"è¼‰å…¥äº† {len(self.chunks)} å€‹ M1 çŸ¥è­˜ç‰‡æ®µ")

    def build_vector_index(self):
        """å»ºç«‹å‘é‡ç´¢å¼•"""
        if not self.chunks:
            return

        # ç‚ºæ¯å€‹ chunk å»ºç«‹å‘é‡
        texts = [f"{chunk['title']} {chunk['content']}" for chunk in self.chunks]
        embeddings = self.sentence_model.encode(texts)

        # å»ºç«‹ FAISS ç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))

        print(f"å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼Œç¶­åº¦ï¼š{dimension}")

    def retrieve_relevant_chunks(self, user_input: str, k: int = 3) -> List[Dict]:
        """
        æª¢ç´¢ç›¸é—œçš„çŸ¥è­˜ç‰‡æ®µ
        é€™æ˜¯æ–°å¢çš„ RAG åŠŸèƒ½
        """
        if not self.index:
            return self.chunks[:k]  # Fallback åˆ°å…¨éƒ¨è³‡æ–™

        # å°‡ä½¿ç”¨è€…è¼¸å…¥è½‰ç‚ºå‘é‡
        query_embedding = self.sentence_model.encode([user_input])

        # FAISS æª¢ç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        # çµ„ç¹”çµæœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx].copy()
                chunk['similarity_score'] = float(1 / (1 + score))
                results.append(chunk)

        return results

    def analyze_with_enhanced_context(self, user_input: str) -> Dict:
        """
        å¢å¼·ç‰ˆåˆ†æï¼šRAG æª¢ç´¢ + ç¾æœ‰ Gemini åˆ†æ
        ä¿ç•™åŸæœ‰åˆ†æé‚è¼¯ï¼ŒåŠ å…¥æª¢ç´¢å¢å¼·
        """

        # 1. RAG æª¢ç´¢ç›¸é—œå…§å®¹
        relevant_chunks = self.retrieve_relevant_chunks(user_input, k=3)

        # 2. å»ºæ§‹å¢å¼·ç‰ˆ promptï¼ˆä¿ç•™åŸæœ‰æ ¼å¼ï¼‰
        context_info = ""
        for chunk in relevant_chunks:
            context_info += f"""
ã€{chunk['title']}ã€‘
{chunk['content']}
---
"""

        # 3. ä½¿ç”¨ç¾æœ‰çš„ Gemini åˆ†æé‚è¼¯
        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¤±æ™ºç—‡æ—©æœŸè­¦è¨Šåˆ†æåŠ©ç†ã€‚

åƒè€ƒä»¥ä¸‹å°ˆæ¥­çŸ¥è­˜å…§å®¹ï¼š
{context_info}

ä½¿ç”¨è€…æè¿°çš„æƒ…æ³ï¼š
"{user_input}"

è«‹åˆ†ææ­¤æƒ…æ³ä¸¦ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
    "matched_warning_code": "M1-XX",
    "symptom_title": "ç›¸ç¬¦çš„è­¦è¨Šæ¨™é¡Œ",
    "user_behavior_summary": "ä½¿ç”¨è€…è¡Œç‚ºæ‘˜è¦",
    "normal_behavior": "æ­£å¸¸è€åŒ–çš„å°ç…§è¡Œç‚º",
    "dementia_indicator": "å¤±æ™ºç—‡çš„è­¦è¨ŠæŒ‡æ¨™", 
    "action_suggestion": "å…·é«”çš„å»ºè­°è¡Œå‹•",
    "confidence_level": "high/medium/low",
    "retrieved_sources": ["ä½¿ç”¨çš„è³‡æ–™ä¾†æº"],
    "source": "TADA åå¤§è­¦è¨Š"
}}

æ³¨æ„ï¼š
1. å¿…é ˆåŸºæ–¼æä¾›çš„å°ˆæ¥­çŸ¥è­˜å›ç­”
2. å¦‚æœä¸ç¢ºå®šï¼Œconfidence_level è¨­ç‚º low
3. æä¾›æº«å’Œä¸”æ”¯æŒæ€§çš„å»ºè­°
4. é¿å…é†«ç™‚è¨ºæ–·ï¼Œåƒ…æä¾›åƒè€ƒè³‡è¨Š
        """

        try:
            # å‘¼å« Gemini APIï¼ˆä¿ç•™ç¾æœ‰é‚è¼¯ï¼‰
            response = self.model.generate_content(prompt)
            result_text = response.text

            # JSON è§£æï¼ˆä¿ç•™ç¾æœ‰çš„å®¹éŒ¯æ©Ÿåˆ¶ï¼‰
            analysis_result = self.safe_json_parse(result_text)

            # å¢å¼·ï¼šåŠ å…¥æª¢ç´¢è³‡è¨Š
            analysis_result["retrieved_chunks"] = relevant_chunks
            analysis_result["total_chunks_used"] = len(relevant_chunks)
            analysis_result["rag_enhanced"] = True

            return analysis_result

        except Exception as e:
            print(f"Gemini åˆ†æå¤±æ•—ï¼š{e}")
            return self.get_fallback_response(user_input, relevant_chunks)

    def safe_json_parse(self, text: str) -> Dict:
        """
        å®‰å…¨çš„ JSON è§£æï¼ˆä¿ç•™ç¾æœ‰é‚è¼¯ï¼‰
        é€™æ˜¯ä½  MVP ä¸­å·²é©—è­‰çš„åŠŸèƒ½
        """
        try:
            # å˜—è©¦ç›´æ¥è§£æ
            return json.loads(text)
        except:
            # æå– JSON ç¨‹å¼ç¢¼å€å¡Š
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass

            # æœ€å¾Œçš„ fallback
            return self.get_basic_fallback()

    def get_fallback_response(self, user_input: str, chunks: List[Dict]) -> Dict:
        """éŒ¯èª¤æ™‚çš„å‚™ç”¨å›æ‡‰"""
        return {
            "matched_warning_code": "M1-01",
            "symptom_title": "éœ€è¦é€²ä¸€æ­¥è©•ä¼°",
            "user_behavior_summary": user_input[:50],
            "normal_behavior": "è¼•å¾®çš„è¨˜æ†¶å•é¡Œå¯èƒ½æ˜¯æ­£å¸¸è€åŒ–",
            "dementia_indicator": "å¦‚æœç—‡ç‹€æŒçºŒæˆ–åŠ é‡ï¼Œå¯èƒ½éœ€è¦é—œæ³¨",
            "action_suggestion": "å»ºè­°è¨˜éŒ„ç—‡ç‹€è®ŠåŒ–ï¼Œå¿…è¦æ™‚è«®è©¢å°ˆæ¥­é†«å¸«",
            "confidence_level": "low",
            "retrieved_sources": [chunk['source'] for chunk in chunks],
            "source": "TADA åå¤§è­¦è¨Š",
            "rag_enhanced": True,
            "fallback_used": True
        }

    def get_basic_fallback(self) -> Dict:
        """åŸºæœ¬å‚™ç”¨å›æ‡‰"""
        return {
            "matched_warning_code": "M1-GENERAL",
            "symptom_title": "ä¸€èˆ¬æ€§é—œæ³¨",
            "user_behavior_summary": "æè¿°çš„æƒ…æ³",
            "normal_behavior": "éš¨è‘—å¹´é½¡å¢é•·ï¼Œè¼•å¾®çš„èªçŸ¥è®ŠåŒ–æ˜¯æ­£å¸¸çš„",
            "dementia_indicator": "æŒçºŒæˆ–åš´é‡çš„èªçŸ¥è®ŠåŒ–éœ€è¦é—œæ³¨",
            "action_suggestion": "å¦‚æœ‰ç–‘æ…®ï¼Œå»ºè­°è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡",
            "confidence_level": "low",
            "source": "ä¸€èˆ¬é†«ç™‚å»ºè­°"
        }

# ===== ç¬¬ä¸‰æ­¥ï¼šæ•´åˆæ¸¬è©¦ =====

def test_enhanced_m1_engine():
    """æ¸¬è©¦å¢å¼·ç‰ˆ M1 å¼•æ“"""

    # éœ€è¦è¨­å®š Gemini API Key
    api_key = os.getenv('AISTUDIO_API_KEY')
    if not api_key:
        print("è«‹è¨­å®š AISTUDIO_API_KEY ç’°å¢ƒè®Šæ•¸")
        return

    # åˆå§‹åŒ–å¢å¼·å¼•æ“
    engine = EnhancedM1RAGEngine(api_key)

    # æ¸¬è©¦æ¡ˆä¾‹ï¼ˆåŸºæ–¼ä½ çš„ MVP ç¶“é©—ï¼‰
    test_cases = [
        "åª½åª½æœ€è¿‘å¸¸å¿˜è¨˜é—œç“¦æ–¯",
        "çˆ¸çˆ¸é–‹è»Šæ™‚æœƒè¿·è·¯",
        "å¥¶å¥¶é‡è¤‡å•åŒæ¨£çš„å•é¡Œ",
        "çˆºçˆºç„¡æ³•è™•ç†éŠ€è¡Œå¸³å–®",
        "åª½åª½åœ¨ç†Ÿæ‚‰çš„åœ°æ–¹è¿·è·¯äº†"
    ]

    print("ğŸ§  æ¸¬è©¦å¢å¼·ç‰ˆ M1 å¤±æ™ºç—‡è­¦è¨Šåˆ†æ")
    print("=" * 60)

    for i, test_input in enumerate(test_cases, 1):
        print(f"\næ¸¬è©¦ {i}: {test_input}")
        print("-" * 40)

        # åŸ·è¡Œåˆ†æ
        result = engine.analyze_with_enhanced_context(test_input)

        # é¡¯ç¤ºçµæœ
        print(f"ğŸ“‹ è­¦è¨Šä»£ç¢¼: {result.get('matched_warning_code', 'N/A')}")
        print(f"ğŸ¯ ç—‡ç‹€æ¨™é¡Œ: {result.get('symptom_title', 'N/A')}")
        print(f"ğŸ“Š ä¿¡å¿ƒç¨‹åº¦: {result.get('confidence_level', 'N/A')}")
        print(f"ğŸ” ä½¿ç”¨è³‡æ–™æº: {result.get('total_chunks_used', 0)} å€‹")
        print(f"âš¡ RAG å¢å¼·: {result.get('rag_enhanced', False)}")

        if result.get('action_suggestion'):
            print(f"ğŸ’¡ å»ºè­°: {result['action_suggestion'][:80]}...")

if __name__ == "__main__":
    print("Day 1: M1 MVP + RAG æ ¸å¿ƒæ•´åˆ")
    print("=" * 50)

    # 1. è½‰æ›ç¾æœ‰è³‡æ–™
    print("ğŸ“Š è½‰æ› M1 MVP è³‡æ–™ç‚º RAG æ ¼å¼...")
    chunks = convert_m1_mvp_to_chunks()
    print(f"âœ… è½‰æ›å®Œæˆï¼š{len(chunks)} å€‹ chunks")

    # 2. å„²å­˜è½‰æ›å¾Œçš„è³‡æ–™
    os.makedirs('data/chunks', exist_ok=True)
    with open('data/chunks/m1_enhanced_chunks.jsonl', 'w', encoding='utf-8') as f:
        for chunk in chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
    print("âœ… è³‡æ–™å·²å„²å­˜åˆ° data/chunks/m1_enhanced_chunks.jsonl")

    # 3. æ¸¬è©¦æ•´åˆåŠŸèƒ½
    print("\nğŸ§ª é–‹å§‹æ¸¬è©¦æ•´åˆåŠŸèƒ½...")
    test_enhanced_m1_engine()

    print(f"\nğŸ‰ Day 1 å®Œæˆï¼")
    print("âœ… M1 MVP è³‡æ–™å·²æˆåŠŸè½‰æ›ç‚º RAG æ ¼å¼")
    print("âœ… å¢å¼·ç‰ˆåˆ†æå¼•æ“å·²å»ºç«‹ä¸¦æ¸¬è©¦")
    print("âœ… ä¿ç•™äº†æ‰€æœ‰ç¾æœ‰çš„æˆç†ŸåŠŸèƒ½")
    print("\nğŸ“Œ æ˜å¤©å°‡é€²è¡Œ API çµ±ä¸€èˆ‡æœ€çµ‚æ•´åˆ")