# lightweight_rag_for_replit.py
"""
é©ç”¨æ–¼ Replit çš„è¼•é‡ç´š RAG è§£æ±ºæ–¹æ¡ˆ
ä¿®æ­£ç‰ˆæœ¬ - ç„¡ç¸®æ’éŒ¯èª¤
"""

import json
import os
import re
import math
from typing import Dict, List, Optional
from collections import Counter
from datetime import datetime

# æª¢æŸ¥ Gemini æ¨¡çµ„
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    print("âš ï¸  google.generativeai æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
    GEMINI_AVAILABLE = False

class LightweightRAGEngine:
    """è¼•é‡ç´š RAG å¼•æ“"""

    def __init__(self, gemini_api_key=None):
        print("ğŸš€ åˆå§‹åŒ–è¼•é‡ç´š RAG å¼•æ“...")

        # Gemini é…ç½®
        self.gemini_available = GEMINI_AVAILABLE and gemini_api_key
        if self.gemini_available:
            try:
                genai.configure(api_key=gemini_api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                print("âœ… Gemini AI é€£æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  Gemini AI é€£æ¥å¤±æ•—: {e}")
                self.gemini_available = False

        # æª¢ç´¢çµ„ä»¶
        self.chunks = []
        self.tfidf_matrix = {}
        self.vocabulary = set()

        # è¼‰å…¥è³‡æ–™
        self.load_m1_chunks()
        self.build_tfidf_index()

        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def load_m1_chunks(self):
        """è¼‰å…¥ M1 å¤±æ™ºç—‡è­¦è¨Šè³‡æ–™"""
        print("ğŸ“Š è¼‰å…¥ M1 è³‡æ–™...")

        self.chunks = [
            {
                "chunk_id": "M1-01",
                "title": "è¨˜æ†¶åŠ›æ¸›é€€å½±éŸ¿æ—¥å¸¸ç”Ÿæ´»",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾å¿˜è¨˜ç´„æœƒä½†äº‹å¾Œæœƒæƒ³èµ·ä¾†ã€‚å¤±æ™ºè­¦è¨Šï¼šå¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹ã€é‡è¦æ—¥æœŸæˆ–äº‹ä»¶ï¼›åè¦†è©¢å•åŒæ¨£äº‹æƒ…ã€‚",
                "keywords": ["è¨˜æ†¶", "å¥å¿˜", "å¿˜è¨˜", "é‡è¤‡", "æ—¥å¸¸ç”Ÿæ´»"],
                "confidence_score": 0.95,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-02",
                "title": "è¨ˆåŠƒäº‹æƒ…æˆ–è§£æ±ºå•é¡Œæœ‰å›°é›£",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾éœ€è¦å”åŠ©ä½¿ç”¨è¨­å‚™ã€‚å¤±æ™ºè­¦è¨Šï¼šç„¡æ³•å°ˆå¿ƒï¼Œåšäº‹éœ€è¦æ›´é•·æ™‚é–“ï¼›è™•ç†é‡‘éŒ¢æœ‰å›°é›£ã€‚",
                "keywords": ["è¨ˆåŠƒ", "è§£æ±ºå•é¡Œ", "å°ˆå¿ƒ", "é‡‘éŒ¢", "å¸³å–®"],
                "confidence_score": 0.92,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-03", 
                "title": "ç„¡æ³•å‹ä»»åŸæœ¬ç†Ÿæ‚‰çš„äº‹å‹™",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾éœ€è¦å”åŠ©ä½¿ç”¨æ–°è¨­å‚™ã€‚å¤±æ™ºè­¦è¨Šï¼šç„¡æ³•å®Œæˆç†Ÿæ‚‰å·¥ä½œï¼Œå¦‚è¿·è·¯ã€ç„¡æ³•ç®¡ç†é ç®—ã€‚",
                "keywords": ["ç†Ÿæ‚‰", "å·¥ä½œ", "è¿·è·¯", "é ç®—", "ç®¡ç†"],
                "confidence_score": 0.90,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-04",
                "title": "å°æ™‚é–“åœ°é»æ„Ÿåˆ°æ··æ·†",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾å¿˜è¨˜æ˜ŸæœŸå¹¾ä½†ç¨å¾Œæƒ³èµ·ã€‚å¤±æ™ºè­¦è¨Šï¼šæä¸æ¸…å¹´æœˆæ—¥ã€å­£ç¯€ï¼›å¿˜è¨˜èº«åœ¨ä½•è™•ã€‚",
                "keywords": ["æ™‚é–“", "åœ°é»", "æ–¹å‘", "å­£ç¯€", "å¹´æœˆæ—¥"],
                "confidence_score": 0.88,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-05",
                "title": "ç†è§£è¦–è¦ºå½±åƒå’Œç©ºé–“é—œä¿‚æœ‰å›°é›£",
                "content": "æ­£å¸¸è€åŒ–ï¼šå› ç™½å…§éšœç­‰è¦–è¦ºè®ŠåŒ–ã€‚å¤±æ™ºè­¦è¨Šï¼šç„¡æ³•åˆ¤æ–·è·é›¢ã€é¡è‰²å°æ¯”ï¼Œå½±éŸ¿é§•é§›ã€‚",
                "keywords": ["è¦–è¦º", "ç©ºé–“", "è·é›¢", "é¡è‰²", "é§•é§›"],
                "confidence_score": 0.85,
                "source": "TADA åå¤§è­¦è¨Š"
            }
        ]

        print(f"âœ… è¼‰å…¥äº† {len(self.chunks)} å€‹çŸ¥è­˜ç‰‡æ®µ")

    def tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è©"""
        text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text.lower())
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', text)
        english_words = re.findall(r'[a-zA-Z]+', text)

        tokens = []
        for char_group in chinese_chars:
            # å–®å­—
            tokens.extend(list(char_group))
            # é›™å­—è©
            for i in range(len(char_group) - 1):
                tokens.append(char_group[i:i+2])

        tokens.extend(english_words)
        return tokens

    def compute_tf_idf(self, documents):
        """è¨ˆç®— TF-IDF"""
        print("ğŸ” å»ºç«‹ TF-IDF ç´¢å¼•...")

        all_tokens = []
        doc_tokens = []

        for doc in documents:
            tokens = self.tokenize_chinese(doc)
            doc_tokens.append(tokens)
            all_tokens.extend(tokens)

        self.vocabulary = set(all_tokens)
        vocab_list = list(self.vocabulary)

        tfidf_matrix = {}
        N = len(documents)

        for doc_idx, tokens in enumerate(doc_tokens):
            doc_vector = {}
            token_count = Counter(tokens)
            doc_length = len(tokens)

            for token in vocab_list:
                if doc_length > 0:
                    tf = token_count[token] / doc_length
                    docs_with_token = sum(1 for dt in doc_tokens if token in dt)
                    idf = math.log(N / docs_with_token) if docs_with_token > 0 else 0
                    tfidf_score = tf * idf
                    if tfidf_score > 0:
                        doc_vector[token] = tfidf_score

            tfidf_matrix[doc_idx] = doc_vector

        return tfidf_matrix

    def build_tfidf_index(self):
        """å»ºç«‹æª¢ç´¢ç´¢å¼•"""
        documents = []
        for chunk in self.chunks:
            doc_text = f"{chunk['title']} {chunk['content']} {' '.join(chunk['keywords'])}"
            documents.append(doc_text)

        self.tfidf_matrix = self.compute_tf_idf(documents)
        print("âœ… æª¢ç´¢ç´¢å¼•å»ºç«‹å®Œæˆ")

    def cosine_similarity(self, vec1, vec2):
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        common_keys = set(vec1.keys()) & set(vec2.keys())

        if not common_keys:
            return 0.0

        dot_product = sum(vec1[key] * vec2[key] for key in common_keys)
        norm1 = math.sqrt(sum(val**2 for val in vec1.values()))
        norm2 = math.sqrt(sum(val**2 for val in vec2.values()))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def retrieve_relevant_chunks(self, query, k=3):
        """æª¢ç´¢ç›¸é—œç‰‡æ®µ"""
        print(f"ğŸ” æª¢ç´¢æŸ¥è©¢: {query}")

        query_tokens = self.tokenize_chinese(query)
        query_token_count = Counter(query_tokens)
        query_length = len(query_tokens)

        if query_length == 0:
            return []

        query_vector = {}
        for token in self.vocabulary:
            tf = query_token_count[token] / query_length
            if tf > 0:
                query_vector[token] = tf

        similarities = []
        for doc_idx, doc_vector in self.tfidf_matrix.items():
            similarity = self.cosine_similarity(query_vector, doc_vector)
            similarities.append((doc_idx, similarity))

        similarities.sort(key=lambda x: x[1], reverse=True)

        results = []
        for doc_idx, similarity in similarities[:k]:
            if similarity > 0:
                chunk = self.chunks[doc_idx].copy()
                chunk['similarity_score'] = round(similarity, 4)
                results.append(chunk)

        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} å€‹ç›¸é—œç‰‡æ®µ")
        return results

    def analyze_with_lightweight_rag(self, user_input):
        """RAG åˆ†æ"""
        print(f"ğŸ§  åˆ†æ: {user_input}")

        relevant_chunks = self.retrieve_relevant_chunks(user_input, k=3)

        if not relevant_chunks:
            return self.get_fallback_response(user_input, [])

        if self.gemini_available:
            return self.analyze_with_gemini(user_input, relevant_chunks)
        else:
            return self.analyze_with_rules(user_input, relevant_chunks)

    def analyze_with_gemini(self, user_input, chunks):
        """ä½¿ç”¨ Gemini åˆ†æ"""
        print("ğŸ¤– ä½¿ç”¨ Gemini AI åˆ†æ...")

        context_info = ""
        for i, chunk in enumerate(chunks, 1):
            context_info += f"ã€ç‰‡æ®µ{i}ã€‘{chunk['title']}: {chunk['content']}\n"

        prompt = f"""ä½ æ˜¯å¤±æ™ºç—‡æ—©æœŸè­¦è¨Šåˆ†æåŠ©ç†ã€‚

åƒè€ƒè³‡æ–™ï¼š
{context_info}

ä½¿ç”¨è€…æè¿°ï¼š"{user_input}"

è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼š
{{
    "matched_warning_code": "M1-XX",
    "symptom_title": "ç›¸ç¬¦çš„è­¦è¨Šæ¨™é¡Œ",
    "user_behavior_summary": "ä½¿ç”¨è€…è¡Œç‚ºæ‘˜è¦",
    "normal_behavior": "æ­£å¸¸è€åŒ–è¡¨ç¾",
    "dementia_indicator": "å¤±æ™ºç—‡è­¦è¨ŠæŒ‡æ¨™",
    "action_suggestion": "å»ºè­°è¡Œå‹•",
    "confidence_level": "high/medium/low",
    "source": "TADA åå¤§è­¦è¨Š"
}}"""

        try:
            response = self.model.generate_content(prompt)
            result_text = response.text
            analysis_result = self.safe_json_parse(result_text)

            analysis_result["retrieved_chunks"] = chunks
            analysis_result["total_chunks_used"] = len(chunks)
            analysis_result["lightweight_rag"] = True
            analysis_result["analysis_method"] = "gemini_ai"

            return analysis_result

        except Exception as e:
            print(f"âš ï¸  Gemini åˆ†æå¤±æ•—: {e}")
            return self.analyze_with_rules(user_input, chunks)

    def analyze_with_rules(self, user_input, chunks):
        """è¦å‰‡åŸºç¤åˆ†æ"""
        print("ğŸ“‹ ä½¿ç”¨è¦å‰‡åˆ†æ...")

        if not chunks:
            return self.get_fallback_response(user_input, [])

        best_chunk = chunks[0]
        similarity = best_chunk.get('similarity_score', 0)

        if similarity > 0.3:
            confidence = "high"
        elif similarity > 0.1:
            confidence = "medium"
        else:
            confidence = "low"

        return {
            "matched_warning_code": best_chunk['chunk_id'],
            "symptom_title": best_chunk['title'],
            "user_behavior_summary": user_input[:100],
            "normal_behavior": "éš¨å¹´é½¡å¢é•·çš„è¼•å¾®è®ŠåŒ–å¯èƒ½æ˜¯æ­£å¸¸çš„",
            "dementia_indicator": "æŒçºŒæˆ–æ˜é¡¯çš„ç—‡ç‹€å¯èƒ½éœ€è¦é—œæ³¨",
            "action_suggestion": "å»ºè­°è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡é€²è¡Œè©•ä¼°",
            "confidence_level": confidence,
            "source": "TADA åå¤§è­¦è¨Š",
            "retrieved_chunks": chunks,
            "total_chunks_used": len(chunks),
            "lightweight_rag": True,
            "analysis_method": "rule_based",
            "similarity_scores": [chunk.get('similarity_score', 0) for chunk in chunks]
        }

    def safe_json_parse(self, text):
        """å®‰å…¨JSONè§£æ"""
        try:
            return json.loads(text)
        except:
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass

            json_match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except:
                    pass

            return self.get_basic_fallback()

    def get_fallback_response(self, user_input, chunks):
        """å‚™ç”¨å›æ‡‰"""
        return {
            "matched_warning_code": "M1-GENERAL",
            "symptom_title": "éœ€è¦é€²ä¸€æ­¥é—œæ³¨çš„ç—‡ç‹€",
            "user_behavior_summary": user_input[:100],
            "normal_behavior": "éš¨è‘—å¹´é½¡å¢é•·ï¼Œè¼•å¾®çš„èªçŸ¥è®ŠåŒ–æ˜¯æ­£å¸¸çš„",
            "dementia_indicator": "å¦‚æœç—‡ç‹€æŒçºŒæˆ–åŠ é‡ï¼Œå¯èƒ½éœ€è¦å°ˆæ¥­è©•ä¼°",
            "action_suggestion": "å»ºè­°è¨˜éŒ„ç—‡ç‹€è®ŠåŒ–ï¼Œå¿…è¦æ™‚è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡",
            "confidence_level": "low",
            "source": "TADA åå¤§è­¦è¨Š",
            "retrieved_chunks": chunks,
            "total_chunks_used": len(chunks),
            "lightweight_rag": True,
            "fallback_used": True
        }

    def get_basic_fallback(self):
        """åŸºæœ¬å‚™ç”¨å›æ‡‰"""
        return {
            "matched_warning_code": "M1-GENERAL",
            "symptom_title": "ä¸€èˆ¬æ€§é—œæ³¨",
            "user_behavior_summary": "æè¿°çš„æƒ…æ³éœ€è¦è©•ä¼°",
            "normal_behavior": "éš¨è‘—å¹´é½¡å¢é•·ï¼Œè¼•å¾®çš„èªçŸ¥è®ŠåŒ–æ˜¯æ­£å¸¸çš„",
            "dementia_indicator": "æŒçºŒæˆ–åš´é‡çš„èªçŸ¥è®ŠåŒ–éœ€è¦é—œæ³¨",
            "action_suggestion": "å¦‚æœ‰ç–‘æ…®ï¼Œå»ºè­°è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡",
            "confidence_level": "low",
            "source": "ä¸€èˆ¬é†«ç™‚å»ºè­°"
        }

def test_lightweight_rag():
    """æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦")
    print("=" * 50)

    api_key = os.getenv('AISTUDIO_API_KEY')
    if not api_key:
        print("âš ï¸  æœªè¨­å®š API Keyï¼Œä½¿ç”¨è¦å‰‡åˆ†æ")

    engine = LightweightRAGEngine(api_key)

    test_cases = [
        "åª½åª½æœ€è¿‘å¸¸å¿˜è¨˜é—œç“¦æ–¯çˆ",
        "çˆ¸çˆ¸é–‹è»Šæ™‚ç¶“å¸¸è¿·è·¯",
        "å¥¶å¥¶é‡è¤‡å•åŒæ¨£çš„å•é¡Œ"
    ]

    for i, test_input in enumerate(test_cases, 1):
        print(f"\næ¸¬è©¦ {i}: {test_input}")
        print("-" * 30)

        try:
            result = engine.analyze_with_lightweight_rag(test_input)

            print(f"ğŸ“‹ è­¦è¨Š: {result.get('matched_warning_code', 'N/A')}")
            print(f"ğŸ¯ æ¨™é¡Œ: {result.get('symptom_title', 'N/A')}")
            print(f"ğŸ“Š ä¿¡å¿ƒ: {result.get('confidence_level', 'N/A')}")
            print(f"ğŸ” æ–¹æ³•: {result.get('analysis_method', 'N/A')}")
            print("âœ… æ¸¬è©¦é€šé")

        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

    print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
    return engine

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ è¼•é‡ç´š RAG ç³»çµ±")
    print("=" * 50)
    print("âœ… é©ç”¨æ–¼ Replit")
    print("âœ… ç„¡éœ€é¡å¤–ä¾è³´")
    print("âœ… è¨˜æ†¶é«”å‹å–„")

    engine = test_lightweight_rag()

    print(f"\nğŸ“Œ ä½¿ç”¨èªªæ˜ï¼š")
    print("åœ¨ç¾æœ‰ API ä¸­å°å…¥:")
    print("from enhanced.lightweight_rag_for_replit import LightweightRAGEngine")

    return engine

if __name__ == "__main__":
    main()