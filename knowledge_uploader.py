# knowledge_uploader.py - æ‰¹é‡ä¸Šä¼ çŸ¥è¯†æ–‡ä»¶åˆ° Pinecone
from pinecone import Pinecone
import json
import csv
import hashlib
import random
import time
from pathlib import Path
from typing import List, Dict

# åˆå§‹åŒ– Pinecone
pc = Pinecone(api_key="pcsk_4WvWXx_G5bRUFdFNzLzRHNM9rkvFMvC18TMRTaeYXVCxmWSPQLmKr4xAs4UaZg5NvVb69m")
index = pc.Index("dementia-care-knowledge")

def create_embedding(text, dimension=1024):
    """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡"""
    text_hash = hashlib.md5(text.encode()).hexdigest()
    vector = []
    for i in range(dimension):
        seed = int(text_hash[i % len(text_hash)], 16) + i
        random.seed(seed)
        vector.append(random.uniform(-1, 1))

    magnitude = sum(x*x for x in vector) ** 0.5
    return [x/magnitude for x in vector] if magnitude > 0 else [1.0/dimension] * dimension

class KnowledgeUploader:
    """çŸ¥è¯†æ–‡ä»¶ä¸Šä¼ å™¨"""

    def __init__(self):
        self.supported_formats = ['.json', '.csv', '.txt', '.md']
        self.batch_size = 10  # æ¯æ‰¹ä¸Šä¼ æ•°é‡

    def process_json_file(self, file_path: str) -> List[Dict]:
        """å¤„ç† JSON æ–‡ä»¶"""
        print(f"ğŸ“„ Processing JSON file: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            chunks = []

            # å¤„ç†ä¸åŒçš„ JSON ç»“æ„
            if isinstance(data, list):
                # å¦‚æœæ˜¯æ•°ç»„æ ¼å¼
                for i, item in enumerate(data):
                    chunk = self._extract_chunk_from_dict(item, f"json-{i:03d}")
                    if chunk:
                        chunks.append(chunk)
            elif isinstance(data, dict):
                if 'chunks' in data:
                    # å¦‚æœæœ‰ chunks å­—æ®µ
                    for i, item in enumerate(data['chunks']):
                        chunk = self._extract_chunk_from_dict(item, f"json-{i:03d}")
                        if chunk:
                            chunks.append(chunk)
                else:
                    # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡
                    chunk = self._extract_chunk_from_dict(data, "json-001")
                    if chunk:
                        chunks.append(chunk)

            print(f"âœ… Extracted {len(chunks)} chunks from JSON")
            return chunks

        except Exception as e:
            print(f"âŒ Error processing JSON file: {str(e)}")
            return []

    def process_csv_file(self, file_path: str) -> List[Dict]:
        """å¤„ç† CSV æ–‡ä»¶"""
        print(f"ğŸ“„ Processing CSV file: {file_path}")

        try:
            chunks = []

            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                for i, row in enumerate(reader):
                    chunk = {
                        'id': f"csv-{i:03d}",
                        'title': row.get('title', row.get('æ¨™é¡Œ', f"CSV Row {i+1}")),
                        'content': row.get('content', row.get('å…§å®¹', '')),
                        'type': row.get('type', row.get('é¡å‹', 'general')),
                        'source': file_path,
                        'metadata': {k: v for k, v in row.items() if k not in ['title', 'content', 'type']}
                    }

                    # ç¡®ä¿æœ‰å†…å®¹
                    if chunk['content'].strip():
                        chunks.append(chunk)

            print(f"âœ… Extracted {len(chunks)} chunks from CSV")
            return chunks

        except Exception as e:
            print(f"âŒ Error processing CSV file: {str(e)}")
            return []

    def process_text_file(self, file_path: str) -> List[Dict]:
        """å¤„ç†çº¯æ–‡æœ¬/Markdown æ–‡ä»¶"""
        print(f"ğŸ“„ Processing text file: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

            chunks = []
            for i, paragraph in enumerate(paragraphs):
                # æå–æ ‡é¢˜ï¼ˆå¦‚æœæ˜¯ markdown æ ¼å¼ï¼‰
                if paragraph.startswith('#'):
                    lines = paragraph.split('\n')
                    title = lines[0].strip('#').strip()
                    content = '\n'.join(lines[1:]).strip()
                else:
                    title = f"Section {i+1}"
                    content = paragraph

                if len(content) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    chunk = {
                        'id': f"txt-{i:03d}",
                        'title': title,
                        'content': content,
                        'type': 'text_section',
                        'source': file_path
                    }
                    chunks.append(chunk)

            print(f"âœ… Extracted {len(chunks)} chunks from text file")
            return chunks

        except Exception as e:
            print(f"âŒ Error processing text file: {str(e)}")
            return []

    def _extract_chunk_from_dict(self, item: Dict, default_id: str) -> Dict:
        """ä»å­—å…¸ä¸­æå– chunk æ•°æ®"""
        # å°è¯•ä¸åŒçš„å­—æ®µåç»„åˆ
        title_fields = ['title', 'æ¨™é¡Œ', 'name', 'åç¨±', 'heading']
        content_fields = ['content', 'å…§å®¹', 'text', 'æ–‡å­—', 'description', 'æè¿°']
        type_fields = ['type', 'é¡å‹', 'category', 'åˆ†é¡', 'chunk_type']

        title = None
        content = None
        chunk_type = 'general'

        # æŸ¥æ‰¾æ ‡é¢˜
        for field in title_fields:
            if field in item and item[field]:
                title = str(item[field])
                break

        # æŸ¥æ‰¾å†…å®¹
        for field in content_fields:
            if field in item and item[field]:
                content = str(item[field])
                break

        # æŸ¥æ‰¾ç±»å‹
        for field in type_fields:
            if field in item and item[field]:
                chunk_type = str(item[field])
                break

        if not title or not content:
            return None

        return {
            'id': item.get('id', item.get('chunk_id', default_id)),
            'title': title,
            'content': content,
            'type': chunk_type,
            'metadata': {k: v for k, v in item.items() 
                        if k not in ['id', 'chunk_id', 'title', 'content', 'type']}
        }

    def upload_chunks_to_pinecone(self, chunks: List[Dict]):
        """æ‰¹é‡ä¸Šä¼  chunks åˆ° Pinecone"""
        print(f"ğŸš€ Uploading {len(chunks)} chunks to Pinecone...")

        total_uploaded = 0

        # åˆ†æ‰¹ä¸Šä¼ 
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            vectors_to_upload = []

            for chunk in batch:
                try:
                    # åˆ›å»ºåµŒå…¥å‘é‡
                    text_content = f"{chunk['title']} {chunk['content']}"
                    embedding = create_embedding(text_content)

                    # å‡†å¤‡å‘é‡æ•°æ®
                    vector_data = {
                        'id': chunk['id'],
                        'values': embedding,
                        'metadata': {
                            'title': chunk['title'][:200],  # é™åˆ¶é•¿åº¦
                            'content': chunk['content'][:500],
                            'type': chunk['type'],
                            'source': chunk.get('source', 'uploaded'),
                            **chunk.get('metadata', {})
                        }
                    }

                    vectors_to_upload.append(vector_data)

                except Exception as e:
                    print(f"âš ï¸ Skipping chunk {chunk.get('id', 'unknown')}: {str(e)}")
                    continue

            # ä¸Šä¼ è¿™ä¸€æ‰¹
            if vectors_to_upload:
                try:
                    upsert_response = index.upsert(vectors=vectors_to_upload)
                    uploaded_count = upsert_response.upserted_count
                    total_uploaded += uploaded_count
                    print(f"âœ… Batch {i//self.batch_size + 1}: Uploaded {uploaded_count} vectors")

                    # ç¨å¾®ç­‰å¾…ä¸€ä¸‹
                    time.sleep(1)

                except Exception as e:
                    print(f"âŒ Failed to upload batch {i//self.batch_size + 1}: {str(e)}")

        print(f"ğŸ‰ Total uploaded: {total_uploaded} vectors")
        return total_uploaded

    def upload_file(self, file_path: str):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        file_path = Path(file_path)

        if not file_path.exists():
            print(f"âŒ File not found: {file_path}")
            return False

        if file_path.suffix.lower() not in self.supported_formats:
            print(f"âŒ Unsupported file format: {file_path.suffix}")
            print(f"Supported formats: {', '.join(self.supported_formats)}")
            return False

        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        chunks = []
        if file_path.suffix.lower() == '.json':
            chunks = self.process_json_file(str(file_path))
        elif file_path.suffix.lower() == '.csv':
            chunks = self.process_csv_file(str(file_path))
        elif file_path.suffix.lower() in ['.txt', '.md']:
            chunks = self.process_text_file(str(file_path))

        if chunks:
            uploaded_count = self.upload_chunks_to_pinecone(chunks)
            print(f"âœ… Successfully processed {file_path.name}: {uploaded_count} chunks uploaded")
            return True
        else:
            print(f"âŒ No valid chunks extracted from {file_path.name}")
            return False

    def upload_directory(self, directory_path: str):
        """ä¸Šä¼ æ•´ä¸ªç›®å½•çš„æ–‡ä»¶"""
        directory = Path(directory_path)

        if not directory.exists():
            print(f"âŒ Directory not found: {directory}")
            return

        # æŸ¥æ‰¾æ”¯æŒçš„æ–‡ä»¶
        files = []
        for ext in self.supported_formats:
            files.extend(directory.glob(f"*{ext}"))
            files.extend(directory.glob(f"**/*{ext}"))  # é€’å½’æŸ¥æ‰¾

        if not files:
            print(f"âŒ No supported files found in {directory}")
            return

        print(f"ğŸ“ Found {len(files)} files to process")

        successful = 0
        for file_path in files:
            print(f"\n{'='*50}")
            if self.upload_file(str(file_path)):
                successful += 1

        print(f"\nğŸ‰ Processing complete: {successful}/{len(files)} files uploaded successfully")

def create_sample_files():
    """åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ä¾›æµ‹è¯•"""
    print("ğŸ“ Creating sample files...")

    # åˆ›å»º JSON ç¤ºä¾‹
    json_data = {
        "chunks": [
            {
                "chunk_id": "sample-001",
                "title": "å¤±æ™ºç—‡æ—¥å¸¸ç…§è­·",
                "content": "å¤±æ™ºç—‡æ‚£è€…çš„æ—¥å¸¸ç…§è­·éœ€è¦è€å¿ƒå’ŒæŠ€å·§ï¼ŒåŒ…æ‹¬å”åŠ©æ´—æ¾¡ã€ç”¨é¤ã€æœè—¥ç­‰åŸºæœ¬ç”Ÿæ´»éœ€æ±‚ã€‚",
                "chunk_type": "care_guide",
                "keywords": ["æ—¥å¸¸ç…§è­·", "ç”Ÿæ´»å”åŠ©", "åŸºæœ¬éœ€æ±‚"]
            },
            {
                "chunk_id": "sample-002", 
                "title": "ç’°å¢ƒå®‰å…¨è¨­è¨ˆ",
                "content": "ç‚ºå¤±æ™ºç—‡æ‚£è€…è¨­è¨ˆå®‰å…¨çš„å±…ä½ç’°å¢ƒå¾ˆé‡è¦ï¼ŒåŒ…æ‹¬ç§»é™¤éšœç¤™ç‰©ã€å¢åŠ ç…§æ˜ã€å®‰è£æ‰¶æ‰‹ç­‰ã€‚",
                "chunk_type": "safety_guide",
                "keywords": ["ç’°å¢ƒå®‰å…¨", "å±…ä½è¨­è¨ˆ", "éšœç¤™ç‰©"]
            }
        ]
    }

    with open('sample_knowledge.json', 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    # åˆ›å»º CSV ç¤ºä¾‹
    csv_data = [
        ["title", "content", "type"],
        ["è—¥ç‰©ç®¡ç†", "å¤±æ™ºç—‡æ‚£è€…å®¹æ˜“å¿˜è¨˜æœè—¥ï¼Œå»ºè­°ä½¿ç”¨è—¥ç›’åˆ†è£å’Œé¬§é˜æé†’ã€‚", "medication"],
        ["ç‡Ÿé¤Šç…§è­·", "ç¢ºä¿æ‚£è€…æ”å–å‡è¡¡ç‡Ÿé¤Šï¼Œæ³¨æ„æ°´åˆ†è£œå……å’Œååš¥å®‰å…¨ã€‚", "nutrition"],
        ["é‹å‹•ç™‚æ³•", "é©åº¦é‹å‹•æœ‰åŠ©æ–¼ç¶­æŒèªçŸ¥åŠŸèƒ½å’Œèº«é«”å¥åº·ã€‚", "exercise"]
    ]

    with open('sample_knowledge.csv', 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(csv_data)

    # åˆ›å»º Markdown ç¤ºä¾‹
    markdown_content = """# å¤±æ™ºç—‡ç…§è­·æŒ‡å—

## èªçŸ¥è¨“ç·´

èªçŸ¥è¨“ç·´å¯ä»¥å¹«åŠ©å»¶ç·©èªçŸ¥åŠŸèƒ½é€€åŒ–ï¼ŒåŒ…æ‹¬è¨˜æ†¶éŠæˆ²ã€æ‹¼åœ–ã€é–±è®€ç­‰æ´»å‹•ã€‚

## ç¤¾äº¤äº’å‹•

ç¶­æŒç¤¾äº¤äº’å‹•å°å¤±æ™ºç—‡æ‚£è€…å¾ˆé‡è¦ï¼Œå¯ä»¥åƒåŠ ç¤¾å€æ´»å‹•æˆ–å®šæœŸèˆ‡å®¶äººæœ‹å‹èšæœƒã€‚

## ç¡çœ ç®¡ç†

è‰¯å¥½çš„ç¡çœ å“è³ªæœ‰åŠ©æ–¼èªçŸ¥åŠŸèƒ½ï¼Œå»ºè­°å»ºç«‹è¦å¾‹çš„ä½œæ¯æ™‚é–“ã€‚
"""

    with open('sample_knowledge.md', 'w', encoding='utf-8') as f:
        f.write(markdown_content)

    print("âœ… Sample files created:")
    print("  - sample_knowledge.json")
    print("  - sample_knowledge.csv") 
    print("  - sample_knowledge.md")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Pinecone Knowledge Uploader")
    print("=" * 50)

    uploader = KnowledgeUploader()

    while True:
        print("\nğŸ“‹ Options:")
        print("1. Upload single file")
        print("2. Upload directory")
        print("3. Create sample files")
        print("4. Check Pinecone status")
        print("5. Exit")

        choice = input("\nSelect option (1-5): ").strip()

        if choice == '1':
            file_path = input("Enter file path: ").strip()
            uploader.upload_file(file_path)

        elif choice == '2':
            dir_path = input("Enter directory path: ").strip()
            uploader.upload_directory(dir_path)

        elif choice == '3':
            create_sample_files()

        elif choice == '4':
            try:
                stats = index.describe_index_stats()
                print(f"ğŸ“Š Pinecone Status:")
                print(f"  Total vectors: {stats.total_vector_count}")
                print(f"  Dimension: {stats.dimension}")
                print(f"  Index fullness: {stats.index_fullness}")
            except Exception as e:
                print(f"âŒ Error: {str(e)}")

        elif choice == '5':
            print("ğŸ‘‹ Goodbye!")
            break

        else:
            print("âŒ Invalid option")

if __name__ == "__main__":
    main()

# å¿«é€Ÿä¸Šä¼ å‘½ä»¤ç¤ºä¾‹:
# uploader = KnowledgeUploader()
# uploader.upload_file("your_file.json")
# uploader.upload_directory("./knowledge_files/")

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ç¤ºä¾‹:

# JSON æ ¼å¼:
"""
{
  "chunks": [
    {
      "chunk_id": "001",
      "title": "æ ‡é¢˜",
      "content": "å†…å®¹",
      "chunk_type": "ç±»å‹",
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
    }
  ]
}
"""

# CSV æ ¼å¼:
"""
title,content,type
å¤±æ™ºç—‡ç—‡çŠ¶,è®°å¿†åŠ›å‡é€€æ˜¯ä¸»è¦ç—‡çŠ¶,symptom
ç…§æŠ¤æŠ€å·§,ä¿æŒè€å¿ƒå¾ˆé‡è¦,care_tip
"""

# Markdown/Text æ ¼å¼:
"""
# ä¸»æ ‡é¢˜

## å­æ ‡é¢˜1
å†…å®¹æ®µè½1

## å­æ ‡é¢˜2  
å†…å®¹æ®µè½2
"""