# lightweight_test.py - ä¿®å¤ç»´åº¦åŒ¹é…å’Œç¼©è¿›é—®é¢˜
from pinecone import Pinecone, ServerlessSpec
import json
import time
import random

# ä½ çš„ Pinecone å®¢æˆ·ç«¯
pc = Pinecone(api_key="pcsk_4WvWXx_G5bRUFdFNzLzRHNM9rkvFMvC18TMRTaeYXVCxmWSPQLmKr4xAs4UaZg5NvVb69m")

INDEX_NAME = "dementia-care-knowledge"

def test_pinecone_basic():
    """åŸºç¡€ Pinecone æµ‹è¯•"""
    print("ğŸ”„ Testing Pinecone connection...")

    try:
        # åˆ—å‡ºç´¢å¼•
        indexes = pc.list_indexes()
        print(f"âœ… Connected! Found {len(indexes)} indexes")

        # æ£€æŸ¥ç°æœ‰ç´¢å¼•
        existing_names = [idx.name for idx in indexes]

        if INDEX_NAME in existing_names:
            print("âœ… Index already exists!")
            index = pc.Index(INDEX_NAME)
            stats = index.describe_index_stats()
            dimension = stats.dimension
            print(f"ğŸ“ Index dimension: {dimension}")
            return index, dimension
        else:
            print(f"âŒ Index '{INDEX_NAME}' not found")
            return None, None

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return None, None

def create_simple_embedding(text, dimension):
    """åˆ›å»ºæŒ‡å®šç»´åº¦çš„ç®€å•æ–‡æœ¬åµŒå…¥å‘é‡"""
    import hashlib

    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—
    text_hash = hashlib.md5(text.encode()).hexdigest()

    # åˆ›å»ºæŒ‡å®šç»´åº¦çš„å‘é‡
    vector = []
    for i in range(dimension):
        # ä½¿ç”¨å“ˆå¸Œå€¼å’Œä½ç½®åˆ›å»ºä¼ªéšæœºä½†ä¸€è‡´çš„å‘é‡
        seed = int(text_hash[i % len(text_hash)], 16) + i
        random.seed(seed)
        vector.append(random.uniform(-1, 1))

    # å½’ä¸€åŒ–å‘é‡
    magnitude = sum(x*x for x in vector) ** 0.5
    if magnitude > 0:
        return [x/magnitude for x in vector]
    else:
        return [1.0/dimension] * dimension

def upload_demo_data(index, dimension):
    """ä¸Šä¼ æ¼”ç¤ºæ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç»´åº¦ï¼‰"""
    print(f"ğŸ“š Uploading demo knowledge with {dimension}D embeddings...")

    demo_chunks = [
        {
            "id": "demo-001",
            "title": "å¤±æ™ºç—‡åå¤§è­¦è¨Š", 
            "content": "å¤±æ™ºç—‡çš„åå¤§è­¦è¨ŠåŒ…æ‹¬è¨˜æ†¶åŠ›æ¸›é€€ã€è¨ˆåŠƒäº‹æƒ…æˆ–è§£æ±ºå•é¡Œæœ‰å›°é›£ã€ç„¡æ³•å‹ä»»åŸæœ¬ç†Ÿæ‚‰çš„äº‹å‹™ç­‰ã€‚åŠæ—©ç™¼ç¾é€™äº›è­¦è¨Šæœ‰åŠ©æ–¼æ—©æœŸè¨ºæ–·å’Œæ²»ç™‚ã€‚",
            "type": "warning_sign",
            "keywords": ["è¨˜æ†¶åŠ›", "è­¦è¨Š", "è¨ºæ–·", "æ²»ç™‚"]
        },
        {
            "id": "demo-002",
            "title": "BPSDè¡Œç‚ºå¿ƒç†ç—‡ç‹€",
            "content": "BPSDæŒ‡å¤±æ™ºç—‡æ‚£è€…çš„è¡Œç‚ºå¿ƒç†ç—‡ç‹€ï¼ŒåŒ…æ‹¬éŠèµ°ã€æ”»æ“Šè¡Œç‚ºã€å¦„æƒ³ã€å¹»è¦ºç­‰ã€‚äº†è§£é€™äº›ç—‡ç‹€æœ‰åŠ©æ–¼æä¾›é©ç•¶çš„ç…§è­·ã€‚",
            "type": "bpsd_symptom",
            "keywords": ["BPSD", "è¡Œç‚ºç—‡ç‹€", "éŠèµ°", "ç…§è­·"]
        },
        {
            "id": "demo-003",
            "title": "å¤±æ™ºç—‡æºé€šæŠ€å·§",
            "content": "èˆ‡å¤±æ™ºç—‡æ‚£è€…æºé€šæ™‚è¦ä¿æŒè€å¿ƒï¼Œä½¿ç”¨ç°¡å–®æ˜ç¢ºçš„èªè¨€ï¼Œé¿å…çˆ­è¾¯æˆ–ç³¾æ­£ï¼Œå¤šç”¨è‚¢é«”èªè¨€å’Œè¡¨æƒ…ä¾†è¡¨é”é—œæ„›ã€‚",
            "type": "coping_strategy",
            "keywords": ["æºé€š", "æŠ€å·§", "è€å¿ƒ", "è‚¢é«”èªè¨€"]
        },
        {
            "id": "demo-004", 
            "title": "ç…§è­·è€…å£“åŠ›ç®¡ç†",
            "content": "ç…§è­·è€…å®¹æ˜“ç”¢ç”Ÿèº«å¿ƒå£“åŠ›ï¼Œéœ€è¦é©ç•¶ä¼‘æ¯ã€å°‹æ±‚æ”¯æŒï¼Œä¸¦å­¸ç¿’å£“åŠ›èª¿é©æŠ€å·§ã€‚å»ºè­°å®šæœŸåƒåŠ æ”¯æŒåœ˜é«”æˆ–è«®è©¢å°ˆæ¥­äººå“¡ã€‚",
            "type": "caregiver_support",
            "keywords": ["ç…§è­·è€…", "å£“åŠ›ç®¡ç†", "æ”¯æŒåœ˜é«”", "è«®è©¢"]
        },
        {
            "id": "demo-005",
            "title": "å¤±æ™ºç—‡ç”¨è—¥å®‰å…¨",
            "content": "å¤±æ™ºç—‡æ‚£è€…ç”¨è—¥éœ€è¦ç‰¹åˆ¥æ³¨æ„åŠ‘é‡ã€æ™‚é–“å’Œå‰¯ä½œç”¨ï¼Œå»ºè­°ä½¿ç”¨è—¥ç›’åˆ†è£ä¸¦å®šæœŸæª¢è¦–ã€‚å®¶å±¬æ‡‰èˆ‡é†«å¸«å¯†åˆ‡é…åˆã€‚",
            "type": "medication_safety",
            "keywords": ["ç”¨è—¥å®‰å…¨", "åŠ‘é‡", "å‰¯ä½œç”¨", "è—¥ç›’"]
        }
    ]

    try:
        vectors_to_upload = []

        for chunk in demo_chunks:
            # å‰µå»ºæ­£ç¢ºç¶­åº¦çš„åµŒå…¥
            content_text = f"{chunk['title']} {chunk['content']} {' '.join(chunk['keywords'])}"
            embedding = create_simple_embedding(content_text, dimension)

            vector_data = {
                'id': chunk['id'],
                'values': embedding,
                'metadata': {
                    'title': chunk['title'],
                    'content': chunk['content'][:400],
                    'type': chunk['type'],
                    'keywords': ', '.join(chunk['keywords'])
                }
            }

            vectors_to_upload.append(vector_data)
            print(f"âœ… Prepared: {chunk['title']} ({len(embedding)}D)")

        # æ‰¹é‡ä¸Šä¼ 
        print(f"ğŸ“¤ Uploading {len(vectors_to_upload)} vectors...")
        upsert_response = index.upsert(vectors=vectors_to_upload)
        print(f"ğŸ‰ Successfully uploaded {upsert_response.upserted_count} vectors!")

        # ç­‰å¾…ç´¢å¼•æ›´æ–°
        print("â³ Waiting for index to update...")
        time.sleep(5)
        return True

    except Exception as e:
        print(f"âŒ Upload failed: {str(e)}")
        return False

def test_multiple_queries(index, dimension):
    """æµ‹è¯•å¤šä¸ªæŸ¥è¯¢"""
    test_queries = [
        "å¤±æ™ºç—‡ç—‡çŠ¶",
        "ç…§è­·æŠ€å·§", 
        "è—¥ç‰©æ²»ç™‚",
        "è¡Œç‚ºå•é¡Œ",
        "å£“åŠ›ç®¡ç†"
    ]

    print("ğŸ” Testing multiple queries...")

    for query_text in test_queries:
        try:
            print(f"\nğŸ“ Query: '{query_text}'")

            # åˆ›å»ºæŸ¥è¯¢å‘é‡
            query_vector = create_simple_embedding(query_text, dimension)

            # æŸ¥è¯¢
            results = index.query(
                vector=query_vector,
                top_k=2,
                include_metadata=True
            )

            if results.matches:
                for i, match in enumerate(results.matches, 1):
                    print(f"  {i}. {match.metadata['title']} (Score: {match.score:.4f})")
                    print(f"     Type: {match.metadata['type']}")
                    print(f"     Keywords: {match.metadata.get('keywords', 'N/A')}")
            else:
                print("  No matches found")

        except Exception as e:
            print(f"  âŒ Query failed: {str(e)}")

def get_detailed_stats(index):
    """è·å–è¯¦ç»†çš„ç´¢å¼•ç»Ÿè®¡"""
    try:
        stats = index.describe_index_stats()
        print("\nğŸ“Š Detailed Index Statistics:")
        print(f"  Total vectors: {stats.total_vector_count}")
        print(f"  Dimension: {stats.dimension}")
        print(f"  Index fullness: {stats.index_fullness}")
        return stats
    except Exception as e:
        print(f"âŒ Stats error: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Fixed Lightweight Pinecone Test")
    print("=" * 60)

    # 1. æµ‹è¯•è¿æ¥å¹¶è·å–ç´¢å¼•ç»´åº¦
    index, dimension = test_pinecone_basic()
    if not index or not dimension:
        print("âŒ Failed to connect to Pinecone or get index info")
        return

    # 2. è·å–åˆå§‹çŠ¶æ€
    initial_stats = get_detailed_stats(index)

    # 3. ä¸Šä¼ æ¼”ç¤ºæ•°æ®
    if upload_demo_data(index, dimension):
        print("âœ… Demo data uploaded successfully!")
    else:
        print("âš ï¸ Demo data upload failed")
        return

    # 4. æµ‹è¯•å¤šä¸ªæŸ¥è¯¢
    test_multiple_queries(index, dimension)

    # 5. è·å–æœ€ç»ˆçŠ¶æ€
    final_stats = get_detailed_stats(index)

    print("\nğŸ‰ Fixed test completed!")
    print("âœ… Your Pinecone setup is working correctly!")
    print(f"âœ… Using {dimension}D vectors")
    print("\nğŸš€ Ready to build your XAI Flex Message system!")

if __name__ == "__main__":
    main()