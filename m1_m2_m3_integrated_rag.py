# ç°¡åŒ–ç‰ˆ M1+M2+M3 æ•´åˆå¼•æ“
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional

class AnalysisResult:
    """åˆ†æçµæœè³‡æ–™çµæ§‹"""
    def __init__(self):
        self.matched_codes = []
        self.symptom_titles = []
        self.confidence_levels = []
        self.bpsd_analysis = None
        self.stage_detection = None
        self.comprehensive_summary = ""
        self.action_suggestions = []
        self.retrieved_chunks = []
        self.modules_used = []

class M1M2M3IntegratedEngine:
    """M1+M2+M3 æ•´åˆåˆ†æå¼•æ“"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.chunks = []
        self.vocabulary = set()
        
        print("ğŸš€ åˆå§‹åŒ– M1+M2+M3 æ•´åˆå¼•æ“...")
        self._load_all_modules()
        self._build_search_index()
        print("âœ… M1+M2+M3 æ•´åˆå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_all_modules(self):
        """è¼‰å…¥æ‰€æœ‰æ¨¡çµ„è³‡æ–™"""
        
        # è¼‰å…¥ M1 è³‡æ–™
        print("ğŸ“Š è¼‰å…¥ M1 è³‡æ–™...")
        m1_data = self._create_m1_data()
        self.chunks.extend(m1_data)
        print(f"âœ… M1 è¼‰å…¥ï¼š{len(m1_data)} å€‹çŸ¥è­˜ç‰‡æ®µ")
        
        # è¼‰å…¥ M2 è³‡æ–™
        print("ğŸ“Š è¼‰å…¥ M2 è³‡æ–™...")
        try:
            with open('m2_stage_data.json', 'r', encoding='utf-8') as f:
                m2_data = json.load(f)
                self.chunks.extend(m2_data)
                print(f"âœ… M2 è¼‰å…¥ï¼š{len(m2_data)} å€‹çŸ¥è­˜ç‰‡æ®µ")
        except FileNotFoundError:
            print("âš ï¸  M2 è³‡æ–™æª”æ¡ˆæœªæ‰¾åˆ°ï¼Œè·³é M2 æ¨¡çµ„")
        
        # è¼‰å…¥ M3 è³‡æ–™  
        print("ğŸ“Š è¼‰å…¥ M3 BPSD è³‡æ–™...")
        try:
            with open('m3_bpsd_data.json', 'r', encoding='utf-8') as f:
                m3_data = json.load(f)
                self.chunks.extend(m3_data)
                print(f"âœ… M3 è¼‰å…¥ï¼š{len(m3_data)} å€‹çŸ¥è­˜ç‰‡æ®µ")
        except FileNotFoundError:
            print("âš ï¸  M3 è³‡æ–™æª”æ¡ˆæœªæ‰¾åˆ°")
        
        total_chunks = len(self.chunks)
        print(f"ğŸ¯ ç¸½è¨ˆè¼‰å…¥ï¼š{total_chunks} å€‹çŸ¥è­˜ç‰‡æ®µ")
        
        # çµ±è¨ˆå„æ¨¡çµ„
        m1_count = len([c for c in self.chunks if c.get("chunk_id", "").startswith("M1")])
        m2_count = len([c for c in self.chunks if c.get("module_id") == "M2"])
        m3_count = len([c for c in self.chunks if c.get("module_id") == "M3"])
        
        print(f"ğŸ“‹ æ¨¡çµ„åˆ†å¸ƒï¼šM1({m1_count}) + M2({m2_count}) + M3({m3_count}) = {total_chunks}")
    
    def _create_m1_data(self):
        """å‰µå»º M1 åå¤§è­¦è¨Šè³‡æ–™"""
        return [
            {
                "chunk_id": "M1-01",
                "title": "è¨˜æ†¶åŠ›æ¸›é€€å½±éŸ¿æ—¥å¸¸ç”Ÿæ´»",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾å¿˜è¨˜ç´„æœƒä½†äº‹å¾Œæœƒæƒ³èµ·ä¾†ã€‚å¤±æ™ºè­¦è¨Šï¼šå¿˜è¨˜å‰›ç™¼ç”Ÿçš„äº‹ã€é‡è¦æ—¥æœŸæˆ–äº‹ä»¶ï¼›åè¦†è©¢å•åŒæ¨£äº‹æƒ…ã€‚",
                "keywords": ["è¨˜æ†¶", "å¥å¿˜", "å¿˜è¨˜", "é‡è¤‡", "æ—¥å¸¸ç”Ÿæ´»"],
                "confidence_score": 0.95,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-02", 
                "title": "ç„¡æ³•å‹ä»»åŸæœ¬ç†Ÿæ‚‰çš„äº‹å‹™",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾éœ€è¦å”åŠ©ä½¿ç”¨æ–°è¨­å‚™ã€‚å¤±æ™ºè­¦è¨Šï¼šç„¡æ³•å®Œæˆç†Ÿæ‚‰å·¥ä½œï¼Œå¦‚è¿·è·¯ã€ç„¡æ³•ç®¡ç†é ç®—ã€‚",
                "keywords": ["ç†Ÿæ‚‰", "å·¥ä½œ", "è¿·è·¯", "é ç®—", "ç®¡ç†"],
                "confidence_score": 0.9,
                "source": "TADA åå¤§è­¦è¨Š"
            },
            {
                "chunk_id": "M1-03",
                "title": "èªè¨€è¡¨é”å‡ºç¾å•é¡Œ",
                "content": "æ­£å¸¸è€åŒ–ï¼šå¶çˆ¾æƒ³ä¸èµ·é©ç•¶ç”¨è©ã€‚å¤±æ™ºè­¦è¨Šï¼šç”¨éŒ¯è©å½™ã€èªªè©±å…§å®¹æ··äº‚ã€ç„¡æ³•ç†è§£å°è©±ã€‚",
                "keywords": ["èªè¨€", "è¡¨é”", "ç”¨è©", "æ··äº‚", "å°è©±"],
                "confidence_score": 0.88,
                "source": "TADA åå¤§è­¦è¨Š"
            }
        ]
    
    def _build_search_index(self):
        """å»ºç«‹æª¢ç´¢ç´¢å¼•"""
        print("ğŸ” å»ºç«‹æª¢ç´¢ç´¢å¼•...")
        for chunk in self.chunks:
            content = chunk.get("content", "")
            keywords = chunk.get("keywords", [])
            title = chunk.get("title", "")
            words = re.findall(r'[\u4e00-\u9fff]+', content + title + " ".join(keywords))
            self.vocabulary.update(words)
        print(f"âœ… è©å½™åº«å»ºç«‹å®Œæˆï¼š{len(self.vocabulary)} å€‹è©å½™")
    
    def analyze_comprehensive(self, user_input: str):
        """ç¶œåˆåˆ†æï¼šM1+M2+M3"""
        print(f"ğŸ§  ç¶œåˆåˆ†æ: {user_input}")
        
        # æª¢ç´¢ç›¸é—œç‰‡æ®µ
        retrieved_chunks = self._retrieve_relevant_chunks(user_input, top_k=5)
        
        result = AnalysisResult()
        result.retrieved_chunks = retrieved_chunks
        
        # åˆ†æ M1 è­¦è¨Š
        m1_chunks = [c for c in retrieved_chunks if c.get("chunk_id", "").startswith("M1")]
        if m1_chunks:
            # æ ¹æ“šç”¨æˆ¶è¼¸å…¥é¸æ“‡æœ€ç›¸é—œçš„ M1 ç‰‡æ®µ
            best_match = self._select_best_m1_chunk(user_input, m1_chunks)
            result.matched_codes.append(best_match["chunk_id"])
            result.symptom_titles.append(best_match["title"])
            result.confidence_levels.append(self._get_confidence_level(best_match.get("similarity_score", 0)))
            result.modules_used.append("M1")
        
        # åˆ†æ M2 éšæ®µ
        m2_chunks = [c for c in retrieved_chunks if c.get("module_id") == "M2"]
        if m2_chunks:
            best_m2 = max(m2_chunks, key=lambda x: x.get("similarity_score", 0))
            stage = self._extract_stage_from_title(best_m2.get("title", ""))
            if stage:
                result.stage_detection = {
                    "detected_stage": stage,
                    "confidence": best_m2.get("similarity_score", 0)
                }
                result.matched_codes.append(f"M2-{stage[:2].upper()}")
                result.symptom_titles.append(f"{stage}å¤±æ™ºç—‡ç‰¹å¾µ")
                result.confidence_levels.append(self._get_confidence_level(best_m2.get("similarity_score", 0)))
                result.modules_used.append("M2")
        
        # åˆ†æ M3 BPSD
        m3_chunks = [c for c in retrieved_chunks if c.get("module_id") == "M3"]
        if m3_chunks:
            detected_categories = []
            for chunk in m3_chunks[:2]:  # æœ€å¤šæª¢æ¸¬ 2 å€‹ BPSD ç—‡ç‹€
                category = {
                    "code": chunk["chunk_id"],
                    "title": chunk["title"],
                    "confidence_level": self._get_confidence_level(chunk.get("similarity_score", 0)),
                    "severity_analysis": self._analyze_severity(user_input, chunk),
                    "management_strategies": chunk.get("management_strategies", [])
                }
                detected_categories.append(category)
                result.matched_codes.append(chunk["chunk_id"])
                result.symptom_titles.append(chunk["title"])
                result.confidence_levels.append(category["confidence_level"])
            
            if detected_categories:
                result.bpsd_analysis = {
                    "detected_categories": detected_categories,
                    "primary_category": detected_categories[0]
                }
                result.modules_used.append("M3")
        
        # ç”Ÿæˆç¶œåˆæ‘˜è¦å’Œå»ºè­°
        result.comprehensive_summary = self._generate_summary(result, user_input)
        result.action_suggestions = self._generate_suggestions(result)
        
        return result
    
    def _retrieve_relevant_chunks(self, query: str, top_k: int = 5):
        """æª¢ç´¢ç›¸é—œçŸ¥è­˜ç‰‡æ®µ"""
        print(f"ğŸ” æª¢ç´¢æŸ¥è©¢: {query}")
        query_words = set(re.findall(r'[\u4e00-\u9fff]+', query))
        
        # æ ¹æ“šæŸ¥è©¢å…§å®¹èª¿æ•´æª¢ç´¢ç­–ç•¥
        if any(word in query for word in ["è¿·è·¯", "æ–¹å‘", "æ‰¾ä¸åˆ°"]):
            # æ–¹å‘æ„Ÿå•é¡Œï¼Œå„ªå…ˆæª¢ç´¢ M1-02
            priority_chunk_ids = ["M1-02"]
        elif any(word in query for word in ["é‡è¤‡", "åŒæ¨£", "ä¸€æ¨£", "åè¦†"]):
            # é‡è¤‡è¡Œç‚ºï¼Œå„ªå…ˆæª¢ç´¢ M1-01
            priority_chunk_ids = ["M1-01"]
        elif any(word in query for word in ["èªè¨€", "èªªè©±", "è¡¨é”", "è©å½™"]):
            # èªè¨€å•é¡Œï¼Œå„ªå…ˆæª¢ç´¢ M1-03
            priority_chunk_ids = ["M1-03"]
        else:
            # é»˜èªç­–ç•¥
            priority_chunk_ids = []
        
        scored_chunks = []
        for chunk in self.chunks:
            chunk_keywords = set(chunk.get("keywords", []))
            content_words = set(re.findall(r'[\u4e00-\u9fff]+', chunk.get("content", "")))
            title_words = set(re.findall(r'[\u4e00-\u9fff]+', chunk.get("title", "")))
            
            all_chunk_words = chunk_keywords | content_words | title_words
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            overlap = len(query_words & all_chunk_words)
            total_words = len(query_words | all_chunk_words)
            
            if total_words > 0:
                similarity = overlap / total_words
                # é—œéµå­—åŒ¹é…åŠ æ¬Š
                keyword_bonus = len(query_words & chunk_keywords) * 0.3
                similarity += keyword_bonus
                
                # å„ªå…ˆç´šåŠ æ¬Š
                if chunk.get("chunk_id") in priority_chunk_ids:
                    similarity += 0.5  # å¤§å¹…æå‡å„ªå…ˆç´š
                
                chunk_copy = chunk.copy()
                chunk_copy["similarity_score"] = round(similarity, 4)
                scored_chunks.append(chunk_copy)
        
        scored_chunks.sort(key=lambda x: x["similarity_score"], reverse=True)
        top_chunks = scored_chunks[:top_k]
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(top_chunks)} å€‹ç›¸é—œç‰‡æ®µ")
        return top_chunks
    
    def _select_best_m1_chunk(self, user_input: str, m1_chunks: List[Dict]) -> Dict:
        """æ ¹æ“šç”¨æˆ¶è¼¸å…¥é¸æ“‡æœ€ç›¸é—œçš„ M1 ç‰‡æ®µ"""
        user_lower = user_input.lower()
        
        # æ ¹æ“šé—œéµè©åŒ¹é…é¸æ“‡æœ€ç›¸é—œçš„ç‰‡æ®µ
        if any(word in user_lower for word in ["è¿·è·¯", "æ–¹å‘", "æ‰¾ä¸åˆ°", "ç†Ÿæ‚‰"]):
            # æ–¹å‘æ„Ÿå•é¡Œ -> M1-02
            for chunk in m1_chunks:
                if chunk.get("chunk_id") == "M1-02":
                    return chunk
        elif any(word in user_lower for word in ["é‡è¤‡", "åŒæ¨£", "ä¸€æ¨£", "åè¦†", "ç¸½æ˜¯"]):
            # é‡è¤‡è¡Œç‚º -> M1-01
            for chunk in m1_chunks:
                if chunk.get("chunk_id") == "M1-01":
                    return chunk
        elif any(word in user_lower for word in ["èªè¨€", "èªªè©±", "è¡¨é”", "è©å½™", "ç”¨è©"]):
            # èªè¨€å•é¡Œ -> M1-03
            for chunk in m1_chunks:
                if chunk.get("chunk_id") == "M1-03":
                    return chunk
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºåŒ¹é…ï¼Œé¸æ“‡ç›¸ä¼¼åº¦æœ€é«˜çš„
        return max(m1_chunks, key=lambda x: x.get("similarity_score", 0))
    
    def _get_confidence_level(self, score: float) -> str:
        """è½‰æ›æ•¸å€¼ç‚ºä¿¡å¿ƒåº¦ç­‰ç´š"""
        if score >= 0.3:
            return "high"
        elif score >= 0.15:
            return "medium"
        else:
            return "low"
    
    def _extract_stage_from_title(self, title: str) -> str:
        """å¾æ¨™é¡Œæå–éšæ®µ"""
        if "è¼•åº¦" in title:
            return "è¼•åº¦"
        elif "ä¸­åº¦" in title:
            return "ä¸­åº¦"
        elif "é‡åº¦" in title:
            return "é‡åº¦"
        return ""
    
    def _analyze_severity(self, user_input: str, bpsd_chunk: Dict) -> Dict:
        """åˆ†æ BPSD ç—‡ç‹€åš´é‡ç¨‹åº¦"""
        severity_indicators = bpsd_chunk.get("severity_indicators", {})
        user_lower = user_input.lower()
        
        severity_scores = {"è¼•åº¦": 0, "ä¸­åº¦": 0, "é‡åº¦": 0}
        
        for severity, indicators in severity_indicators.items():
            for indicator in indicators:
                if any(word in user_lower for word in indicator.lower().split()):
                    severity_scores[severity] += 1
        
        if max(severity_scores.values()) > 0:
            detected_severity = max(severity_scores, key=severity_scores.get)
            return {
                "severity": detected_severity,
                "scores": severity_scores,
                "confidence": severity_scores[detected_severity]
            }
        
        return {"severity": "æœªç¢ºå®š", "scores": severity_scores, "confidence": 0}
    
    def _generate_summary(self, result: AnalysisResult, user_input: str = "") -> str:
        """ç”Ÿæˆç¶œåˆåˆ†ææ‘˜è¦"""
        summary_parts = []
        
        # æ ¹æ“šå…·é«”ç—‡ç‹€å’Œç”¨æˆ¶è¼¸å…¥ç”Ÿæˆæ‘˜è¦
        if result.symptom_titles:
            # æå–ä¸»è¦ç—‡ç‹€
            main_symptoms = []
            for title in result.symptom_titles[:2]:  # æœ€å¤šå–å‰2å€‹ç—‡ç‹€
                # æ ¹æ“šç”¨æˆ¶è¼¸å…¥å’Œç—‡ç‹€æ¨™é¡Œåˆ¤æ–·å…·é«”ç—‡ç‹€
                if "è¨˜æ†¶åŠ›æ¸›é€€" in title:
                    if any(word in user_input for word in ["é‡è¤‡", "åŒæ¨£", "ä¸€æ¨£", "åè¦†", "ç¸½æ˜¯"]):
                        main_symptoms.append("é‡è¤‡è¡Œç‚º")
                    else:
                        main_symptoms.append("è¨˜æ†¶åŠ›æ¸›é€€")
                elif "ç„¡æ³•å‹ä»»" in title or "ç†Ÿæ‚‰" in title:
                    main_symptoms.append("æ–¹å‘æ„Ÿå•é¡Œ")
                elif "èªè¨€è¡¨é”" in title or "è¡¨é”" in title:
                    main_symptoms.append("èªè¨€è¡¨é”å›°é›£")
                elif "é‡è¤‡" in title or "åè¦†" in title:
                    main_symptoms.append("é‡è¤‡è¡Œç‚º")
                else:
                    main_symptoms.append(title.split("ï¼š")[0] if "ï¼š" in title else title)
            
            if main_symptoms:
                summary_parts.append(f"è§€å¯Ÿåˆ°{', '.join(main_symptoms)}ç­‰ç—‡ç‹€")
        
        # éšæ®µè©•ä¼°
        if "M2" in result.modules_used and result.stage_detection:
            stage = result.stage_detection.get("detected_stage", "")
            if stage:
                summary_parts.append(f"è©•ä¼°ç‚º{stage}éšæ®µ")
        
        # BPSD ç—‡ç‹€
        if "M3" in result.modules_used and result.bpsd_analysis:
            bpsd_count = len(result.bpsd_analysis.get("detected_categories", []))
            if bpsd_count > 0:
                summary_parts.append(f"ç™¼ç¾{bpsd_count}ç¨®è¡Œç‚ºå¿ƒç†ç—‡ç‹€")
        
        # ç”Ÿæˆæœ€çµ‚æ‘˜è¦
        if summary_parts:
            summary = "ï¼›".join(summary_parts) + "ã€‚"
            
            # æ ¹æ“šç—‡ç‹€åš´é‡ç¨‹åº¦æ·»åŠ å»ºè­°
            if any("é‡åº¦" in part for part in summary_parts):
                summary += "å»ºè­°ç«‹å³å°‹æ±‚å°ˆæ¥­é†«ç™‚è©•ä¼°ã€‚"
            elif any("ä¸­åº¦" in part for part in summary_parts):
                summary += "å»ºè­°ç›¡å¿«è«®è©¢é†«ç™‚å°ˆæ¥­äººå“¡ã€‚"
            else:
                summary += "å»ºè­°å°‹æ±‚å°ˆæ¥­é†«ç™‚è©•ä¼°ã€‚"
        else:
            summary = "å»ºè­°æŒçºŒè§€å¯Ÿä¸¦é©æ™‚è«®è©¢é†«ç™‚å°ˆæ¥­äººå“¡ã€‚"
        
        return summary
    
    def _generate_suggestions(self, result: AnalysisResult) -> List[str]:
        """ç”Ÿæˆè¡Œå‹•å»ºè­°"""
        suggestions = []
        
        # M3 BPSD ç®¡ç†å»ºè­°
        if result.bpsd_analysis:
            primary_category = result.bpsd_analysis.get("primary_category")
            if primary_category:
                strategies = primary_category.get("management_strategies", [])
                suggestions.extend(strategies[:2])  # å–å‰ 2 å€‹å»ºè­°
        
        # åŸºæœ¬å»ºè­°
        if not suggestions:
            suggestions.append("å»ºè­°è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡é€²è¡Œè©³ç´°è©•ä¼°")
        
        if "M2" in result.modules_used:
            suggestions.append("è€ƒæ…®ç”³è«‹é•·ç…§ 2.0 æœå‹™")
        
        return suggestions
