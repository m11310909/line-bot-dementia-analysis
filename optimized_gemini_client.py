#!/usr/bin/env python3
"""
å„ªåŒ– Gemini API å®¢æˆ¶ç«¯
é™ä½æˆæœ¬ã€æå‡æ•ˆèƒ½ã€æ™ºèƒ½å¿«å–
"""

import google.generativeai as genai
import time
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from functools import wraps
import os
from redis_cache_manager import RedisCacheManager

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedGeminiClient:
    """å„ªåŒ– Gemini API å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–å„ªåŒ– Gemini å®¢æˆ¶ç«¯"""
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.cache_manager = RedisCacheManager()
        
        # æˆæœ¬å„ªåŒ–é…ç½®
        self.model_config = {
            'gemini-1.5-flash': {
                'max_tokens': 1000,
                'temperature': 0.3,
                'top_p': 0.8,
                'cost_per_1k_tokens': 0.000075  # ç¾å…ƒ
            },
            'gemini-1.5-pro': {
                'max_tokens': 2000,
                'temperature': 0.4,
                'top_p': 0.9,
                'cost_per_1k_tokens': 0.00375  # ç¾å…ƒ
            }
        }
        
        # ä½¿ç”¨çµ±è¨ˆ
        self.usage_stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'total_tokens': 0,
            'estimated_cost': 0.0
        }
        
        # åˆå§‹åŒ– Gemini
        self._init_gemini()
    
    def _init_gemini(self):
        """åˆå§‹åŒ– Gemini API"""
        if not self.api_key or self.api_key == 'your_actual_gemini_api_key_here':
            logger.error("âŒ ç¼ºå°‘ Gemini API é‡‘é‘°")
            self.model = None
            return
        
        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-1.5-flash')
            logger.info("âœ… Gemini API åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Gemini API åˆå§‹åŒ–å¤±æ•—: {e}")
            self.model = None
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®— token æ•¸é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰"""
        # ä¸­æ–‡ç´„ 1.5 å­—å…ƒ/tokenï¼Œè‹±æ–‡ç´„ 4 å­—å…ƒ/token
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        english_chars = len(text) - chinese_chars
        
        return int(chinese_chars / 1.5 + english_chars / 4)
    
    def _calculate_cost(self, input_tokens: int, output_tokens: int, model: str = 'gemini-1.5-flash') -> float:
        """è¨ˆç®— API å‘¼å«æˆæœ¬"""
        config = self.model_config.get(model, self.model_config['gemini-1.5-flash'])
        cost_per_1k = config['cost_per_1k_tokens']
        
        total_tokens = input_tokens + output_tokens
        return (total_tokens / 1000) * cost_per_1k
    
    def _optimize_prompt(self, prompt: str, max_tokens: int = 1000) -> str:
        """å„ªåŒ–æç¤ºè©ï¼Œæ¸›å°‘ token ä½¿ç”¨"""
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        prompt = ' '.join(prompt.split())
        
        # æˆªæ–·éé•·çš„æç¤ºè©
        estimated_tokens = self._estimate_tokens(prompt)
        if estimated_tokens > max_tokens * 0.7:  # ä¿ç•™ 30% çµ¦å›æ‡‰
            # æ™ºèƒ½æˆªæ–·ï¼Œä¿ç•™é‡è¦éƒ¨åˆ†
            words = prompt.split()
            target_words = int(max_tokens * 0.7 / 2)  # ç²—ç•¥ä¼°ç®—
            if len(words) > target_words:
                prompt = ' '.join(words[:target_words]) + '...'
        
        return prompt
    
    def _get_cached_response(self, prompt: str) -> Optional[str]:
        """ç²å–å¿«å–çš„å›æ‡‰"""
        return self.cache_manager.get_cached_gemini_response(prompt)
    
    def _cache_response(self, prompt: str, response: str) -> bool:
        """å¿«å–å›æ‡‰"""
        return self.cache_manager.cache_gemini_response(prompt, response)
    
    def generate_response(self, prompt: str, model: str = 'gemini-1.5-flash', 
                         max_tokens: int = None, use_cache: bool = True) -> Dict[str, Any]:
        """ç”Ÿæˆå›æ‡‰ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        start_time = time.time()
        
        # æ›´æ–°ä½¿ç”¨çµ±è¨ˆ
        self.usage_stats['total_requests'] += 1
        
        # æª¢æŸ¥å¿«å–
        if use_cache:
            cached_response = self._get_cached_response(prompt)
            if cached_response:
                self.usage_stats['cache_hits'] += 1
                logger.info(f"âœ… å¿«å–å‘½ä¸­ï¼Œç¯€çœ API å‘¼å«")
                return {
                    'response': cached_response,
                    'cached': True,
                    'tokens_used': 0,
                    'cost': 0.0,
                    'response_time': time.time() - start_time
                }
        
        # å„ªåŒ–æç¤ºè©
        optimized_prompt = self._optimize_prompt(prompt, max_tokens or 1000)
        
        # ä¼°ç®—è¼¸å…¥ tokens
        input_tokens = self._estimate_tokens(optimized_prompt)
        
        try:
            # é…ç½®æ¨¡å‹åƒæ•¸
            config = self.model_config.get(model, self.model_config['gemini-1.5-flash'])
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=max_tokens or config['max_tokens'],
                temperature=config['temperature'],
                top_p=config['top_p']
            )
            
            # ç”Ÿæˆå›æ‡‰
            response = self.model.generate_content(
                optimized_prompt,
                generation_config=generation_config
            )
            
            # è¨ˆç®—çµ±è¨ˆ
            output_tokens = self._estimate_tokens(response.text)
            total_tokens = input_tokens + output_tokens
            cost = self._calculate_cost(input_tokens, output_tokens, model)
            
            # æ›´æ–°çµ±è¨ˆ
            self.usage_stats['total_tokens'] += total_tokens
            self.usage_stats['estimated_cost'] += cost
            
            # å¿«å–å›æ‡‰
            if use_cache:
                self._cache_response(prompt, response.text)
            
            response_time = time.time() - start_time
            
            logger.info(f"ğŸ’¡ API å‘¼å«å®Œæˆ - Tokens: {total_tokens}, æˆæœ¬: ${cost:.6f}, æ™‚é–“: {response_time:.2f}s")
            
            return {
                'response': response.text,
                'cached': False,
                'tokens_used': total_tokens,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'cost': cost,
                'response_time': response_time,
                'model': model
            }
            
        except Exception as e:
            logger.error(f"âŒ Gemini API å‘¼å«å¤±æ•—: {e}")
            return {
                'response': f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                'cached': False,
                'error': str(e),
                'tokens_used': 0,
                'cost': 0.0,
                'response_time': time.time() - start_time
            }
    
    def batch_generate(self, prompts: List[str], model: str = 'gemini-1.5-flash') -> List[Dict[str, Any]]:
        """æ‰¹æ¬¡ç”Ÿæˆå›æ‡‰ï¼ˆæˆæœ¬å„ªåŒ–ï¼‰"""
        results = []
        
        for i, prompt in enumerate(prompts):
            logger.info(f"ğŸ”„ è™•ç†æ‰¹æ¬¡è«‹æ±‚ {i+1}/{len(prompts)}")
            result = self.generate_response(prompt, model)
            results.append(result)
            
            # æ‰¹æ¬¡é–“éš”ï¼Œé¿å…é€Ÿç‡é™åˆ¶
            if i < len(prompts) - 1:
                time.sleep(0.5)
        
        return results
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ç²å–ä½¿ç”¨çµ±è¨ˆ"""
        cache_stats = self.cache_manager.get_cache_stats()
        
        return {
            'api_usage': self.usage_stats,
            'cache_stats': cache_stats,
            'cost_optimization': {
                'cache_hit_rate': (self.usage_stats['cache_hits'] / max(self.usage_stats['total_requests'], 1)) * 100,
                'estimated_savings': self.usage_stats['cache_hits'] * 0.001,  # ä¼°ç®—ç¯€çœ
                'total_cost': self.usage_stats['estimated_cost']
            }
        }
    
    def clear_cache(self) -> bool:
        """æ¸…é™¤å¿«å–"""
        return self.cache_manager.clear_pattern("cache:gemini*")
    
    def reset_stats(self):
        """é‡ç½®çµ±è¨ˆ"""
        self.usage_stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'total_tokens': 0,
            'estimated_cost': 0.0
        }

# æˆæœ¬å„ªåŒ–è£é£¾å™¨
def optimize_gemini_call(cache: bool = True, max_tokens: int = 1000):
    """Gemini API å‘¼å«å„ªåŒ–è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç²å– Gemini å®¢æˆ¶ç«¯
            client = getattr(wrapper, '_gemini_client', None)
            if not client:
                client = OptimizedGeminiClient()
                wrapper._gemini_client = client
            
            # æå–æç¤ºè©åƒæ•¸
            prompt = None
            if 'prompt' in kwargs:
                prompt = kwargs['prompt']
            elif args:
                prompt = args[0]
            
            if prompt:
                # ä½¿ç”¨å„ªåŒ–å®¢æˆ¶ç«¯
                result = client.generate_response(prompt, max_tokens=max_tokens, use_cache=cache)
                return result['response']
            else:
                # å›é€€åˆ°åŸå§‹å‡½æ•¸
                return func(*args, **kwargs)
        
        return wrapper
    return decorator

# å…¨åŸŸå„ªåŒ–å®¢æˆ¶ç«¯å¯¦ä¾‹
optimized_client = OptimizedGeminiClient()

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    print("ğŸ§ª æ¸¬è©¦å„ªåŒ– Gemini API å®¢æˆ¶ç«¯...")
    
    # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
    test_prompts = [
        "è«‹ç°¡çŸ­èªªæ˜å¤±æ™ºç—‡çš„æ—©æœŸç—‡ç‹€",
        "ä»€éº¼æ˜¯è¨˜æ†¶åŠ›æ¸›é€€ï¼Ÿ",
        "å¦‚ä½•ç…§é¡§å¤±æ™ºç—‡æ‚£è€…ï¼Ÿ"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ æ¸¬è©¦æç¤ºè©: {prompt[:50]}...")
        result = optimized_client.generate_response(prompt)
        print(f"âœ… å›æ‡‰: {result['response'][:100]}...")
        print(f"ğŸ“Š Tokens: {result['tokens_used']}, æˆæœ¬: ${result['cost']:.6f}")
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = optimized_client.get_usage_stats()
    print(f"\nğŸ“ˆ ä½¿ç”¨çµ±è¨ˆ: {stats}") 