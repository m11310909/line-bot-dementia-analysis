#!/usr/bin/env python3
"""
Dockerized RAG Service with GPU Acceleration
Enhanced for microservices architecture
"""

import os
import json
import logging
import torch
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import faiss

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/rag-service.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="RAG Service - GPU Accelerated",
    description="Microservices-based RAG for dementia knowledge retrieval with GPU acceleration",
    version="3.0.0"
)

# Pydantic models
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    threshold: float = 0.5
    use_gpu: bool = True

class QueryResponse(BaseModel):
    success: bool
    results: List[Dict[str, Any]]
    confidence: float
    gpu_used: bool
    processing_time: float
    timestamp: datetime

# Initialize RAG engine with GPU support
class GPUAcceleratedRAGEngine:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üöÄ Using device: {self.device}")
        
        # Initialize sentence transformer
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.model.to(self.device)
        
        # Initialize knowledge base
        self.knowledge_base = self._initialize_knowledge_base()
        self.vector_index = None
        self.embeddings = None
        
        # Build vector index
        self._build_vector_index()
        
        logger.info("‚úÖ GPU Accelerated RAG Engine initialized")
    
    def _initialize_knowledge_base(self) -> Dict[str, Any]:
        """Initialize knowledge base with dementia care information"""
        return {
            "warning_signs": {
                "Ë®òÊÜ∂ÂäõÊ∏õÈÄÄ": "Á∂ìÂ∏∏ÂøòË®òÊúÄËøëÁôºÁîüÁöÑ‰∫ãÊÉÖÔºåÈáçË§áÂïèÂêåÊ®£ÁöÑÂïèÈ°åÔºåÈÄôÊòØÂ§±Êô∫ÁóáÊúÄÂ∏∏Ë¶ãÁöÑÊó©ÊúüÁóáÁãÄ",
                "Ë™ûË®ÄÂõ∞Èõ£": "Êâæ‰∏çÂà∞Ê≠£Á¢∫ÁöÑË©ûÂΩôÔºåË™™Ë©±ÊôÇÂÅúÈ†ìÊàñÁå∂Ë±´ÔºåÂèØËÉΩÂøòË®òÁ∞°ÂñÆÁöÑË©ûÂΩô",
                "Ëø∑Ë∑Ø": "Âú®ÁÜüÊÇâÁöÑÂú∞ÊñπËø∑Ë∑ØÔºåÂøòË®òÂõûÂÆ∂ÁöÑË∑ØÔºåÊñπÂêëÊÑüËÆäÂ∑Æ",
                "Âà§Êñ∑Âäõ‰∏ãÈôç": "ÂÅöÊ±∫ÂÆöÊôÇÂá∫ÁèæÂõ∞Èõ£ÔºåÁÑ°Ê≥ïËôïÁêÜË§áÈõúÊÉÖÊ≥ÅÔºåË≤°ÂãôÁÆ°ÁêÜËÉΩÂäõ‰∏ãÈôç",
                "ÊÉÖÁ∑íËÆäÂåñ": "ÊÉÖÁ∑í‰∏çÁ©©ÂÆöÔºåÂÆπÊòìÁÑ¶ÊÖÆÊàñÊÜÇÈ¨±ÔºåÊÄßÊ†ºÂèØËÉΩÁôºÁîüÊîπËÆä"
            },
            "care_guidelines": {
                "Êó©ÊúüÁÖßË≠∑": "Âª∫Á´ãË¶èÂæãÁîüÊ¥ªÔºå‰øùÊåÅË™çÁü•Ê¥ªÂãïÔºåÂÆöÊúüÂÅ•Â∫∑Ê™¢Êü•ÔºåÁ∂≠ÊåÅÁ§æ‰∫§Ê¥ªÂãï",
                "‰∏≠ÊúüÁÖßË≠∑": "Êèê‰æõÂÆâÂÖ®Áí∞Â¢ÉÔºåÂçîÂä©Êó•Â∏∏ÁîüÊ¥ªÔºåÂ∞ãÊ±ÇÂ∞àÊ•≠ÊîØÊè¥ÔºåÂª∫Á´ãÁÖßË≠∑Ë®àÁï´",
                "ÊôöÊúüÁÖßË≠∑": "ÂÖ®Èù¢ÁÖßË≠∑ÊîØÊè¥ÔºåËàíÈÅ©ÁÖßË≠∑ÔºåÂÆ∂Â±¨ÂøÉÁêÜÊîØÊåÅÔºåÂÆâÂØßÁÖßË≠∑Ê∫ñÂÇô"
            },
            "resources": {
                "ÈÜ´ÁôÇË≥áÊ∫ê": "Â§±Êô∫ÁóáÈñÄË®∫„ÄÅÁ•ûÁ∂ìÁßëÈÜ´Â∏´„ÄÅÁ≤æÁ•ûÁßëÈÜ´Â∏´„ÄÅË®òÊÜ∂ÈñÄË®∫„ÄÅË™çÁü•ÂäüËÉΩË©ï‰º∞",
                "ÁÖßË≠∑ÊúçÂãô": "Â±ÖÂÆ∂ÁÖßË≠∑„ÄÅÊó•ÈñìÁÖßË≠∑„ÄÅÊ©üÊßãÁÖßË≠∑„ÄÅÂñòÊÅØÊúçÂãô„ÄÅÁÖßË≠∑ËÄÖÊîØÊåÅ",
                "Á§æÊúÉÊîØÊåÅ": "Â§±Êô∫ÁóáÂçîÊúÉ„ÄÅÂÆ∂Â±¨ÊîØÊåÅÂúòÈ´î„ÄÅÁÖßË≠∑ËÄÖÂñòÊÅØÊúçÂãô„ÄÅÂøÉÁêÜË´ÆÂïÜ",
                "Á∂ìÊøüË£úÂä©": "Ë∫´ÂøÉÈöúÁ§ôÊâãÂÜä„ÄÅÈï∑ÁÖßÊúçÂãô„ÄÅÁÖßË≠∑Ë£úÂä©„ÄÅÈÜ´ÁôÇË£úÂä©„ÄÅÁ§æÊúÉÁ¶èÂà©"
            },
            "treatment_options": {
                "Ëó•Áâ©Ê≤ªÁôÇ": "ËÜΩÈπºÈÖ∂ÊäëÂà∂Âäë„ÄÅNMDAÂèóÈ´îÊãÆÊäóÂäë„ÄÅÊäóÁ≤æÁ•ûÁóÖËó•Áâ©„ÄÅÊäóÊÜÇÈ¨±Ëó•Áâ©",
                "ÈùûËó•Áâ©Ê≤ªÁôÇ": "Ë™çÁü•Ë®ìÁ∑¥„ÄÅÈü≥Ê®ÇÊ≤ªÁôÇ„ÄÅËóùË°ìÊ≤ªÁôÇ„ÄÅÂúíËóùÊ≤ªÁôÇ„ÄÅÂØµÁâ©Ê≤ªÁôÇ",
                "ÁîüÊ¥ªË™øÊï¥": "Ë¶èÂæã‰ΩúÊÅØ„ÄÅÁáüÈ§äÂùáË°°„ÄÅÈÅ©Â∫¶ÈÅãÂãï„ÄÅÁ§æ‰∫§Ê¥ªÂãï„ÄÅÁí∞Â¢ÉÂÆâÂÖ®"
            }
        }
    
    def _build_vector_index(self):
        """Build FAISS vector index for fast similarity search"""
        try:
            # Prepare documents
            documents = []
            for domain, items in self.knowledge_base.items():
                for title, content in items.items():
                    documents.append({
                        "id": f"{domain}_{title}",
                        "domain": domain,
                        "title": title,
                        "content": content,
                        "text": f"{title}: {content}"
                    })
            
            # Generate embeddings
            texts = [doc["text"] for doc in documents]
            embeddings = self.model.encode(texts, convert_to_tensor=True, device=self.device)
            
            # Convert to numpy for FAISS
            embeddings_np = embeddings.cpu().numpy().astype('float32')
            
            # Build FAISS index
            dimension = embeddings_np.shape[1]
            self.vector_index = faiss.IndexFlatIP(dimension)
            self.vector_index.add(embeddings_np)
            
            # Store documents for retrieval
            self.documents = documents
            self.embeddings = embeddings_np
            
            logger.info(f"‚úÖ Vector index built with {len(documents)} documents")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to build vector index: {e}")
            self.vector_index = None
    
    def search_knowledge_gpu(self, query: str, top_k: int = 5, threshold: float = 0.5) -> Dict[str, Any]:
        """GPU-accelerated knowledge search"""
        try:
            start_time = datetime.now()
            
            # Generate query embedding
            query_embedding = self.model.encode([query], convert_to_tensor=True, device=self.device)
            query_vector = query_embedding.cpu().numpy().astype('float32')
            
            # Search in vector index
            if self.vector_index is not None:
                scores, indices = self.vector_index.search(query_vector, top_k)
                
                results = []
                for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                    if score >= threshold and idx < len(self.documents):
                        doc = self.documents[idx]
                        results.append({
                            "type": doc["domain"],
                            "title": doc["title"],
                            "content": doc["content"],
                            "confidence": float(score),
                            "rank": i + 1
                        })
            else:
                # Fallback to keyword search
                results = self._keyword_search(query, top_k)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            avg_confidence = sum(r["confidence"] for r in results) / len(results) if results else 0
            
            return {
                "success": True,
                "results": results,
                "confidence": avg_confidence,
                "total_found": len(results),
                "gpu_used": self.device.type == "cuda",
                "processing_time": processing_time
            }
            
        except Exception as e:
            logger.error(f"GPU search failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": [],
                "confidence": 0,
                "gpu_used": False,
                "processing_time": 0
            }
    
    def _keyword_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """Fallback keyword search"""
        results = []
        query_lower = query.lower()
        
        for domain, items in self.knowledge_base.items():
            for title, content in items.items():
                if query_lower in title.lower() or query_lower in content.lower():
                    results.append({
                        "type": domain,
                        "title": title,
                        "content": content,
                        "confidence": 0.6,
                        "rank": len(results) + 1
                    })
                    if len(results) >= top_k:
                        break
        
        return results

# Initialize RAG engine
rag_engine = GPUAcceleratedRAGEngine()

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "RAG Service - GPU Accelerated",
        "status": "running",
        "version": "3.0.0",
        "architecture": "microservices",
        "gpu_available": torch.cuda.is_available(),
        "device": str(rag_engine.device),
        "knowledge_domains": ["warning_signs", "care_guidelines", "resources", "treatment_options"]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "rag-service",
        "version": "3.0.0",
        "gpu_available": torch.cuda.is_available(),
        "device": str(rag_engine.device),
        "vector_index_ready": rag_engine.vector_index is not None,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/search")
async def search_knowledge(request: QueryRequest):
    """GPU-accelerated knowledge search"""
    try:
        logger.info(f"üîç GPU Search: {request.query[:50]}...")
        
        result = rag_engine.search_knowledge_gpu(
            request.query, 
            top_k=request.top_k,
            threshold=request.threshold
        )
        
        logger.info(f"‚úÖ Search completed with {result.get('total_found', 0)} results in {result.get('processing_time', 0):.3f}s")
        
        return QueryResponse(**result)
        
    except Exception as e:
        logger.error(f"‚ùå Search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/gpu-status")
async def gpu_status():
    """Get GPU status and capabilities"""
    return {
        "cuda_available": torch.cuda.is_available(),
        "device": str(rag_engine.device),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "current_device": torch.cuda.current_device() if torch.cuda.is_available() else None,
        "device_name": torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU"
    }

@app.get("/knowledge/{domain}")
async def get_knowledge_domain(domain: str):
    """Get knowledge by domain"""
    try:
        if domain not in rag_engine.knowledge_base:
            raise HTTPException(status_code=400, detail=f"Domain {domain} not found")
        
        return {
            "domain": domain,
            "knowledge": rag_engine.knowledge_base[domain],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Get knowledge domain failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/domains")
async def list_domains():
    """List available knowledge domains"""
    return {
        "domains": list(rag_engine.knowledge_base.keys()),
        "descriptions": {
            "warning_signs": "Â§±Êô∫ÁóáË≠¶Ë®äÂæµÂÖÜ",
            "care_guidelines": "ÁÖßË≠∑ÊåáÂçó",
            "resources": "ÁÖßË≠∑Ë≥áÊ∫ê",
            "treatment_options": "Ê≤ªÁôÇÈÅ∏È†Ö"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8006) 