#!/usr/bin/env python3
"""
ğŸ§ª å¯¦æ¸¬ç´ æ - M1-M4 å„ªåŒ– XAI ç³»çµ±
æä¾›å®Œæ•´çš„æ¸¬è©¦æ¡ˆä¾‹ã€æ•ˆèƒ½åŸºæº–å’Œé©—è­‰è…³æœ¬
"""

import time
import requests
import json
import statistics
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime
import asyncio

@dataclass
class TestCase:
    """æ¸¬è©¦æ¡ˆä¾‹"""
    input_text: str
    expected_module: str
    description: str
    category: str
    expected_confidence_range: tuple

class XAITestMaterials:
    """XAI ç³»çµ±å¯¦æ¸¬ç´ æ"""
    
    def __init__(self):
        self.xai_wrapper_url = "http://localhost:8009/analyze"
        self.chatbot_api_url = "http://localhost:8008/analyze"
        self.line_bot_url = "http://localhost:8081/health"
        
        # çœŸå¯¦æ¸¬è©¦æ¡ˆä¾‹
        self.test_cases = [
            # M1 è­¦è¨Šæ¸¬è©¦æ¡ˆä¾‹
            TestCase(
                input_text="çˆ¸çˆ¸å¿˜è¨˜é—œç“¦æ–¯çˆ",
                expected_module="M1",
                description="M1-01 å¿˜è¨˜é—œç“¦æ–¯çˆ",
                category="memory_warning",
                expected_confidence_range=(0.7, 0.9)
            ),
            TestCase(
                input_text="åª½åª½é‡è¤‡å•åŒæ¨£çš„å•é¡Œ",
                expected_module="M1",
                description="M1-02 é‡è¤‡ç™¼å•",
                category="memory_warning",
                expected_confidence_range=(0.8, 0.95)
            ),
            TestCase(
                input_text="çˆºçˆºå¿˜è¨˜å›å®¶çš„è·¯",
                expected_module="M1",
                description="M1-03 ç©ºé–“å®šå‘éšœç¤™",
                category="memory_warning",
                expected_confidence_range=(0.75, 0.9)
            ),
            TestCase(
                input_text="å¥¶å¥¶ä¸æœƒä½¿ç”¨æ´—è¡£æ©Ÿ",
                expected_module="M1",
                description="M1-04 å·¥å…·ä½¿ç”¨å›°é›£",
                category="memory_warning",
                expected_confidence_range=(0.7, 0.85)
            ),
            
            # M2 ç—…ç¨‹æ¸¬è©¦æ¡ˆä¾‹
            TestCase(
                input_text="åª½åª½è¼•åº¦å¤±æ™ºç—‡ç‹€",
                expected_module="M2",
                description="M2-01 æ—©æœŸéšæ®µ",
                category="progression",
                expected_confidence_range=(0.6, 0.8)
            ),
            TestCase(
                input_text="çˆ¸çˆ¸ä¸­åº¦å¤±æ™º",
                expected_module="M2",
                description="M2-02 ä¸­æœŸéšæ®µ",
                category="progression",
                expected_confidence_range=(0.7, 0.85)
            ),
            TestCase(
                input_text="çˆºçˆºé‡åº¦å¤±æ™º",
                expected_module="M2",
                description="M2-03 æ™šæœŸéšæ®µ",
                category="progression",
                expected_confidence_range=(0.8, 0.95)
            ),
            TestCase(
                input_text="å¥¶å¥¶ç—…ç¨‹é€²å±•å¿«é€Ÿ",
                expected_module="M2",
                description="M2-04 å¿«é€Ÿé€²å±•",
                category="progression",
                expected_confidence_range=(0.75, 0.9)
            ),
            
            # M3 BPSD æ¸¬è©¦æ¡ˆä¾‹
            TestCase(
                input_text="çˆºçˆºæœ‰å¦„æƒ³ç—‡ç‹€",
                expected_module="M3",
                description="M3-01 å¦„æƒ³ç—‡ç‹€",
                category="bpsd",
                expected_confidence_range=(0.8, 0.95)
            ),
            TestCase(
                input_text="åª½åª½æœ‰å¹»è¦º",
                expected_module="M3",
                description="M3-02 å¹»è¦ºç—‡ç‹€",
                category="bpsd",
                expected_confidence_range=(0.75, 0.9)
            ),
            TestCase(
                input_text="çˆ¸çˆ¸æœ‰æ”»æ“Šè¡Œç‚º",
                expected_module="M3",
                description="M3-03 æ”»æ“Šè¡Œç‚º",
                category="bpsd",
                expected_confidence_range=(0.8, 0.95)
            ),
            TestCase(
                input_text="å¥¶å¥¶æœ‰èºå‹•ä¸å®‰",
                expected_module="M3",
                description="M3-04 èºå‹•ç—‡ç‹€",
                category="bpsd",
                expected_confidence_range=(0.7, 0.85)
            ),
            
            # M4 ç…§è­·æ¸¬è©¦æ¡ˆä¾‹
            TestCase(
                input_text="éœ€è¦é†«ç™‚å”åŠ©",
                expected_module="M4",
                description="M4-01 é†«ç™‚éœ€æ±‚",
                category="care_navigation",
                expected_confidence_range=(0.7, 0.85)
            ),
            TestCase(
                input_text="éœ€è¦ç…§è­·è³‡æº",
                expected_module="M4",
                description="M4-02 ç…§è­·è³‡æº",
                category="care_navigation",
                expected_confidence_range=(0.75, 0.9)
            ),
            TestCase(
                input_text="éœ€è¦ç¤¾æœƒæ”¯æŒ",
                expected_module="M4",
                description="M4-03 ç¤¾æœƒæ”¯æŒ",
                category="care_navigation",
                expected_confidence_range=(0.6, 0.8)
            ),
            TestCase(
                input_text="éœ€è¦ç¶“æ¿Ÿå”åŠ©",
                expected_module="M4",
                description="M4-04 ç¶“æ¿Ÿå”åŠ©",
                category="care_navigation",
                expected_confidence_range=(0.65, 0.8)
            )
        ]
        
        # é‚Šç•Œæ¸¬è©¦æ¡ˆä¾‹
        self.edge_cases = [
            TestCase(
                input_text="",
                expected_module="general",
                description="ç©ºå­—ä¸²æ¸¬è©¦",
                category="edge_case",
                expected_confidence_range=(0.0, 0.3)
            ),
            TestCase(
                input_text="ä»Šå¤©å¤©æ°£å¾ˆå¥½",
                expected_module="general",
                description="ç„¡é—œå…§å®¹æ¸¬è©¦",
                category="edge_case",
                expected_confidence_range=(0.0, 0.4)
            ),
            TestCase(
                input_text="å¤±æ™ºç—‡æ‚£è€…éœ€è¦å°ˆæ¥­é†«ç™‚ç…§è­·å’Œç¤¾æœƒæ”¯æŒ",
                expected_module="M4",
                description="è¤‡é›œæè¿°æ¸¬è©¦",
                category="edge_case",
                expected_confidence_range=(0.7, 0.9)
            )
        ]
        
        # æ•ˆèƒ½åŸºæº–
        self.performance_benchmarks = {
            "immediate": {"target": 1.0, "acceptable": 1.5},
            "quick": {"target": 3.0, "acceptable": 4.0},
            "detailed": {"target": 5.0, "acceptable": 6.0}
        }
    
    def test_single_case(self, test_case: TestCase, stage: str = "immediate") -> Dict[str, Any]:
        """æ¸¬è©¦å–®ä¸€æ¡ˆä¾‹"""
        start_time = time.time()
        
        try:
            response = requests.post(
                self.xai_wrapper_url,
                json={
                    "user_input": test_case.input_text,
                    "user_id": "test_user",
                    "stage": stage
                },
                timeout=10
            )
            
            response_time = time.time() - start_time
            response.raise_for_status()
            
            data = response.json()
            xai_data = data.get("xai_enhanced", {})
            
            # é©—è­‰çµæœ
            actual_module = xai_data.get("module", "unknown")
            actual_confidence = xai_data.get("confidence", 0.0)
            expected_min, expected_max = test_case.expected_confidence_range
            
            # è©•ä¼°çµæœ
            module_correct = actual_module == test_case.expected_module
            confidence_in_range = expected_min <= actual_confidence <= expected_max
            performance_ok = response_time <= self.performance_benchmarks[stage]["acceptable"]
            
            return {
                "test_case": test_case.description,
                "input": test_case.input_text,
                "expected_module": test_case.expected_module,
                "actual_module": actual_module,
                "module_correct": module_correct,
                "expected_confidence_range": test_case.expected_confidence_range,
                "actual_confidence": actual_confidence,
                "confidence_in_range": confidence_in_range,
                "response_time": response_time,
                "performance_ok": performance_ok,
                "stage": stage,
                "success": module_correct and confidence_in_range and performance_ok
            }
            
        except Exception as e:
            return {
                "test_case": test_case.description,
                "input": test_case.input_text,
                "error": str(e),
                "response_time": time.time() - start_time,
                "success": False
            }
    
    def run_comprehensive_test(self, iterations: int = 3) -> Dict[str, Any]:
        """åŸ·è¡Œç¶œåˆæ¸¬è©¦"""
        print("ğŸ§ª é–‹å§‹ç¶œåˆæ¸¬è©¦...")
        print(f"ğŸ“Š æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {len(self.test_cases)}")
        print(f"ğŸ”„ æ¯æ¡ˆä¾‹é‡è¤‡æ¬¡æ•¸: {iterations}")
        print("="*60)
        
        all_results = []
        
        # æ¸¬è©¦ä¸»è¦æ¡ˆä¾‹
        for i, test_case in enumerate(self.test_cases, 1):
            print(f"\nğŸ“‹ æ¸¬è©¦æ¡ˆä¾‹ {i}/{len(self.test_cases)}: {test_case.description}")
            print(f"   è¼¸å…¥: {test_case.input_text}")
            print(f"   é æœŸæ¨¡çµ„: {test_case.expected_module}")
            
            case_results = []
            for j in range(iterations):
                print(f"   è¿­ä»£ {j+1}/{iterations}...", end=" ")
                
                # æ¸¬è©¦å³æ™‚éšæ®µ
                immediate_result = self.test_single_case(test_case, "immediate")
                case_results.append(immediate_result)
                
                # æ¸¬è©¦å¿«é€Ÿéšæ®µ
                quick_result = self.test_single_case(test_case, "quick")
                case_results.append(quick_result)
                
                print(f"âœ“ ({immediate_result.get('response_time', 0):.2f}s, {quick_result.get('response_time', 0):.2f}s)")
                
                # çŸ­æš«ä¼‘æ¯
                time.sleep(0.5)
            
            all_results.extend(case_results)
        
        # æ¸¬è©¦é‚Šç•Œæ¡ˆä¾‹
        print(f"\nğŸ” æ¸¬è©¦é‚Šç•Œæ¡ˆä¾‹...")
        for edge_case in self.edge_cases:
            print(f"   é‚Šç•Œæ¸¬è©¦: {edge_case.description}")
            edge_result = self.test_single_case(edge_case, "immediate")
            all_results.append(edge_result)
        
        return self.analyze_test_results(all_results)
    
    def analyze_test_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ¸¬è©¦çµæœ"""
        if not results:
            return {"error": "No test results available"}
        
        # åˆ†é¡çµæœ
        successful_results = [r for r in results if r.get("success", False)]
        failed_results = [r for r in results if not r.get("success", False)]
        
        # æŒ‰æ¨¡çµ„åˆ†çµ„
        module_results = {}
        for result in results:
            module = result.get("actual_module", "unknown")
            if module not in module_results:
                module_results[module] = []
            module_results[module].append(result)
        
        # æŒ‰éšæ®µåˆ†çµ„
        stage_results = {}
        for result in results:
            stage = result.get("stage", "unknown")
            if stage not in stage_results:
                stage_results[stage] = []
            stage_results[stage].append(result)
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        response_times = [r.get("response_time", 0) for r in results if "response_time" in r]
        confidences = [r.get("actual_confidence", 0) for r in results if "actual_confidence" in r]
        
        analysis = {
            "summary": {
                "total_tests": len(results),
                "successful_tests": len(successful_results),
                "failed_tests": len(failed_results),
                "success_rate": len(successful_results) / len(results) if results else 0,
                "average_response_time": statistics.mean(response_times) if response_times else 0,
                "median_response_time": statistics.median(response_times) if response_times else 0,
                "average_confidence": statistics.mean(confidences) if confidences else 0
            },
            "by_module": {},
            "by_stage": {},
            "performance_analysis": {},
            "detailed_results": results
        }
        
        # æŒ‰æ¨¡çµ„åˆ†æ
        for module, module_data in module_results.items():
            module_success = len([r for r in module_data if r.get("success", False)])
            module_response_times = [r.get("response_time", 0) for r in module_data if "response_time" in r]
            
            analysis["by_module"][module] = {
                "count": len(module_data),
                "success_count": module_success,
                "success_rate": module_success / len(module_data) if module_data else 0,
                "average_response_time": statistics.mean(module_response_times) if module_response_times else 0
            }
        
        # æŒ‰éšæ®µåˆ†æ
        for stage, stage_data in stage_results.items():
            stage_response_times = [r.get("response_time", 0) for r in stage_data if "response_time" in r]
            target_time = self.performance_benchmarks.get(stage, {}).get("target", 5.0)
            
            analysis["by_stage"][stage] = {
                "count": len(stage_data),
                "average_response_time": statistics.mean(stage_response_times) if stage_response_times else 0,
                "target_time": target_time,
                "target_met": all(rt <= target_time for rt in stage_response_times) if stage_response_times else False
            }
        
        # æ•ˆèƒ½åˆ†æ
        for stage in ["immediate", "quick", "detailed"]:
            if stage in stage_results:
                stage_times = [r.get("response_time", 0) for r in stage_results[stage] if "response_time" in r]
                if stage_times:
                    analysis["performance_analysis"][stage] = {
                        "average_time": statistics.mean(stage_times),
                        "target_time": self.performance_benchmarks[stage]["target"],
                        "acceptable_time": self.performance_benchmarks[stage]["acceptable"],
                        "target_met": all(t <= self.performance_benchmarks[stage]["target"] for t in stage_times),
                        "acceptable_met": all(t <= self.performance_benchmarks[stage]["acceptable"] for t in stage_times)
                    }
        
        return analysis
    
    def print_test_report(self, analysis: Dict[str, Any]) -> None:
        """æ‰“å°æ¸¬è©¦å ±å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š XAI ç³»çµ±å¯¦æ¸¬å ±å‘Š")
        print("="*80)
        
        summary = analysis["summary"]
        print(f"\nğŸ¯ æ•´é«”æ¸¬è©¦çµæœ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {summary['total_tests']}")
        print(f"   æˆåŠŸæ¸¬è©¦: {summary['successful_tests']}")
        print(f"   å¤±æ•—æ¸¬è©¦: {summary['failed_tests']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"   å¹³å‡å›æ‡‰æ™‚é–“: {summary['average_response_time']:.2f}ç§’")
        print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {summary['average_confidence']:.1%}")
        
        # æŒ‰æ¨¡çµ„åˆ†æ
        print(f"\nğŸ§  æ¨¡çµ„åˆ†æ:")
        for module, data in analysis["by_module"].items():
            status = "âœ…" if data["success_rate"] > 0.8 else "âš ï¸" if data["success_rate"] > 0.6 else "âŒ"
            print(f"   {status} {module}: {data['count']} æ¸¬è©¦, æˆåŠŸç‡ {data['success_rate']:.1%}, å¹³å‡æ™‚é–“ {data['average_response_time']:.2f}s")
        
        # æŒ‰éšæ®µåˆ†æ
        print(f"\nâš¡ éšæ®µåˆ†æ:")
        for stage, data in analysis["by_stage"].items():
            target_status = "âœ…" if data["target_met"] else "âŒ"
            print(f"   {target_status} {stage.upper()}: {data['count']} æ¸¬è©¦, å¹³å‡æ™‚é–“ {data['average_response_time']:.2f}s / ç›®æ¨™ {data['target_time']}s")
        
        # æ•ˆèƒ½åˆ†æ
        print(f"\nğŸ¯ æ•ˆèƒ½ç›®æ¨™é”æˆ:")
        for stage, data in analysis["performance_analysis"].items():
            target_icon = "âœ…" if data["target_met"] else "âŒ"
            acceptable_icon = "âœ…" if data["acceptable_met"] else "âŒ"
            print(f"   {target_icon} {stage.upper()}: {data['average_time']:.2f}s / {data['target_time']}s (ç›®æ¨™) / {data['acceptable_time']}s (å¯æ¥å—)")
        
        # å»ºè­°
        print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
        if summary["success_rate"] < 0.9:
            print("   âš ï¸  æˆåŠŸç‡ä½æ–¼ 90%ï¼Œå»ºè­°æª¢æŸ¥æ¨¡çµ„æª¢æ¸¬é‚è¼¯")
        if summary["average_response_time"] > 2.0:
            print("   âš ï¸  å¹³å‡å›æ‡‰æ™‚é–“éé•·ï¼Œå»ºè­°å„ªåŒ–æ•ˆèƒ½")
        
        # æª¢æŸ¥å„éšæ®µæ•ˆèƒ½
        for stage, data in analysis["performance_analysis"].items():
            if not data["target_met"]:
                print(f"   âš ï¸  {stage.upper()} éšæ®µæœªé”ç›®æ¨™æ™‚é–“ï¼Œå»ºè­°å„ªåŒ–")
        
        print("="*80)
    
    def generate_test_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
        return {
            "test_cases": [
                {
                    "input": tc.input_text,
                    "expected_module": tc.expected_module,
                    "description": tc.description,
                    "category": tc.category,
                    "confidence_range": tc.expected_confidence_range
                }
                for tc in self.test_cases
            ],
            "edge_cases": [
                {
                    "input": tc.input_text,
                    "expected_module": tc.expected_module,
                    "description": tc.description,
                    "category": tc.category,
                    "confidence_range": tc.expected_confidence_range
                }
                for tc in self.edge_cases
            ],
            "performance_benchmarks": self.performance_benchmarks
        }
    
    def save_test_results(self, analysis: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"xai_test_results_{timestamp}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
        
        return filename

def main():
    """ä¸»å‡½æ•¸"""
    tester = XAITestMaterials()
    
    print("ğŸ§ª XAI ç³»çµ±å¯¦æ¸¬ç´ æ")
    print("="*50)
    
    # æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹
    print("\nğŸ” æª¢æŸ¥ç³»çµ±å¥åº·ç‹€æ…‹...")
    try:
        health_response = requests.get("http://localhost:8009/health", timeout=5)
        if health_response.status_code == 200:
            print("âœ… XAI Wrapper æœå‹™æ­£å¸¸")
        else:
            print("âŒ XAI Wrapper æœå‹™ç•°å¸¸")
            return
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° XAI Wrapper: {e}")
        return
    
    # åŸ·è¡Œç¶œåˆæ¸¬è©¦
    print("\nğŸš€ é–‹å§‹åŸ·è¡Œç¶œåˆæ¸¬è©¦...")
    analysis = tester.run_comprehensive_test(iterations=2)
    
    # æ‰“å°å ±å‘Š
    tester.print_test_report(analysis)
    
    # ä¿å­˜çµæœ
    filename = tester.save_test_results(analysis)
    print(f"\nğŸ’¾ æ¸¬è©¦çµæœå·²ä¿å­˜è‡³: {filename}")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    test_data = tester.generate_test_data()
    test_data_filename = f"test_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(test_data_filename, "w", encoding="utf-8") as f:
        json.dump(test_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“‹ æ¸¬è©¦æ•¸æ“šå·²ä¿å­˜è‡³: {test_data_filename}")

if __name__ == "__main__":
    main() 